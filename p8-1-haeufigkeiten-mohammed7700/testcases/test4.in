
Dieses Skript orientiert sich an einem Lehrbuch von Cormen, Leiserson, Rivest und Stein , einem Lehrbuch von Sedgewick und Wayne  und an verschiedenen Vorlesungsskripten .


Die Autoren freuen sich über Hinweise auf Fehler und andere Kommentare zum Skript
unter der Emailadresse mailto:dschmidt@hhu.dedschmidt@hhu.de.


Einleitung: Wie wir Algorithmen und Datenstrukturen aufschreiben und untersuchen

Was sind Algorithmen und Datenstrukturen?

Wir beschäftigen uns in dieser Vorlesung mit Algorithmen, also Rechenverfahren, die zu einer gegebenen Fragestellung - der Eingabe - eine Antwort - die Ausgabe - berechnen, indem sie eine Reihe von festgelegten Rechenschritten durchführen.
Damit ein Rechenverfahren ein richtiger Algorithmus ist, sollte es einige Eigenschaften erfüllen:

  Das Verfahren sollte deterministisch sein, d.h., es sollte für die gleiche Eingabe stets die gleiche Ausgabe liefern; egal wie oft wir das Verfahren ausführen.
  Die Schritte des Verfahrens sollten wohldefiniert sein, d.h., es muss eindeutig sein, was jeder einzelne Schritt tut.
  Die Abfolge der Schritte muss definiert sein, d.h. es sollte kein Zweifel darüber herrschen, in welcher Reihenfolge die Rechenschritte ausgeführt werden und welcher Schritt als nächstes folgt.
  Das Verfahren sollte nach endlich vielen Schritten beendet sein.
Jedes (wohlgeformte) Computerprogramm beschreibt also einen Algorithmus im obigen Sinne. Dennoch sind Algorithmen nicht an Computer gebunden: Sie funktionieren genau so gut auf Papier, in unseren Köpfen,Tatsächlich kennen wir auch Algorithmen im Alltag, z.B. als Kochrezepte, Geschäftsprozesse, Anleitungen oder Wegbeschreibungen.
Fast immer ist dabei das Ziel eines Algorithmus, ein Problem zu lösen.
Wir interessieren uns daher für endliche Algorithmen, also solche, die nach endlich vielen Rechenschritten eine Antwort auf unsere Frage liefern.

Eine Datenstruktur dient dazu, Daten so strukturiert abzuspeichern, dass wir eine gewünschte Information schnell finden können. Auch Datenstrukturen kennen wir aus dem Alltag: Zum Beispiel hilft uns ein Glossar in einem Buch Informationen über einen Begriff zu finden, ohne dass wir das ganze Buch durchlesen müssen.

Ziele dieser Vorlesung

Algorithmen und Datenstrukturen sind Werkzeuge, um Fragestellungen zu beantworten.
Wie finden wir aber die richtigen Werkzeuge für eine Fragestellung? Der Lösungsprozess sieht häufig so aus:

  Wir modellieren die Fragestellung als abstraktes Problem, d.h. wir entscheiden, welche Aspekte der Fragestellung für eine hinreichend gute Lösung wirklich relevant sind und welche Aspekte wir vernachlässigen können.
  Das Ziel dieses Schrittes ist es, eine präzise Formulierung der Aufgabenstellung zu erhalten.
  Wir versuchen, das Problem mit einem uns bekannten Algorithmus zu lösen.
  Löst kein uns bekannter Algorithmus das Problem, so tut es vielleicht eine geschickte Kombination oder eine leichte Abwandlung von bekannten Algorithmen!
  Schlägt auch das fehl, probieren wir systematisch verschiedene Algorithmenentwurfstechniken aus.
  Derartige Techniken lernen wir im Laufe der Vorlesung kennen.
Das Ziel der Vorlesung ist es, dass Sie den Algorithmenentwurf bis wenigstens Schritt 2 beherrschen. Gute und sehr gute Teilnehmer:innen beherrschen auch Schritt 3 bzw. 4.

Zur erfolgreichen Durchführung des Algorithmenentwurfes sind einige Fertigkeiten erforderlich.

  Die Vorlesung soll Ihnen beibringen, strukturiert über Algorithmen zu sprechen und ihre Eigenschaften zu untersuchen. Insbesondere sollen Sie die Fertigkeit erlangen, abstrakte Problemstellungen, Algorithmen und Datenstrukturen präzise zu formulieren.
  Diese Fertigkeiten sind z.B. erforderlich für Schritt 1 des Algorithmenentwurfs.
  Sie ist aber auch notwendig, um Programmcode und Programmierschnittstellen zu dokumentieren und um in einem Entwicklerteam über Programmcode, Probleme und ihre Lösungen zu diskutieren.
  Die Vorlesung soll vermitteln, nach welchen Kriterien wir geeignete Algorithmen und Datenstrukturen auswählen. Sie soll Ihnen eine informierte Entscheidung ermöglichen, indem sie Ihnen Faktenwissen über wichtige grundlegende Algorithmen und Datenstrukturen vermittelt. Diese Fertigkeiten sind nicht nur wichtig für Schritt 2 und 3 des Algorithmenentwurfs, sie sollen Ihnen auch helfen, sich in den Standardbibliotheken wichtiger Programmiersprachen zurecht zu finden.
  Haben wir uns für einen Algorithmus entschieden, müssen wir uns vergewissern, dass unsere Wahl korrekt ist, also unser Algorithmus tatsächlich die Aufgabenstellung löst.
  Schließlich möchten wir auch verlässliche Aussagen über die Eigenschaften unseres Algorithmus treffen können; etwa, wieviele Rechenschritte benötigt werden, um zur Lösung zu gelangen. Das ermöglicht es uns, verschiedene Algorithmen zu vergleichen.


Binäre Suche: Eine beispielhafte Algorithmenanalyse

Als einführendes Beispiel betrachten wir einen Algorithmus, der bereits in der Programmierung im ersten Semester kurz vorgestellt wird.
Stellen wir uns vor, dass wir eine Software entwickeln und sich in der aktuellen Revision 100 ein Fehler aufgetreten ist.
Zufällig wissen wir, dass der Fehler in Revision 0 noch nicht vorhanden war.
Welches ist die früheste Revision, die mit dem Fehler behaftet ist?
Schauen wir uns zunächst Revision 50 an - sagen wir, dort tritt der Fehler nicht auf.
Er wurde also zwischen Revision 50 und Revision 100 eingeführt.
Als nächstes betrachten wir Revision 75. Hier tritt der Fehler auf, so dass wir nun zwischen den Revisionen 50 und 75 weitersuchen müssen.
Wir fahren fort bis wir unseren Suchbereich auf eine einzige Revision eingeschränkt haben. Hier muss der Fehler zum ersten Mal aufgetreten sein (oder es gibt gar keinen Fehler und wir haben nur vergessen neu zu kompilieren).
Dieses Verfahren heißt binäre Suche.

Binäre Suche als Pseudocode
Eine binäre Suche funktioniert natürlich nicht nur für Software-Revisionen.
Wir können den Algorithmus genauso gut verwenden, um z.B. einen Namenseintrag in einem Telefonbuch zu suchen oder zu entscheiden, ob ein sortiertes Array  eine gegebene Zahl  enthält.
Dazu vergleichen wir  mit dem Eintrag in der Mitte von : Ist der Eintrag größer als , suchen wir links von der Mitte weiter; ist er kleiner, suchen wir rechts. Ist der Eintrag gleich , sind wir fertig.
Abbildung  zeigt ein Beispiel.

example-binary-search


Ein Beispiel für eine binäre Suche. Wir suchen das Element 32 in einer
sortierten Folge mit 14 Zahlen.
Der aktuelle Suchbereich ist hellblau (), das mittlere
Element dunkelblau () markiert. Wir beobachten, dass
sich die Größe des Suchbereiches in jeder Iteration des Verfahrens mindestens
halbiert und dass nur Bereiche ausgeschlossen werden, die
das gesuchte Element nicht enthalten können.

Als Vorbereitung auf die kommenden Kapitel werden wir den Algorithmus einmal beispielhaft analysieren.
Dazu müssen wir als ersten Schritt präzisieren, wie der Algorithmus funktionieren soll indem wir ihn in Pseudocode notieren.[caption=Binäre Suche auf einem sortierten Array]
  Eingabe: aufsteigend sortiertes A der Länge n
           eine Zahl x als Suchschlüssel
  Ausgabe: ein Index i mit A[i]=x oder n,
           wenn kein solcher Index existiert

  function binaereSuche(A, x):
    links = 0
    rechts = A.laenge - 1

    # Invariante: Wenn x in A, dann in A[links...rechts]
    while links <= rechts: 
      mitte = (links + rechts) div 2
      if A[mitte] < x:     
        links = mitte + 1
      else if A[mitte] > x:
        rechts = mitte - 1
      else:
        return mitte       

    return A.laenge
Unsere Formaliserung ist keineswegs die einzig mögliche: Zum Beispiel gäbe es
auch andere Pseudocode-Schreibweisen oder wir könnten auch ein tatsächliches
Javaprogramm schreiben.
Die Formalisierung erklärt aber die Details des Verfahrens; z.B. legt sie genau
fest, was wir mit der Mitte von  meinen (das ist z.B. nicht offensichtlich,
wenn  eine gerade Anzahl von Einträgen enthält).




Korrektheit unserer Implementierung
Als ersten Teil der Analyse weisen wir nach, dass unsere Implementierung auf
jeder zulässigen Eingabe das gewünschte Ergebnis liefert, also korrekt
funktioniert.
Der Beweis mag hier unnötig lang anmuten, aber er demonstriert eine wichtige
Beweistechnik.

Zunächst müssen wir nachweisen, dass das Verfahren terminiert (stoppt).
Dazu argumentieren wir, dass mit jedem Durchlauf der while-Schleife
(einen solchen Durchlauf nennen wir im Folgenden auch eine Iteration)
der Suchbereich A[links...rechts] kleiner wird.
Da der Bereich zu Beginn aus  vielen Elementen besteht und das
Verfahren spätestens stoppt, wenn wir den Suchbereich auf ein einzelnes
Element reduziert haben, muss die binäre Suche also irgendwann stoppen.

Die binäre Suche terminiert für alle gültigen Eingaben.

Sofern das Verfahren nicht vorzeitig stoppt, steigt in jeder Iteration
entweder der Wert von links um mindestens 1 oder der Wert von
rechts sinkt um mindestens 1.
Da aber sowohl links wie auch rechts rechts mit einem
endlichen Wert initialisiert werden muss nach endlich vielen Iterationen die
Bedingung der while-Schleife in Zeile verletzt werden oder das Verfahren stoppt vorzeitig.
Unser Algorithmus terminiert also immer und wir müssen nur noch nachweisen, dass
er auch die korrekte Antwort liefert.

  Die binäre Suche liefert  zurück, falls  den Suchschlüssel
   nicht enthält.

Enthält  den Suchschlüssel  nicht, werden die Vergleiche in den Zeilen
- nie ergeben,
dass  ist und die Implementierung gibt korrekterweise 
zurück.
Intuitiv funktioniert die binäre Suche, weil wir nur Teile von  von der Suche
ausschließen, die  sicher nicht enthalten.
Diese Idee möchten wir jetzt formal fassen.
Dabei stoßen wir aber auf ein Problem: Wir müssen argumentieren, dass keine
Verkleinerung des Suchbereiches die Arrayposition ausschließt, an der sich 
befindet.
Wir müssen also eine Aussage treffen, die zu jedem Zeitpunkt während der
Ausführung des Algorithmus gilt.
Eine solche Aussage nennen wir eine Invariante.
Konkret argumentieren wir, dass eine von zwei Möglichkeiten eintreten muss:

 Der Suchschlüssel  befindet sich nicht in .
 Während der gesamten Ausführung des Algorithmus sind links
 und rechts so gewählt, dass der Suchschlüssel  im Teilarray
 A[links...rechts] liegt.
Dazu beobachten wir, dass links und rechts nur innerhalb der
while-Schleife verändert werden.
Wir zeigen also:

  Unsere initiale Wahl von links und rechts
erfüllt unsere Behauptung.
  In jeder Iteration der werden links
  und rechts so verändert, dass  weiterhin im Suchbereich A[links...rechts]
  liegt - vorausgesetzt,  lag schon vor der Iteration im Suchbereich.
Mit dieser Strategie gewappnet führen wir jetzt einen formalen Beweis.
Bei jedem Durchlauf des Schleifenkopfes der while-Schleife in Zeile
 gilt für die aktuelle
Werte von  und :
Wenn  in  enthalten ist, dann ist  auch im Teilarray
 enthalten.

Wir führen einen Induktionsbeweis über die Iterationen der in
Zeile .
Für den Induktionsanfang zeigen wir, dass die Invariante gilt, bevor der
Schleifenrumpf zum ersten Mal durchlaufen wird:
Tatsächlich gilt unsere Behauptung beim ersten Durchlauf des Kopfes der
while-Schleife, denn hier ist .

Für den Induktionsschritt müssen wir nachweisen, dass die Schleifeninvariante nach einer beliebigen Iteration des
Schleifenrumpfes weiterhin gilt, sofern sie zu Beginn der Iteration im Schleifenkopf galt.
Wir sagen: Die Iteration erhält die Invariante aufrecht.

Betrachten wir eine beliebige Iteration und gehen wir davon aus, dass die Invariante im Schleifenkopf vor der Iteration gilt.
Ist in dieser Iteration , muss  in  liegen, da  aufsteigend sortiert ist.
Wir erhalten also die Invariante, da wir  setzen.
Ist umgekehrt , muss  in  liegen.
Wir erhalten die Invariante, da wir in diesem Fall  setzen.
Auch wenn wir davon ausgehen, dass die Invariante vor der betrachteten Iteration gilt, genügt diese Argumentation
um die Korrektheit nachzuweisen, denn ganz am Anfang des Beweises haben wir überlegt, dass die Invariante vor der
ersten Iteration gilt. Die erste Iteration erhält nach unserer Argumentation die Invariante (wir hatten schließlich
nachgewiesen, dass jede Iteration das tut - insbesondere die erste). Also gilt die Invariante nach der ersten
Iteration und damit auch vor der zweiten Iteration. Die zweite Iteration erhält ebenfalls die Invariante,
so dass sie auch vor der dritten Iteration gilt usw. Auch in den folgenden Kapiteln werden wir häufig die Korrektheit
von Algorithmen beweisen, indem wir die Einhaltung einer Invarianten in einem Induktionsbeweis nachweisen.
Mit Hilfe der Invarianten können wir nun die Korrektheit des Verfahrens zeigen.

  Enthält  den Suchschlüssel , so liefert die binäre Suche einen Index  mit .

Stoppt die binäre Suche vorzeitig mit  in Zeile 18 ist nach Konstruktion .
Anderenfalls ist in der letzten Iteration der while-Schleife  und  enthält nur noch ein Element .
Unsere Invariante  liefert, dass dieses Element  ist.
Außerdem gibt uns der Korrektheitsbeweis Aufschluss darüber, unter welchen Bedingungen der Algorithmus funktioniert.
Für den Induktionsanfang setzen wir voraus, dass der Suchbereich endlich ist und wir eine linke und rechte Grenze kennen. Für den Induktionsschritt benötigen wir, dass die Zahlen sortiert sind.
In unserem einführenden Beispiel hatten wir ähnliche Annahmen gemacht: Hier gehen wir davon aus, dass wir eine Revision ohne Fehler und eine Revision mit Fehler kennen und begrenzen so den Suchbereich. Außerdem gehen wir implizit davon aus, dass die Revisionen nach Fehlerfreiheit sortiert sind: Damit unsere Suche funktioniert, müssen zunächst alle fehlerfreien und dann erst alle fehlerbehafteten Revisionen kommen.
Diese Monotonieeigenschaft wird immer benötigt, wenn eine binäre Suche durchgeführt werden soll.

Unser Rechnermodell: Random Access Machines

Um die Laufzeit der binären Suche abzuschätzen könnten wir den Algorithmus in einer Programmiersprache unserer Wahl implementieren und auf einem Computer ausführen.
Dieses Vorgehen birgt aber einige Probleme: Wie lange der Algorithmus braucht würde davon abhängen, wie gut unsere Implementierung ist und auf welchem Computer wir sie ausführen.
Darüberhinaus müsste eine Analyse eines tatsächlichen Computers unzählige Faktoren wie die Prozessortaktung oder die Geschwindigkeit und Größe der einzelnen Ebenen der Speicherhierarchie berücksichtigen.

Anstatt also eine konkrete Laufzeit auf einem konkreten Computer zu messen zählen wir die Anzahl der durchgeführten (elementaren) Rechenoperation in einem allgemeingültigeren Computermodell.
Dazu reduzieren wir Computer zunächst auf ihre wesentlichen Bestandteile. Wir betrachten arithmetische Operationen, Speicherzugriffe, Sprünge und Vergleiche als elementare Operationen.
Listing  zeigt als Beispiel eine mögliche Übersetzung unseres Pseudocodes in die elementaren Operationen eines MIPS-Assemblers;
ohne notwendigerweise die Bedeutung der elementaren Operation zu kennen sehen wir, dass die Zeilen unseres Pseudocodes in eine oder mehrere elementare
Operationen übersetzt werden - die genaue Anzahl kann zwischen unterschiedlichen Übersetzungen variieren.

Zur Vereinfachung gehen wir des Weiteren davon aus, dass alle elementaren Operationen die gleiche Laufzeit besitzen.
Zum Beispiel nehmen wir an, dass eine Multiplikation genauso lange dauert wie eine Addition und dass der Zugriff auf jede Speicherzelle die gleiche Zeit benötigen soll
(wir ignorieren also die Existenz der Speicherhierarchie aus Caches, Arbeitsspeicher und Festplatten/SSDs und deren Suchzeiten und Anbindung an den Prozessor).
Außerdem besitzen in unserem Computermodell alle elementaren Operationen eine konstante Laufzeit.
Das bedeutet, dass die Zeit, die zur Durchführung einer elementaren Operation benötigt wird unabhängig von der Größe der  beteiligten Operanden sein soll:
Zum Beispiel nehmen wir an, dass die Laufzeit einer Multiplikation nicht davon abhängt, wie groß die multiplizierten Zahlen sind.
Schließlich nehmen wir an, dass unser Prozessor alle Operationen sequentiell (also nicht parallel) abarbeitet.
Dieses Modell eines Computers heißt random access machine (RAM).
Es ist hinreichend genau, um aussagekräftige Vergleiche und Vorhersagen über Algorithmen machen zu können, aber so einfach, dass es sich für theoretische Analysen eignet.

Eine erste Laufzeitanalyse

Wieviele elementare Rechenoperationen unsere binäre Suche auf einer beliebigen Eingabe  und  durchführt, hängt wesentlich davon ab, wieviele Iteration die while-Schleife in Zeile 11 durchläuft.
Nennen wir diese Anzahl .
Wenn wir davon ausgehen, dass die -te Zeile gerade  viele elementare Rechenoperationen (diese Anzahl ist in jeder Zeile konstant) enthält haben wir:




Die wichtige Beobachtung ist hier, dass die Zeilen 11-18 in jeder Iteration der while-Schleife, also  mal, ausgeführt werden.
Insgesamt erhalten wir eine Laufzeit von
*
mit geeignet gewählten Konstanten  und .
Die Anzahl  der Iterationen der while-Schleife hängt von der konkreten Eingabe  und  ab:
Wenn wir Glück haben, befindet sich  genau in der Mitte von  und das Verfahren stoppt vorzeitig nach einer einzigen Iteration der while-Schleife. In diesem Fall führt unser Algorithmus eine konstante Anzahl von Rechenoperationen durch.
Ebenso kann es aber passieren, dass wir  erst finden, wenn  ist oder dass das Verfahren in Zeile 20 endet, weil  nicht in  enthalten ist.
Wir nehmen eine pessimistische Sichtweise ein und untersuchen, wieviele Iterationen der Algorithmus im schlimmsten Fall (englisch: im worst-case) durchführen wird.
Wir beobachten, dass der Schleifenrumpf spätestens dann zum letzten Mal ausgeführt wird, wenn die Länge  des verbleibenden Suchbereiches  zu Beginn des Schleifendurchlaufs eins beträgt:
Dann wird entweder die Schleife enden, weil  ist oder am Ende des Durchlaufs ist .

Zu Beginn des ersten Schleifendurchlaufs beträgt die Länge des Suchbereiches .
In jeder Iteration wird die Länge des Suchbereiches aber mindestens halbiert, da wir stets die linke oder die rechte Hälfte von  verwerfen können.
Spätestens nach  Halbierungen besitzt also der Suchbereichs zu Beginn der Schleife noch Größe .
Inklusive dieses - dann letzten - Durchlaufs macht die Schleife also höchstens  viele Iterationen
und wir schlussfolgern, dass  ist.
Formaler: Zu Beginn der -ten Iteration der while-Schleife ist .
Wir rechnen aus, für welche  der Term  auf 1 fällt:
*
Im schlimmsten Fall wird also eine binäre Suche auf  sortierten Elementen Zeit  benötigen.

Insbesondere haben wir herausgefunden, dass die worst-case Laufzeit der binären Suche davon abhängt, wie lang die Eingabefolge ist. Je länger die Eingabefolge ist, je mehr Elementaroperationen werden potentiell benötigt.

Wieviele Elementaroperationen das genau sind, interessiert uns hingegen nicht: Wir verzichten darauf, die Konstanten  und  aus unserer Analyse berechnen, denn die tatsächliche Anzahl an Elementaroperationen, die der Algorithmus auf einem realen Computer benötigt hängt unter anderem vom verwendeten Compiler, der Hardwarearchitektur und dem Betriebssystem ab.
Unser abstraktes RAM Modell ist nicht genau genug, um eine sinnvolle Vorhersage zu liefern.
Stattdessen interessieren wir uns nur für die asymptotische Laufzeit, also dafür, wie sich die Laufzeit in Abhängigkeit einer steigenden Eingabelänge entwickelt.
Wir sagen: Die asymptotische worst-case Laufzeit der binären Suche wächst logarithmisch in  und schreiben  oder  um anzudeuten, dass das Laufzeitwachstum von logaritmischer Ordnung ist.

[caption=Binäre Suche in MIPS-Assemblercode, label=lst:binsearch-asm, morekeywords=add,li,div, mult,lw,j,sub,bgt, blt, move,]code/binsearch.asm

Vereinfachung: Die 
example-o-notation
  
 Veranschaulichung der  Wir betrachten  und .
 Es gilt , aber  beschränkt  nicht.
 Auch nach Multiplikation mit  liegt  nicht überall über .
 Wir finden allerdings ein  so, dass  für alle  gilt.

Wir verallgemeinern unser intuitives Verständnis des Wachstums einer Laufzeitfunktion aus dem vorangehenden Abschnitt etwas und verwenden die um zu beschreiben, wie stark eine Laufzeitfunktion  wächst, wenn die Eingabelänge  gegen unendlich läuft.
Dabei interessieren wir uns stets für die Laufzeit im schlimmsten Fall (worst-case).
Wir definieren zunächst die łorst-case Laufzeit in Abhängigkeit von der Eingabelänge .

  Für einen gegebenen Algorithmus  und für alle  sei  die
  Menge aller gültigen Eingaben der Länge  für .
  Außerdem sei  die Anzahl an Elementaroperationen, die  auf der Eingabe 
  ausführt.
  Dann ist die worst-case Laufzeit auf Eingabelänge  gegeben durch:
  *
Die asymptotische worst-case Laufzeit können wir dann mit der -Notation beschreiben.
  Seien  und  zwei Funktionen.
  Wir sagen  wächst höchstens so schnell wie  und schreiben  falls es eine Konstante  und ein  so gibt, dass  für alle  gilt.
Abbildung  veranschaulicht die Definition.
Es kann passieren, dass  gilt, obwohl  für alle 
gilt. Das passiert zum Beispiel, wenn wir  und  wählen.
Damit die Definition trotzdem wie gewünscht funktioniert, fordern wir lediglich,
dass es eine Konstante  gibt, so dass  größer als  ist,
sofern wir jedenfalls hinreichend große  betrachten.
Hier könnten wir etwa  wählen, denn dann ist 
für alle .
Wir betrachten ein weiteres Beispiel, das auch zeigt, wie wir ein passendes
 und  finden können.

Intuitiv sollte  sein.
Wir möchten nachweisen, dass diese Intuition korrekt ist.
Dazu müssen wir zeigen, dass ein  und ein  so existieren, dass
*
Für  ist  und  ist eine monoton steigende Funktion.
Daher ist  monoton fallend und es gilt für alle , dass
*
ist.
Wir wählen also  und  und sind fertig.
Analog zu Definition  definieren wir weitere Wachstumsrelationen.

  Seien  und  zwei Funktionen.
  
  Wir sagen  wächst mindestens so schnell wie  und schreiben  falls es eine Konstante  und ein  so gibt, dass  für alle  gilt.
  Wir sagen  wächst genauso schnell wie  oder  wächst proportional zu  und schreiben  falls  und  gilt.
  Wir sagen  wächst echt langsamer als  und schreiben  falls es für alle Konstanten  ein  so gibt, dass  für alle  gilt.
  Wir sagen  wächst echt schneller als  und schreiben  falls es für alle Konstanten  ein  so gibt, dass  für alle  gilt.
  Die Definitionen von  und  sagen aus: Egal welches  wir betrachten, es wird sich  von  um mehr als einen Faktor  unterscheiden, sofern wir nur  groß genug wählen.

Wir schließen dieses Kapitel mit einer Übersicht über häufig verwendete Ordnungen von Laufzeitfunktionen und deren Verhältnis.



























Ziele dieses Kapitels
Am Ende dieses Kapitel haben Sie Folgendes gelernt.

  Sie wissen, was ein Algorithmus ist und wissen, warum wir Algorithmen präzise aufschreiben möchten.
  Sie können das RAM Modell beschreiben.
  Sie wissen, wie eine binäre Suche funktioniert, kennen ihre asymptotische worst-case Laufzeit und wissen, unter welchen Voraussetzungen eine binäre Suche durchgeführt werden kann.
  Sie wissen, was eine Schleifeninvariante ist und kennen ein Beweisverfahren, um Schleifeninvarianten zu beweisen.
  Sie wissen, welche Schritte für eine Laufzeitanalyse nötig sind.
  Sie kennen die Bedeutung der Symbole  und  der -Notation.
  Sie können die Ordnung von Laufzeitfunktion abschätzen.
  Sie wissen, warum wir auf die -Notation zurückgreifen.


  
    [Java] Im finden Sie eine binäre Suche für Arrays als java.util.Arrays.binarySearch und für Collections als java.util.Collections.binarySearch.
    [Python] Die enthält verschiedene Varianten einer binäre Suche im Modul bisect.
    [C] In der findet sich eine binäre Suche als Funktion bsearch im Header stdlib.h.
    [C++] Die enthält im Header algorithm die Funktionen std::binary_search, std::lower_bound und std::upper_bound mit Implementierungen einer binären Suche.
  


Erste Algorithmen: Sortierverfahren
Egal ob ein Onlineshop seine Produkte nach Preis sortiert, ein Client Emails sortiert nach Eingangsdatum darstellt oder ein Dateibrowser Dateien alphabetisch aufführt: In vielen Anwendungen moderner Software ist es notwendig, Daten zu sortieren.
In diesem Kapitel lernen wir klassische Sortieralgorithmen kennen.
Dabei verfolgt das Kapitel mehrere Ziele:

  Es untersucht die Stärken und Schwächen der Sortieralgorithmen und stellt heraus, in welchen Situation sich welcher Algorithmus besonders eignet.
  Es wiederholt und vertieft die Vorgehensweise bei der Algorithmenanalyse.
  Es vermittelt eine erste Intuition dafür, was unterschiedliche Ordnungen von Laufzeitfunktionen für die praktische Lösbarkeit von Problemen bedeutet.
Bevor wir aber Sortieralgorithmen studieren können, müssen wir spezifizieren, welches Problem diese Algorithmen lösen sollen.










Sortierproblem
  Ein Array  mit  Zahlen
  Eine Permutation (Umordnung) der Zahlen als Array  mit
  *

Natürlich müssen wir nicht unbedingt Zahlen sortieren. Sofern nicht anders angegeben
funktionieren die hier besprochenen Verfahren auch für andere Folgen von Elementen,
sofern wir die -Relation sinnvoll definieren können: Dazu müssen wir für je zwei
beliebige Elemente der Folge entscheiden können, welches kleiner (bzw. nicht größer)
ist.
Im folgenden Text gehen wir davon aus, dass die Zahlen alle paarweise verschieden sind.
Auch diese Annahme stellt keine Einschränkung dar, vereinfacht aber die folgenden
Beschreibungen.

Sortieren durch Auswahl - SelectionSort

Wir untersuchen als erstes das vielleicht natürlichste Sortierverfahren:
Nacheinander suchen wir zunächst das kleinste, dann das zweitkleinste, dann das
drittkleinsteElement, bis wir die gesamte Zahlenfolge sortiert haben.
Abbildung  zeigt das Verfahren an einem Beispiel.


  example-selection-sort
  
   Ein Beispieldurchlauf von SelectionSort.
  Der Algorithmus teilt das Eingabearray in zwei Bereiche.
  Der vordere, hellblaue () Bereich ist bereits sortiert.
  Der hintere, blaue () Bereich ist potentiell unsortiert.
  In jeder Iteration sucht der Algorithmus das kleinste Element ()
  im unsortierten Bereich und tauscht es an den Beginn des unsortierten Bereiches.
  Auf diese Weise wächst der sortierte Bereich in jeder Iteration um ein Element,
  bis schließlich das gesamte Array sortiert ist.

Präziser: Auf einem Array mit  Elementen macht SelectionSort 
Iterationen.
In Iteration  sucht das Verfahren das Element, das in einem sortierten Array
an Stelle  stehen würde und schreibt es an die Position .
Wir schreiben das Verfahren in Pseudocode in Listing auf:
Die Methode selectionSort erwartet ein Array mit  Zahlen.
Ein Durchlauf der in Zeile  entspricht
jeweils einer Zeile in Beispiel .
Der Rumpf der dient dazu, das kleinste Element im unsortierten
Bereich zu finden; der Index dieses Elements wird in der Variablen minpos
gespeichert.
Abschließend tauschen wir in Zeile  das kleinste Element
im unsortierten Bereich an das Ende des sortierten Bereiches, der damit um eine
Position wächst.


[caption=SelectionSort, float, label=lst:sorting:selection-sort]
Eingabe: Ein Array A von n Zahlen
Ausgabe: Eine Umordnung von A so, dass
         A[0] <= A[1] <= ... <= A[n-1].

function selectionSort(A):
  for i=0,...,n-2: 
    # Suche das kleinste Element in A[i,...,n-1]
    min_pos = i
    for j=i+1,...,n-1:      
      if A[j] < A[min_pos]: 
        min_pos = j         

    # tausche ans Ende des sortierten Bereiches
    swap(A[i], A[min_pos])  

  return A

Wir möchten nun beweisen, dass das Verfahren wie gewünscht funktioniert und
zeigen dazu, dass der Pseudocode die folgenden beiden Eigenschaften erfüllt:

Bei jedem Durchlauf des Schleifenkopfes der in
Zeile  gilt für das aktuelle :
Das Array  ist sortiert.
Bei jedem Durchlauf des Schleifenkopfes der in
Zeile  gilt für das aktuelle :
Alle Elemente in  sind größer als alle Elemente in .

Initialisierung:
Wenn der Schleifenkopf zum ersten Mal erreicht wird ist .
Invariante  gilt, denn als leeres
Array ist  sortiert. Auch gilt
Invariante  für das leere Array.

Erhaltung:
Für den Induktionsschritt betrachten wir einen beliebigen Durchlauf des
Schleifenrumpfes der äußeren in Zeile mit einem  und nehmen als Induktionsvoraussetzung an,
dass beide Invarianten zu Beginn der Iteration im Schleifenkopf galten.
Wir zeigen, dass sie dann immer noch gelten, wenn der Schleifenkopf wieder
erreicht wird.
Zunächst beobachten wir, dass der Schleifenrumpf das kleinste Element
in  sucht und an Position  tauscht, verzichten aber darauf,
diese Beobachtung zu beweisen.
Nach Induktionsvoraussetzung ist zu Beginn der Iteration das Teilarray
 sortiert und alle Elemente in  sind größer als
alle Elemente in , insbesondere das kleinste Element 
in .
Tauschen wir also  an Stelle , so ist  am Ende
der Iteration sortiert.
Da  das kleinste Element in  ist, gilt weiterhin,
dass alle Elemente in  kleiner sind als alle Elemente in 
und Invariante  bleibt erhalten.

Damit gelten beide Invarianten auch nach der letzten Iteration der Wir beobachten, dass die Ausgabe von SelectionSort eine Permutation der
ursprünglichen Eingabe ist und müssen uns nur noch überzeugen, dass diese
Permutation eine sortierte Reihenfolge hervorbringt.
Invariante  garantiert uns, dass am Ende
des Algorithmus das Teilarray  sortiert ist(Hier und
im Folgenden gehen wir davon aus, dass der Schleifenkopf nach der letzten Iteration
noch ein letztes Mal durchlaufen wird - hier mit  - um festzustellen, dass die Schleife beendet ist.
Der Induktionsbeweis sichert uns dabei zu, dass auch die letzte Schleifeniteration die Invarianten
erhält und sie daher auch bei diesem letzten Durchlauf des Schleifenkopfes gelten.).
Es wäre also noch denkbar, dass das letzte Element  nicht am richtigen
Platz steht; das wäre aber ein Widerspruch zu Invariante ,
die sicherstellt, dass bei Beendigung alle Elemente in  kleiner sind als .

Für die Laufzeitanalyse von SelectionSort verzichten wir auf eine ausführliche
zeilenweise Aufstellung der Anzahl von Elementaroperationen, wie wir sie bei der
binären Suche durchgeführt haben. Es ist aber eine gute Übung, das folgende
Argument auszuformulieren.

SelectionSort besitzt eine asymptotische worst-case Laufzeit von .
Hierbei bezeichnet  die Länge des Eingabearrays .

  Der Schleifenrumpf (Zeilen +)
  der inneren verursacht konstanten Aufwand, so dass es für die
  Analyse ausreichend ist, nur die  Anzahl der Iterationen der Schleife zu
  zählen.
  Diese betragen für ein festes  genau .
  Da  von  bis  läuft und alle sonstigen Operation im Rumpf der
  äußeren nur konstanten Aufwand verursachen, haben wir eine
  Gesamtlaufzeit, die proportional zu










  *
  ist.
Für den Beweis haben wir die Gaußsche-Summenformel formulas:gauss aus
dem Anhang benutzt, die uns noch häufiger begegnen wird.

Als einfache Daumenregel für die worst-case Laufzeit können wir uns merken,
dass die innere  Iterationen macht, um das kleinste Element
aus  Elementen zu finden. In den ersten  Iterationen der äußeren
müssen wir jeweils das Minimum aus mindestens  Elementen
finden; insgesamt benötigen wir also mindestens Zeit .
Der obige Beweis zeigt, dass die asymptotische worst-case Laufzeit auch nicht
schlechter als  ist.

Sortieren durch Einfügen - InsertionSort

InsertionSort verfolgt eine ähnliche Idee wie SelectionSort und verwaltet im
vorderen Teil von  ein sortiertes Teilarray.
Anders als SelectionSort suchen wir aber nicht in jeder Iteration das kleinste
Element des unsortierten Bereiches.
Stattdessen sortieren wir in jeder Iteration das erste Element des unsortierten
Bereichs im vorderen, sortierten Bereich ein wie das Beispiel in
Abbildung  zeigt.


  example-insertion-sort
  
  Ein Beispieldurchlauf von InsertionSort.
  Der Algorithmus teil das Eingabearray in zwei Bereiche.
  Der vordere, hellblaue () Bereich ist bereits sortiert.
  Der hintere, blaue () Bereich ist potentiell unsortiert.
  In jeder Iteration sortiert der Algorithmus das vorderste Element ()
  des unsortierten Bereiches im vorderen, sortierten Bereich ein.
  Das einsortierte Element ist grau () markiert.
  Auf diese Weise wächst der sortierte Bereich in jeder Iteration um ein Element,
  bis schließlich das gesamte Array sortiert ist.

Wir präzisieren den Algorithmus in Listing .
Auch hier entspricht die äußere in Zeile einer Zeile in Beispiel .
Der Rumpf der Schleife muss das Element an Position A[i] im bereits
sortierten Bereich einsortieren.
Wir speichern dieses einzusortierende Element in key.
Dann wandern wir mit Hilfe der in Zeile von hinten nach vorne durch den sortierten Bereich, solange bis wir ein Element finden,
dass kleiner oder gleich dem einzusortierende Element key ist oder wir
den gesamten sortierten Bereich durchlaufen haben.
Dabei schieben wir Elemente, die größer als key sind jeweils um eine Position
nach rechts, um Platz für das Einfügen von key zu schaffen.
[caption=InsertionSort, label=lst:sorting:insertion-sort]
Eingabe: Ein Array A von n Zahlen
Ausgabe: Eine Umordnung von A so, dass
         A[0] <= A[1] <= ... <= A[n-1].

function insertionSort(A):
  for i=1,...,n-1: 
    # fuege A[i] in A[0...i-1] ein
    key = A[i], j=i-1
    # solange das einzufuegende Element vor A[j] gehoert
    while j >= 0 and key < A[j]: 
      # verschiebe A[j] nach rechts
      A[j+1] = A[j]
      -j

    A[j+1] = key 
  return A

Jedes Mal, wenn der Schleifenkopf der in Zeile durchlaufen wird gilt für das aktuelle :
Das Teilarray  ist sortiert.



Initialisierung:
Die Invariante gilt vor dem ersten Durchlauf des Schleifenkopfs (demjenigen mit ),
denn hier ist  als Array mit einem einzigen Element per Definition
sortiert.

Erhaltung:
Für den Induktionsschritt zeigen wir, dass ein Durchlauf des Schleifenrumpfes
die Invariante erhält, sofern sie zu Beginn des Durchlaufs im Schleifenkopf galt.
Dazu betrachten wir eine beliebige Iteration mit einem .
Wir beobachten, dass der Schleifenrumpf das Element  vor das
rechteste Element in  verschiebt, das kleiner als  ist
und ansonsten die Reihenfolge der Element in  nicht
verändert(Diese Aussage könnten wir mit einer geeigneten
Schleifeninvarianten präziser zeigen, darauf verzichten wir hier aber.).
Nach der Induktionsvoraussetzung ist  zu Beginn des Rumpfes
sortiert, so dass  an die richtige Stelle eingefügt wird und damit
 am Ende des Schleifenrumpfes sortiert ist.





Wie zuvor beobachten wir, dass die Ausgabe von InsertionSort eine Permutation der
ursprünglichen Eingabe ist.
Da aber nach dem letzten Durchlauf des Schleifenrumpfes gemäß der Invarianten die
Elemente  sortiert sind, arbeitet InsertionSort korrekt.

Abschließend analysieren wir die Laufzeit des Verfahrens.

  InsertionSort besitzt eine asymptotische worst-case Laufzeit von
  .
  Hierbei bezeichnet  die Länge des Eingabearrays .

Der Schleifenrumpf der inneren in Zeile verursacht pro Iteration konstanten Aufwand, so dass wir nur die Anzahl der Iterationen der Schleife
zählen.
Sei  diese Anzahl an Iterationen, die die innere für ein
festes  durchläuft.
Abgesehen von der inneren entsteht im Rumpf der äußeren
nur konstanter Aufwand; ebenso ensteht nur konstanter Aufwand
durch die Return-Anweisung.
Damit ist die Gesamtlaufzeit proportional zu
*
und wir müssen  abschätzen.
In jeder Iteration der haben wir das Element  spätestens
nach  Iterationen der auf Position 0 verschoben.
Es gilt also .
Damit kommen wir auf eine Gesamtlaufzeit von
*
indem wir erneut die Gaußsche Summenformel  verwenden.

Für die Aussage des Theorems müssen wir noch zeigen, dass die asymptotische
worst-case Laufzeit von InsertionSort auch  ist; wir müssen also
nachweisen, dass unsere Abschätzung nicht unnötig konservativ ist.
Dazu betrachten wir ein beliebiges absteigend sortiertes Array  mit 
Elementen.
Hier ist in jeder Iteration  kleiner als alle Elemente in 
und muss daher um  Positionen verschoben werden.
Im schlimmsten Fall ist also tatsächlich  für alle .

Ein vorläufiger Vergleich.
Nach unseren beiden Analysen halten wir fest, dass SelectionSort im schlimmsten
und im besten Fall eine asymptotische Laufzeit von  besitzt.
Im Gegensatz zu InsertionSort muss SelectionSort aber nur zweimal pro Iteration
schreibend auf  zugreifen und macht daher insgesamt höchstens 
Schreibzugriffe auf  -  im besten Fall sogar weniger.
InsertionSort macht im schlimmsten Fall  Schreibzugriffe pro
Iteration, und damit  Schreibzugriffe insgesamt.
Auf der anderen Seite profitiert InsertionSort von vorsortierten Folgen:
Ist  z.B. bereits sortiert, ist in der obigen Analyse  für alle
 und InsertionSort arbeitet in diesem besten Fall in Zeit .
Dieser Zusammenhang wird in den Übungen genauer untersucht.



Divide-and-Conquer: MergeSort
Der MergeSort-Algorithmus löst das Sortierproblem nach dem
Divide-and-Conquer-Prinzip.
(In der deutschen Literatur wird das Prinzip auch manchmal
Teile-und-Herrsche genannt)
Er zerlegt die Eingabe solange in kleinere Teilprobleme, bis sich eine einfache
Lösung für die Teilprobleme ergibt. Die Schwierigkeit liegt darin, die Lösungen
der Teilprobleme zu einer Gesamtlösung zusammenzusetzen.

Konkreter teilt MergeSort das Eingabearray in zwei Hälften und sortiert diese
mit je einem rekursiven Aufruf unabhängig voneinander.
Anschließend verschmilzt (Englisch: to merge) der Algorithmus
die beiden sortierten Teilarrays zu einem insgesamt sortierten Array.
Die Rekursion wird dabei solange fortgesetzt, bis die beiden Hälften jeweils
höchstens ein Element enthalten und also per Definition sortiert sind.
Der Pseudocode für diesen Algorithmus steht in Listing und Abbildung  zeigt einen Beispieldurchlauf.
Um ein Array  der Länge  mit dem Algorithmus zu sortieren rufen wir mergeSort(A,0,n-1)
auf.

[caption=MergeSort, float,label=lst:sorting:merge-sort]
Eingabe: Ein Array A von n Zahlen
Ausgabe: Eine Umordnung von A so, dass
         A[0] <= A[1] <= ... <= A[n-1].

# Sortiert die Einträge l,...,r von A aufsteigend
function mergeSort(A, l, r):
  # Sofern A[l,...,r] mindestens zwei Elemente enthält:
  if l < r:                                                 
    m = (l+r) div 2       # Mitte von A[l,...,r]              
    mergeSort(A, l, m)    # sortiere A[l,...m] rekursiv       
    mergeSort(A, m+1, r)  # sortiere A[m+1,...,r]             
    merge(A, l, m, r)     # verschmelze sortierte Teilarrays  


    example-merge-sort
  
  
  Ein Beispieldurchlauf von MergeSort auf acht Elementen.
  Der Algorithmus teilt das Eingabearray zunächst rekursiv solange auf,
  bis jedes Teilarray nur noch ein Element enthält.
  Bei jeder Aufteilung halbiert sich die Arraygröße, so dass wir stets
  zwei rekursive Aufrufe - einen für das linke und einen für das rechte
  Teilarray - generieren.
  In der zweiten Hälfte des Rekursionsbaumes kehren stets zwei rekursive Aufrufe
  mit je einem sortierten Teilarray zurück.
  MergeSort verschmilzt diese beiden Teilarrays zu einem sortierten Teilarray
  mit Hilfe der Methode merge.


  empty
    example-merge
  
  
  Ein Beispieldurchlauf von merge: Links das Eingabearray , rechts
  das Hilfsarray . Nach sechs Schritten ist das linke
  Teilarray von  leer. Da  initial eine Kopie von  ist, stehen die verbleibenden
  Elemente in  schon an der richtigen Stelle in , denn diese Arraypositionen
  wurden weder in  noch in  verändert. Die Methode stoppt daher nach
  sechs Schritten mit einem sortierten Array .



Um die rekursiv erhaltenen sortierten Teilarrays zu verschmelzen verwenden
wir die Methode merge.
Die Methode erhält zwei sortierte Teilarrays  und 
und verschmilzt sie zu einem sortierten Array in .
Stellen wir uns die beiden Teilarrays als aufsteigend sortierte Spielkartenstapel
vor, so dass auf beiden Stapeln die oberste Karte die jeweils kleinste
Karte des Stapels darstellt.
Wir können die zwei Stapel zu einem sortierten Ausgabestapel
verschmelzen, indem wir stets die obersten Karten vergleichen und die kleinere
der beiden Karten als nächste auf den Ausgabestapel legen.
Genauso funktioniert das Verschmelzen zweier sortierte Teilarrays in
Listing :
Analog zu den beiden obersten Karten der Eingabestapel vergleichen wir in der
Implementierung in Listing  jeweils die vordersten
bislang noch nicht gewählten Elemente an den Positionen  bzw.
 der beiden sortierten Teilarrays.
Hier tritt als zusätzliche Schwierigkeit auf, dass wir unsere Ausgabe an die
gleiche Stelle schreiben möchten, von der wir auch die Eingabe lesen, so dass
wir zu Beginn der Methode eine Kopie der Eingabe in ein globales Hilfsarray
 machen.
Abbildung  zeigt ein Beispiel.

[caption=Die Methode merge, float,label=lst:sorting:merge]
# Annahme: Die Elemente l,...,m und m+1,...r von A sind
# jeweils bereits in sortierter Reihenfolge.
H = Array(n) # globales Hilfsarray

function merge(A, l, m, r):
  # Kopiere relevanten Teil von A in das Hilfsarray H
  for i = l,...,r: 
    H[i] = A[i]

  outpos = l   # Schreibposition in A
  lpos = l     # Leseposition im linken Teil
  rpos = m+1   # Leseposition im rechten Teil

  # Verschmelze bis eines der Teilarrays leer ist
  while lpos <= m and rpos <= r:  
    if H[lpos] < H[rpos]:
      A[outpos] = H[lpos]
      ++lpos
    else:
      A[outpos] = H[rpos]
      ++rpos

    ++outpos

  # Falls linkes Teilarray leer läuft bleibt nichts zu tun; die
  # restlichen Elemente des rechten Teilarrays stehen schon am
  # richtigen Platz. Fall rechtes Teilarray leer läuft müssen
  # restliche Elemente des linken Teils zurückkopiert werden.
  while lpos <= m: 
    A[outpos] = H[lpos]
    ++lpos
    ++outpos


Die Methode merge funktioniert, weil sie die folgende Invariante
erhält:

Jedes Mal, wenn der Schleifenkopf der in
Zeile  durchlaufen wird, gilt für die aktuellen
Werte von , , :

  Das Teilarray  enthält die 
  kleinsten Elemente aus  in sortierter Reihenfolge.
  Die Elemente  und  sind die kleinsten
  Elemente ihrer jeweiligen Arrays, die noch nicht nach  kopiert
  worden sind.

Wir verzichten hier auf einen vollständigen Beweis, dass die Methode die Invariante
erhält und auch darauf, zu zeigen, dass aus der Invarianten die Korrektheit der Methode
folgt.
Stattdessen geben wir lediglich eine Beweisskizze dafür an, dass die while-Schleife
in Zeile  Invariante 1 erhält.
[Beweisskizze]
Wir betrachten eine beliebige Iteration der while-Schleife in Zeile und nehmen per Induktion an, dass beide Invarianten für die aktuelle Wahl von
,  und  zu Beginn der Iteration im Schleifenkopf gelten.
Damit Invariante 1 am Ende der Iteration gilt, müssen wir das
nächst-kleinste Element (das ist das -kleinste aus der Eingabe) an
Position  schreiben.
Dieses Element muss sich entweder
in  oder in  befinden, da es aufgrund unserer
Annahme (Invariante 1) jedenfalls nicht in  enthalten ist.
Gemäß Invariante 2 steht es also in  oder in  und muss weiterhin
das Kleinere der beiden Elemente sein.
Genau dieses Element schreiben wir aber nach .


Wir interessieren uns nun für die Laufzeitanalyse.
Die Methode merge können wir mit unserer bewährten Methodik analysieren:
Die in den Zeilen  macht genau
 Iterationen mit jeweils
konstantem Aufwand. Jede Iteration der in
Zeile   erhöht entweder  oder  um eins.
Initial ist  und ; der Schleifenkopf stellt sicher, dass
 und .
Im schlimmsten Fall macht die Schleife also höchstens
 viele Iterationen mit je konstantem Aufwand.
Die abschließende in Zeile  macht
höchstens  viele Iterationen, und diese mit je konstantem Aufwand.
(Bei einer genaueren Analyse würden wir feststellen, dass die beiden
zusammen höchstens  Iterationen machen.
Diese Beobachtung hat aber keinen Einfluss auf die asymptotische Laufzeit.)
Die asymptotische worst-case Laufzeit der Methode merge beträgt also
, d.h. sie ist linear in der Summe der Längen der zu
verschmelzenden Teilarrays.

Bei der Analyse des eigentlichen MergeSort-Algorithmus stoßen wir auf ein
Problem: Da der Algorithmus sich selbst aufruft müssen wir die Laufzeit
der rekursiven mergeSort-Aufrufe kennen, um die Laufzeit von mergeSort
analyisieren zu können! Wir lösen dieses Problem, indem wir die Laufzeit als
rekursive Funktion beschreiben.
Dabei gehen wir zur Vereinfachung davon aus, dass die Länge  des
Eingabearrays  eine Zweierpotenz ist, d.h. dass  für ein 
gilt.
Diese Annahme stellt für Analyse keine Einschränkung dar, da wir  immer mit
Dummy-Elementen so auffüllen können, dass seine Länge eine Zweierpotenz ist.
Dadurch verdoppelt sich im schlimmsten Fall die Eingabelänge.
Da unsere Implementierung aber auch für Arrays funktioniert, deren Länge keine
Zweierpotenz ist, müssen wir  nie tatsächlich auffüllen.
Sei also  die Laufzeit von mergeSort auf einem (Teil-)Array mit 
Elementen.
Die Zeilen  und verursachen konstanten Aufwand.
Aufgrund unserer Wahl von  und gemäß unserer Annahme, dass  eine
Zweierpotenz ist, erfolgen die beiden rekursiven Aufrufe je auf einem Teilarray
mit Länge genau , d.h. es gilt .
Sie benötigen in unserer Notation also Zeit .
Der abschließende Aufruf von merge in Zeile benötigt nach unseren
Überlegungen Zeit .
Wir erhalten damit die Rekursionsgleichung

für feste Konstanten .

Eine Rekursionsgleichung zu lösen ist im Allgemeinen nicht einfach.
In diesem Fall können wir aber glücklicherweise die richtige Lösung raten
(und anschließend mit einem Induktionsbeweis verifizieren), indem wir denn
Rekursionsbaum in Abbildung  genauer betrachten.
Die obere Hälfte des Rekursionsbaumes beschreibt die rekursiven
Aufrufe des Algorithmus.
Jeder Aufruf halbiert die Länge des betrachteten Teilarrays, und die Rekursion
stoppt, wenn die Teilarrays Länge 1 haben.
Folglich besitzt die obere Hälfte des Rekursionsbaumes  Ebenen.
Analog können wir argumentieren, dass auch die zweite Hälfte des Baumes
 Ebenen besitzt.
Betrachten wir jetzt eine beliebige, feste Ebene .
Jedes Element von  wird auf Ebene  in genau einem rekursiven Aufruf
bearbeitet.
Die Summe der Längen der auf  betrachteten Teilarrays beträgt also .
Die Laufzeit von merge ist aber linear in der Länge des betrachteten
Teilarrays, so dass der Gesamtaufwand für merge pro Ebene 
beträgt.
In der oberen Hälfte des Rekursionsbaumes entsteht pro Aufruf nur konstanter
Aufwand (und es gibt höchstens  Aufrufe pro Ebene) so dass auch hier ein
Aufwand von  pro Ebene entsteht.
Wir vermuten daher, dass die Gesamtlaufzeit von MergeSort 
beträgt.

  Die asymptotische worst-case Laufzeit von mergeSort auf einem
  Eingabearray mit  Elementen beträgt .

  Wir zeigen zunächst per Induktion, dass  ist und nehmen dazu an,
  dass  für ein  gilt.
  
  Dafür müssen wir zeigen, dass es eine Konstante  und eine
  Zahl  so gibt, dass
  
  für alle  gilt.

  Da für  die rechte Seite von Ungleichung eq:sorting:merge-runtime-proof
  null ist, aber 
  gilt, schließen wir, dass  größer als eins sein muss und wählen .
  Außerdem wählen wir  und weisen per Induktion nach,
  dass mit dieser Wahl von  und  die Behauptung
   für alle  gilt.

  Für den Induktionsanfang betrachten wir den Fall  und erhalten
  *
  so dass unsere Behauptung stimmt.

  Für den Induktionsschritt sei  eine Zweierpotenz und .
  Wir setzen in Ungleichung eq:sorting:merge-recursion für  ein
  und erhalten:
  *
  Damit folgt unsere Behauptung.

  Nun müssen wir noch nachweisen, dass auch  gilt.
  Wir erinnern uns zunächst, dass  für jedes  die Worst-Case-Laufzeit
  von MergeSort auf einer Eingabe der Länge  beschreibt.
  Es genügt daher, wenn wir für jedes  ein Array  mit  Einträgen finden,
  das dazu führt, dass MergeSort  viele Elementaroperation
  ausführt.
  Wir wählen  und stellen fest, dass der Rekursionsbaum des
  MergeSort-Aufrufs (s. Abbildung ) dann
   Ebenen enthalten muss: Auch wenn  keine Zweierpotenz ist
  benötigen wir mindestens  viele Ebenen im Rekursionsbaum um
  die Eingabe in Teilarrays der Länge 1 zu zerlegen (und ebensoviele Ebenen
  um sie wieder zusammenzusetzen).
  Ist  keine Zweierpotenz, so kann es passieren, dass in der rechten Hälfte
  des Baumes rekursive Aufrufe entfallen; der Aufwand pro Ebene beträgt aber immer
  mindestens . Damit ist der Gesamtaufwand .

Divide-and-Conquer II: Quicksort

Nach MergeSort betrachten wir nun mit QuickSort einen weiteren
Divide-and-Conquer-Algorithmus.
Die Schwierigkeit bei MergeSort besteht darin, die gefundenden Teillösungen aus
dem Aufteilungsschritt effizient wieder zusammen zu setzen.
Bei QuickSort ist das Zusammensetzen der Teillösungen trivial, dafür erfordert
der Aufteilungsschritt einige clevere Ideen.

Die Methode partition als Aufteilungsschritt von QuickSort

Anders als MergeSort teilt der Aufteilungsschritt von Quicksort das
Eingabearray  nicht an einer festen Stelle auf.
Stattdessen wählen wir im Aufteilungsschritt ein Element aus , verschieben
es an die Position , an der es im (sortierten) Ausgabearray stehen wird und
machen dann je einen rekursiven QuickSort-Aufruf für die Teilarrays
links und rechts von .
Indem wir durch zusätzliche Vertauschungen dafür sorgen, dass links von 
nur Elemente stehen, die nicht größer als  sind, und dass rechts von
 nur Elemente stehen, die nicht kleiner als  sind, stellen wir sicher,
dass wir auf diese Weise ein sortiertes Array erhalten.
Wir notieren das Vorgehen in Listing ; hier übernimmt
die Methode partition den Aufteilungsschritt.
Um ein beliebiges Array  zu sortieren rufen wir QuickSort(A, 0, A.laenge-1)
auf.
[caption=Der Algorithmus QuickSort,label=lst:sorting:quicksort]
function quickSort(A, l, r):
  if l >= r: 
    return   

  # Teile A auf in A[l,...,p-1], A[p], A[p+1,...,r]
  # alle Elemente in A[l,...,p-1] sind nicht größer als A[p]
  # alle Elemente in A[p+1,...,r] sind nicht kleiner als A[p]
  p = partition(A, l, r) 

  # Sortiere A[l,...,p-1] und A[p+1,...,r] rekursiv.
  quickSort(A, l, p-1)
  quickSort(A, p+1, r)












Die Schwierigkeit liegt nun darin, die Methode partition zu implementieren.
Die grundsätzliche Idee der Methode ist, das linke Teilarray vom linken Arrayrand
und das rechte Teilarray vom rechten Arrayrand wachsen zu lassen.
Das Beispiel in Abbildung  verdeutlicht die Idee.
Stoßen die beiden Teilarrays zusammen, platzieren wir das Pivotelement dazwischen
und haben die gewünschte Aufteilung erreicht.
Nun kann es uns aber natürlich passieren, dass das linke Teilarray an ein
Element  stößt, das größer als das Pivotelement ist.
Gemäß unserer Überlegungen dürfen wir  nicht in das linke Teilarray aufnehmen
und stoppen daher zunächst das Wachstum des linken Teilarrays vor .
Analog kann das rechte Teilarray auf ein Element  stoßen, das kleiner als das
Pivotelement ist und nicht ins rechte Teilarray aufgenommen werden darf; in
diesem Fall stoppen wir das Wachstum des rechten Teilarrays.
Es gibt jetzt zwei Möglichkeiten: Können die Teilarrays so weit wachsen,
dass sie zusammen stoßen und erhalten wir die gewünschte Aufteilung.
Anderenfalls können beide Teilarrays können nicht mehr weiter wachsen.
Tatsächlich wird das linke Teilarray dann von einem Element blockiert, das ins
rechte Teilarray gehört, während des rechte Teilarray von einem Element
blockiert wird, dass ins linke Teilarray gehört! Wir können uns also aus der
misslichen Lage befreien, indem wir schlicht  und  vertauschen.


  fig-partition-example
  
  Ein Beispieldurchlauf der Methode partition.





Wir präzisieren das Vorgehen in Listing .
[caption=Die Methode partition, label=lst:sorting:partition]
function partition(A, l, r):
  # Wähle A[l] als Pivotelement und suche die Position für A[l]
  # in A[l...r]
  i = l+1; j = r
  while True: 
    # Invariante: Alle Elemente in A[l+1...i-1] sind <= A[l];
    # alle Elemente in A[j+1...r] sind >= A[l].

    while i < r and A[i] <= A[l]: 
      i++                         

    while j > l and A[j] >= A[l]: 
      j-                         

    # Stoppe, wenn wir gesamtes Array durchsucht haben.
    if i >= j:                    
      break

    # A[i] und A[j] in falscher Arrayhälfte; vertausche
    # und bewege Zeiger zum nächsten Element
    swap(A[i], A[j])              
    i++; j-                      

  # Tausche Pivotelement in korrekte Position und gib
  # Pivotposition zurück.
  swap(A[l], A[j])                
  return j                        

Der Algorithmus erhält die folgende Invariante aufrecht.
In Schleifenrumpf der in Zeile gilt zu jedem Zeitpunkt:
  
    Alle Elemente in  sind kleiner oder gleich dem Pivotelement
  .
    Alle Element in  sind größer oder gleich dem Pivotelement .

Induktionsanfang. Initial sind  und 
leere Arrays und die Invariante gilt trivialerweise.

Induktionsschritt.  Nehmen wir also per Induktion an, dass die
Invariante zu Beginn einer beliebigen Iteration der Schleife in
Zeile  für die aktuelle Wahl von
 und  gilt.
Wir zeigen, dass die Iteration die Invariante aufrecht erhält.
Zeile  ist die erste Zeile, die potentiell
 verändert.
Nach Induktionsvoraussetzung sind , wenn wir die
Zeile zum ersten Mal erreichen.
Der Schleifenkopf in Zeile  stellt nun
sicher, dass die Zeile nur erreicht wird, wenn
auch  ist. Folglich können wir  um eins erhöhen ohne die
Invariante zu verletzen; sie gilt also insbesondere nach jeder Iteration dieser
Schleife.
Analog können wir  in Zeile  um eins
verringern, weil nach Induktionsvoraussetzung  ist
und der Schleifenkopf in Zeile  uns
 garantiert.

Wird Zeile  durchlaufen, so muss
gelten, dass  und ,
weil die Zeile nur erreicht wird, wenn die inneren Schleifen in den
Zeilen  und stoppen, aber die in Zeile nicht greift.
Indem wir  und  vertauschen, stellen wir also sicher, dass nach der
Vertauschung  und  ist.
Wir dürfen also in der Zeile  die
Variable  um eins erhöhen und  um eins verringern, ohne die Invariante zu
verletzen.
Haben wir  erreicht und die Invariante gilt, vertauschen wir  mit , denn vor der Vertauschung von  und
 muss gelten, dass  ist (ansonsten wäre  weiter verringert
worden).
Danach erhalten wir die gewünscht Aufteilung als  und .
































Laufzeitanalyse der Methode partition

Als nächstes weisen wir nach, dass die asymptotisch Worst-Case-Laufzeit von
partition ebenso wie die Laufzeit von merge linear in der
Größe des betrachteten Teilarrays ist, also  beträgt.
Sei  die Anzahl der Iterationen, die die äußere in Zeile  auf der Eingabe 
durchläuft.
In der -ten Iteration der äußeren schreiben wir
 für die Anzahl der Iterationen der inneren in
Zeile  und  für die Anzahl der
Iterationen der inneren in Zeile .

Jede Iteration der inneren verursacht konstanten Aufwand.
Auch der restliche Schleifenrumpf der äußeren in den
Zeilen -und die abschließenden Zeilen und  verursachen konstanten
Aufwand.
Die Gesamtlaufzeit  von partition auf der Eingabe  beträgt also
höchstens
*
für geeignet gewählte Konstanten .

In einer ersten, naiven Abschätzung stellen wir fest, dass  ist:
Initial ist , jede Iteration erhöht  um eins, und die Schleife stoppt
spätestens, wenn .
Analog ist .
Die äußere stoppt spätestens in Zeile ,
wenn  oder .
Da in jeder Iteration  um eins erhöht und  um eins verringert wird,
tritt die Stopp-Bedingung nach spätestens  Iterationen ein.
Mit dieser Abschätzung erhalten wir
*

Bei genauerer Betrachtung haben wir gerade argumentiert, dass jede Iteration
der inneren Schleifen und jede Iteration der äußeren Schleife den Abstand 
zwischen  und  um mindestens eins verringert.
Initial ist .
Die Schleifen stoppen spätestens, wenn  und , und damit 
ist.
Folglich können alle Schleifen zusammen höchstens 
Iterationen machen, d.h.
*
Wir folgern:
*
Es ist eine gute Übung, eine Eingabe zu finden, auf der partition tatsächlich
Laufzeit  benötigt.
Damit haben wir das folgende Lemma.

  Die asymptotische Worst-Case-Laufzeit von partition auf einem
  Teilarray mit  Elementen beträgt .
Worst-Case-Laufzeit von QuickSort

Trotz des recht komplizierten Aufteilungsschrittes - und obwohl partition
asymptotisch nicht langsamer ist als merge - beträgt die
asymptotische Worst-Case-Laufzeit von QuickSort auf einem Array mit
 Elementen .
Das liegt daran, dass partition im schlimmsten Fall in jedem
Aufteilungsschritt das Pivot-Element am linken oder rechten Rand einsortiert und
so eine unbalancierte Aufteilung erzeugt.
Betrachten wir als Beispiel die Ausführung von QuickSort auf einem bereits
sortierten Array  in Abbildung .
Hier erzeugt partition auf jeder Ebene der Rekursion die Aufrufe
QuickSort(A,l+1,r) und QuickSort(A,l,l).
Mit jeder Ebene der Rekursion verringert sich also die Eingabelänge für
den ersten rekursiven Aufruf (und damit auch für partition) um lediglich
1 und wir benötigen eine Rekursionstiefe von  damit die Rekursion stoppt.
Da die Eingabelänge für partition auf den ersten  Ebenen der
Rekursion mindestens  beträgt und partition Zeit 
benötigt um einen Bereich von  Elementen aufzuteilen, benötigen wir in diesem
Fall also insgesamt Zeit .


[T]0.45
        
    
    Im besten Fall wählt QuickSort in jedem Schritt ein
    Pivotelement, dass in der Mitte einsortiert wird und so die Arraygröße
    halbiert.
    Der Rekursionsbaum besitzt  Ebenen mit je linearem Aufwand
    und die Gesamtlaufzeit beträgt im besten Fall .
    [T]0.5
      
  Im schlechtesten Fall wählt QuickSort ein Pivotelement, das
  am Rand einsortiert wird. In diesem Fall sinkt die Arraygröße für einen der
  beiden rekursiven Aufrufe lediglich um eins.
  Folglich muss QuickSort  rekursive Aufrufe durchführen.
  Da der Aufwand des -ten Aufrufs proportional zu  ist, ist der Gesamtaufwand
  im schlechtesten Fall proportional zu .
Die Laufzeit von QuickSort im besten und im schlechtesten Fall. Welcher
Fall eintritt hängt davon ab, wo das gewählte Pivotelement einsortiert werden muss.


Wir gehen in diesem Beispiel davon aus, dass QuickSort stets  als das
Pivotelement wählt.
Wählt QuickSort das Pivotelement von einer beliebigen anderen festen
Position lassen sich ähnliche Worst-Case-Eingaben konstruieren.
Wir werden später Alternativen diskutieren.

Zunächst weisen wir aber nach, dass die worst-case Laufzeit von QuickSort auch
nicht schlechter als  ist.
Dazu untersuchen wir wieder eine Rekursionsgleichung und bezeichnen wir
 die Laufzeit von QuickSort auf einem (Teil-)array mit 
Elementen.
Aus unserer Laufzeitanalyse von partition wissen wir, dass es eine
Konstante  gibt, so dass die Laufzeit von partition auf einem
(Teil-)array mit  Elementen durch  beschränkt ist.
Gemäß unserer Überlegungen ist die von partition generierte Aufteilung
so unbalanciert wie möglich, wenn das Pivot-Element am Rand von 
einsortiert wird und wir erhalten:

wobei die Konstante  die Laufzeit der konstant vielen Elementaroperationen der
Methode QuickSort in Listing  beschreibt, die außerhalb
des Aufrufs von partition und der beiden rekursiven
QuickSort-Aufrufe durchgeführt werden (also z.B. den Aufwand für die
in den Zeilen f und die Zuweisung in
Zeile ).
Wir lernen an dieser Stelle eine weitere Methode zum Lösen von Rekursionsgleichungen
kennen und analysieren  durch rekursives Einsetzen:
*
Wir fassen unsere Überlegungen in einem Theorem zusammen.

  Die asymptotische Worst-Case-Laufzeit von QuickSort auf  Elementen
  beträgt , wenn QuickSort das Pivot-Element von einer festen
  Position wählt.
Das Laufzeitverhalten von QuickSort lässt sich verbessern, indem wir
das Pivot-Element nicht von einer festen Position, sondern abhängig von 
wählen.
Zum Beispiel ist es möglich, in Zeit  den Median von  zu
bestimmen. Wählen wir den Median als Pivotelement erhalten wir in jedem
partition-Aufruf garantiert eine balancierte Aufteilung und können
analog zur MergeSort-Analyse zeigen, dass die Worst-Case-Laufzeit
mit der Median-Pivot-Strategie  beträgt.
Die Mediansuche in Zeit  ist jedoch mit hohen Konstanten behaftet.
Wir sehen im nächsten Abschnitt eine einfachere Möglichkeit, eine praktische
Verbesserung des Algorithmus zu erzielen.

Durchschnittliches Laufzeitverhalten von QuickSort

Trotz der quadratischen Worst-Case-Laufzeit wird QuickSort in der Praxis verwendet
(s. Programmierecke).
Das liegt unter anderem daran, dass der Algorithmus im Durchschnitt ebenso
wie schnell ist wie MergeSort - also in Zeit  - arbeitet,
aber der Aufteilungsschritt anders als bei MergeSort mit konstantem
Zusatzspeicher auskommt.(Beide Algorithmen benötigen allerdings zusätzlichen Speicher
für den Rekursionsstack. Bei geschickter Implementierung lässt sich der Zusatzspeicher
für QuickSort auf Platz  begrenzen.)

Wegen der hohen Bedeutung für die Praxis analysieren wir das durchschnittliche
Laufzeitverhalten.




Für die Analyse des Durchschnittsfalls untersuchen wir den Fall, dass alle 
Permutationen von  verschiedenen Zahlen mit gleicher Wahrscheinlichkeit als
Eingabe auftreten und bestimmen die durchschnittliche Laufzeit über alle
Eingaben.
Wir bezeichnen mit  die durchschnittliche Laufzeit von Quicksort
auf einem Array mit  Elementen.

Im ersten Schritt wählt QuickSort das Element  als Pivotelement
und sortiert es an eine Position des Arrays.
Da in unserer Analyse alle möglichen Permutationen von  gleich wahrscheinlich
sind, wird für  jede Position  mit gleicher
Wahrscheinlichkeit  - im Durchschnitt also in einem -tel der Fälle -
gewählt.
Damit erzeugt Partition zwei Teilarrays  und
 der Größe  bzw. .
Dabei treten in  alle Permutationen der Zahlen in , die
kleiner als  sind, mit gleicher Wahrscheinlichkeit auf
(wir erinnern  uns, dass wir davon ausgehen, dass alle Zahlen verschieden sind).
QuickSort benötigt auf diesem Teilarray im Durchschnitt also Zeit
.
Ebenso treten in  alle Permutationen der Zahlen in , die
größer als  sind mit gleicher Wahrscheinlichkeit auf.
Auf diesem Teilarray benötigt QuickSort also im Durchschnitt Zeit
.
Im Allgemeinen erhalten wir, dass
*
für geeignete Konstanten  ist.

Es gilt .Wir verzichten hier auf den Beweis des Theorems.

































Unsere Analyse der Durchschnittslaufzeit hat den Nachteil, dass wir eine
Annahme über die Verteilung der Eingaben treffen müssen; tatsächlich ist es nicht
besonders realistisch, dass jede Permutation der Daten mit gleicher Wahrscheinlichkeit
bzw. gleich häufig auftritt.
Wir können das gute durchschnittliche Verhalten erhalten, aber auf die Verteilungsannahme
verzichten indem wir das Pivotelement zufällig gleichverteilt wählen.




Listing  zeigt diese randomisierte QuickSort-Variante.
Hier wählen wir zunächst ein zufälliges Element  aus  und
tauschen es an Position .
Bei der zufälligen Wahl wird jedes Element  mit  mit der
gleichen Wahrscheinlichkeit  gewählt.
Anschließend können wir die Methode partition unverändert aufrufen.
[caption=QuickSort mit zufälliger Pivotwahl, label=lst:sorting:random-quicksort]
function randomQuickSort(A, l, r):
  if l >= r:
    return

  # Wähle ein Pivotelement zufällig gleichverteilt aus A[l,...,r]
  r = uniformRandomInt(l,r)
  swap(A[r], A[l])

  # Partition wird unverändert verwendet
  p = partition(A, l, r)

  # Sortiere A[l,...,p-1] und A[p+1,...,r] rekursiv.
  randomQuickSort(A, l, p-1)
  randomQuickSort(A, p+1, r)
Unsere Analyse der durchschnittlichen Laufzeit funktioniert unverändert für
RandomQuickSort, allerdings mit dem Unterschied, dass wir den Durchschnitt
nicht über die möglichen Permutationen der Eingabe sondern über die zufälligen
Wahlen des Pivotelements bilden.

  RandomQuickSort besitzt auf einem Eingabearray mit  Elementen
  eine durchschnittliche Laufzeit von .
  Hier bilden wir den Durchschnitt über die zufällige Wahl des Pivotelementes.
Das bedeutet, dass wir für jede Eingabe im Durchschnitt Zeit 
benötigen.













































Eine untere Schranke
Nachdem wir herausgefunden haben, dass wir das Sortierproblem mit  Zahlen in
Zeit  lösen können, ist eine naheliegende Frage, ob es
Sortieralgorithmen mit einer besseren Laufzeitgarantie gibt.
In der Regel ist es sehr schwierig zu zeigen, dass jeder Algorithmus für ein
Problem eine gewisse asymptotische worst-case Laufzeit besitzen muss.
Im Falle des Sortierproblems ist aber eine solche untere Laufzeitschranke bekannt.

Die Laufzeitschranke bezieht sich auf Sortieralgorithmen, die ausschließlich
mit Hilfe von paarweisen Vergleichen sortieren.
Sie gilt daher nicht, wenn ein Algorithmus etwa die richtige Position einer
Zahl anhand ihrer Größe errät oder einzelne Ziffern der Eingabezahlen zur
Sortierung verwendet.

Wir werden in diesem Abschnitt zeigen, dass jeder solche vergleichsbasierte Sortieralgorithmus
eine asymptotische Worst-Case-Laufzeit von mindestens  besitzt.
Zur Vereinfachung gehen wir davon aus, dass die Eingabe aus  paarweise verschiedenen
Zahlen besteht.
Wir betrachten also nur einen Spezialfall des Sortierproblems.
Dies stellt aber keine Einschränkung für die Anwendbarkeit unserer unteren Schranke dar:
Kann ein Algorithmus schon das eingeschränkte Sortierproblem bestenfalls in Zeit
 lösen, dann erst recht das allgemeine.

Betrachten wir also einen beliebigen Sortieralgorithmus und eine beliebige, feste
Eingabelänge .
Wir vereinfachen unsere Analyse mit einem weiteren Trick: Anstelle der Anzahl der
Elementaroperationen zählen wir lediglich die Anzahl der paarweisen Vergleiche, die
unser Algorithmus machen muss -
da jeder Vergleich mindestens eine Elementaroperation benötigt, liefert uns das eine
untere Schranke.
Aufgrund unserer Annahme gibt es außerdem für jede Eingabe eine eindeutige Permutation,
die die Eingabe in ein sortiertes Array überführt.
Unser Sortieralgorithmus muss (zumindest implizit) diese Permutation berechnen.

Zu Beginn der Ausführung haben wir noch nichts über das Eingabearray  gelernt und
jede der  vielen Permutationen der  Eingabezahlen könnte die richtige Ausgabe
sein.
Jeder paarweise Vergleich schränkt die möglichen Ausgaben aber ein:
Vergleichen wir etwa  und  und erhalten, dass , so
muss die Ausgabepermutation  vor  sortieren.
Umgekehrt muss  vor  stehen, wenn  ist. Der Fall
 kann nicht auftreten, da wir davon ausgehen, dass alle Elemente
von  paarweise verschieden sind.
Auf diese Weise schränken wir Menge der möglichen Permuationen immer weiter ein,
bis schließlich nur eine einzige Permutation übrig ist; diese muss die gesuchte
Permutation sein (s. Abbildung ).

Mit der Notation aus Abbildung  haben wir damit
die Ausführung eines beliebigen Sortieralgorithmus als einen Entscheidungsbaum
modelliert, in dem jeder innere Knoten zu einem paarweisen Vergleich gehört,
dessen Ausgang von genau zwei Kinderknoten dargestellt wird.
In dem Entscheidungsbaum ist jeder Knoten mit einer Menge an noch möglichen
Permutationen annotiert.
Zu jeder Permutation  gibt es eine Eingabe , die von  (und nur von )
in ein sortiertes Array überführt wird.
Der Entscheidungsbaum besitzt also genau  Blätter;
für jede Permutation genau eines.
Die worst-case Laufzeit des betrachteten Algorithmus ist nun die Länge eines
längsten Pfades von der Wurzel des Entscheidungsbaum zu einem Blatt, also gerade
die Höhe des Entscheidungsbaumes.
Da jeder innere Knoten in einem Entscheidungsbaum genau zwei Kinder
besitzt, hat ein Entscheidungsbaum mit Höhe  höchstens  Blätter (so viele
Blätter erhalten wir, wenn der Baum vollständig ist).
Unser Entscheidungsbaum besitzt wie oben argumentiert  viele Blätter, so dass
 und damit auch  sein muss.
Lemma  aus dem Anhang sagt uns, dass  ist.
 Jeder Sortieralgorithmus, der ausschließlich auf paarweisen Vergleichen basiert
 besitzt eine asymptotische Worst-Case-Laufzeit von mindestens .Zusammengefasst haben wir festgestellt, dass es  viele Permutationen jeder möglichen
Ausgabe gibt. Jeder korrekte Sortieralgorithmus muss diesen  Möglichkeiten
unterscheiden können.
Wir haben nachgewiesen, dass wir dazu  paarweise Vergleiche
benötigen.








    sorting-lowerbound-decision-tree
  

Ein beispielhafter Entscheidungsbaum, in dem Knoten in 4 Ebenen angeordnet sind.
Wir nummerieren die Ebenen von  bis  und sagen, dass der Baum Höhe  besitzt (die Höhe ist
im Allgemeinen also der Index der untersten Ebene).
Knoten des Entscheidungsbaumes, die mit einer Kante zu einem Kind(-erknoten) verbunden sind,
heißen innere Knoten () des Baumes.
Alle anderen Knoten nennen wir Blätter ().
Der Knoten auf Ebene 0 heißt außerdem Wurzel.
Da jeder Knoten des Baumes höchstens zwei Kinder besitzt, nennen wir den Baum binär.
Besitzt jeder innere Knoten des Baumes genau zwei Kinder und befinden sich alle Blätter des Baumes
auf Ebene  sagen wir, dass der Baum vollständig ist.
Der Baum in diesem Beispiel ist also binär, aber nicht vollständig.
Da sich also die Anzahl der Knoten eines vollständigen binären Baumes in jeder Ebene verdoppelt, und
Ebene 0 des Baumes einen einzelnen Knoten besitzt, enthält ein vollständiger binärer Entscheidungsbaum auf
Ebene  gerade  viele Knoten, und insbesondere genau  Blätter.

Sortieren in Linearzeit

Wir haben gerade eingesehen, dass vergleichsbasierte Sortieralgorithmen auf  Elementen
immer eine Laufzeit von  besitzen.
In einem Sonderfall ist es jedoch möglich, schneller zu sortieren:
Wir betrachten das Szenario, dass wir Zahlen aus einem begrenzten Zahlenbereich
sortieren möchten.
Außerdem erlauben wir es nun, dass eine Zahl mehrfach in der Eingabe vorkommt.
Formaler möchten wir folgendes Problem lösen.
Sortierproblem mit begrenztem Zahlenbereich
  Eine Zahl , ein Array  mit  Zahlen aus .
  Eine Permutation (Umordnung) der Zahlen als Array  mit
  *
Wir überlegen uns nun, wie wir das Sortierproblem mit begrenztem Zahlenbereich in Zeit  lösen können.

BucketSort

Gehen wir kurz davon aus, dass alle Zahlen in  paarweise verschieden sind.
Dann funktioniert folgender Algorithmus:

  Lege ein Array  mit Länge  an und initialisiere alle Einträge
  von  mit einem Platzhalter, der in der Eingabe nicht vorkommen kann
  (etwa ).
  Sortierphase: Anschließend iterieren wir über  und schreiben jede Zahl  aus
   an Position  in , also nach .
  Sammelphase: Wir iterieren über  und schreiben alle Zahlen in  zurück nach ,
  und zwar in der Reihenfolge in der wir sie finden.
  Platzhalter überspringen wir.
Schritt 1 und 3 benötigen Zeit , Schritt 2 benötigt Zeit .
Insgesamt haben wir also die gewünschte Laufzeit von  erreicht.
Schritt 2 funktioniert aber nur aufgrund unserer Annahme, dass alle Zahlen paarweise
verschieden sind - denn mehrere Vorkommen einer Zahl  in  würden alle an
Position  geschrieben und wir würden die Information verlieren, wie oft  in 
vorkommt.
Um dieses Problem zu beheben, richten wir für jede Zahl  einen
sog. Bucket (Eimer) ein, in dem wir in Schritt 2 alle Vorkommen von 
sammeln.
Die Abbildung  und zeigen ein Beispiel des Algorithmus mit und ohne Buckets.
Das resultierende Verfahren heißt Bucketsort.

 bucketsort-unique-example
 
  BucketSort mit  für paarweise verschiedene Zahlen. Wir schreiben 
  an Position  in  und füllen alle restlichen Positionen mit einem
  Platzhalter (hier ). Wir können die sortierte
  Folge aus  ablesen, indem wir alle Platzhalter überspringen.

  bucketsort-general-example
  )
  BucketSort mit Wiederholungen und . Wir legen 
  in Bucket  in .
   Wir können die sortierte Folge aus  ablesen, indem wir nicht-leeren
   Buckets von links nach rechts aneinanderhengen.
   Unten: Effiziente Speicherung von  in einem Array der Länge  ohne Platzhalter.
   Die sortierte Folge kann direkt abgelesen werden.)

Für die Implementierung von BucketSort müssen wir uns überlegen, wie wir die
Buckets im Speicher ablegen.
Eine Möglichkeit wäre etwa, für jeden Bucket ein Array anlegen.
Dazu müssen wir in einem zusätzlichen Zählschritt) für jedes 
die Anzahl  der Vorkommen von  in  berechnen.
In  speichern wir dann an Position  ein Array der Länge .

In praktischen Implementierungen hat sich bewährt, nicht für jeden Bucket ein eigenes Array
anzulegen, sondern alle Buckets in einem gemeinsamen Array  der Länge  zu verwalten.
Der Bucket von  befindet sich dann an den Positionen
 bis  in .
Insgesamt erhalten wir damit den Pseudocode aus Listing .
[caption=BucketSort, float,label=lst:sorting:bucketsort]
function bucketSort(A, M):
  buckets = Array(n)

  # Zähle Vorkommen von A[i] in count
  count = Array(M)
  for i=0,...,n-1:
    count[A[i]]++

  # Fuer alle b=0,...,M-1:
  # Berechne Position des ersten Eintrags von Bucket b
  count[M-1] = A.laenge - count[M-1]
  for b=M-2,M-3,...,0:
    count[b] = count[b+1] - count[b]

  # Sortierphase:
  # count[b] ist Schreibposition fuer bucket b
  for i=0,...,n-1:
    buckets[count[A[i]]] = A[i]
    count[A[i]] += 1

  # Sammelphase
  for i=0,...,n-1:
    A[i] = buckets[i]


RadixSort

Zum Abschluss des Kapitels betrachten wir eine Anwendung von BucketSort und
machen zunächst die Beobachtung, dass BucketSort eine nützliche Eigenschaft besitzt:
Tritt eine Zahl wiederholt in der Eingabe auf, so enthält das sortierte Ausgabearray
die Wiederholungen der Zahl in der gleichen Reihenfolge wie die Eingabe.
Einen solchen Sortieralgorithmus nennt man stabil.
Wir möchten nun BucketSort verwenden, um das folgende Problem zu lösen.
Sortierproblem mit beschränkten Zahlenlängen
  Eine Zahl , ein Array  mit  Zahlen.
  Jede Zahl in  hat (in Basis-10-Darstellung) höchstens  Stellen.
  Eine Permutation (Umordnung) der Zahlen als Array  mit
  *
Wir lösen das Sortierproblem mit beschränkten Zahlenlängen, indem wir die
Einträge von  in insgesamt  Iterationen sortieren; genauer
sortieren wir zunächst nach der 0-ten Stelle, dann nach der 1-ten Stelle, usw.
(dabei ist die -te Stelle diejenige, die die Wertigkeit 
repräsentiert).
Dabei ist es wichtig, dass jede Iteration die Vorsortierung der vorherigen
Iterationen beibehält.
Da bei der Sortierung nach Stelle  höchstens 10 Werte - nämlich
die Ziffern , auftreten können - können wir für jede Iteration
BucketSort mit  verwenden.
BucketSort erhält die Vorsortierung, da der Algorithmus stabil ist.
Insgesamt ergibt sich folgender Algorithmus (s. Beispiel in Abbildung ).

  Für :
  
    Erzeuge zehn Buckets B[0],,B[9]
    Für :
 Sei  die -te Stelle von .
    Dann lege  in Bucket .
    Schreibe hintereinander , ,, nach .
  Besitzt eine Zahl weniger als  Stellen, gehen wir davon aus, dass die Zahl
mit 0en aufgefüllt wird.
Da hier  konstant ist, benötigt jeder BucketSort-Aufruf Zeit .
Wir machen insgesamt  dieser Aufrufe und erhalten eine asymptotische
Worst-Case-Laufzeit von .
Ist  ebenfalls konstant (z.B. weil wir 32bit Integer sortieren) arbeitet
RadixSort in Linearzeit .

[caption=RadixSort, float,label=lst:sorting:radixsort]
# RadixSort zur Basis 10
function radixSort10(A, d):
  buckets = Array(n)

  # Für jede Stelle: Führe einmal BucketSort durch
  for t=0,...,d-1:
    # Wieviele Zahlen mit Ziffer x an Stelle t gibt es?
    count = Array(10)
    for i=0,...,n-1
      x = digit_t(A[i]) # liefert Stelle t von A[i]
      count[x] += 1

    # Berechne Position des ersten Eintrages in Bucket b
    count[9] = n - count[9]
    for d=8,7,...,0
      count[d] = count[d+1] - count[d]

    # Lege Eingabe in Buckets gemäß Stelle t
    for i=0,...,n-1
      x = digit_t(A[i])
      buckets[count[x]] = A[i]
      count[x] += 1

    # Sammelphase
    for i=0,...,n-1:
      A[i] = buckets[i]


  RadixSort löst das Sortierproblem mit  Zahlen mit höchstens  Ziffern
  in Zeit .



Ein Beispiel für RadixSort. Am Ende der Sortierphase für Stelle  ist jeder Bucket gemäß der Stellen  sortiert.
Das Ergebnis der Sammelphase  ist stets die Eingabe für die Sortierphase .


Elementare Datenstrukturen: Spezifikation und Analyse
Datenstrukturen dienen dazu, Informationen organisiert zu speichern.
Sie sollen sicherstellen, dass wir effizient auf die abgelegten Informationen
zugreifen können ohne verschwenderisch mit dem Speicherplatz umzugehen.
Eine Datenstruktur besteht dabei stets aus zwei Teilen: Die Schnittstelle
definiert, mit welchen Methoden wir die gespeicherten Daten manipulieren und
auf sie zugreifen können.
Die Spezifikation einer Schnittstelle entspricht der Definition einer
Problemstellung im vorherigen Kapitel.
Die Implementierung beschreibt, wie wir eine definierte Schnittstelle
realisieren können und entspricht einem Algorithmus im vorherigen Kapitel.
Auch in diesem Kapitel wird es also darum gehen, Anforderungen geeignet zu
spezifizieren und Problemlösungen strukturiert aufzuschreiben.
Ebenso werden wir untersuchen, mit welchen Laufzeitgarantien unterschiedliche
Implementierungen eine gegebene Schnittstelle realisieren.




Listen
Stellen wir uns als einführendes Beispiel ein Bildbearbeitungsprogramm vor.
Das Programm erlaubt es dem Benutzer, auf verschiedenen transparenten Bildebenen
zu zeichnen, die übereinander liegen.
Zeichnet der Benutzer etwas auf einer Ebene, so werden die Zeichnungen auf allen
darunterliegenden Ebenen überdeckt.
Der Benutzer kann beliebig neue Ebenen hinzufügen, Ebenen löschen und die Reihenfolge
der Ebenen ändern.
Wie können wir die Zeichenebenen - und deren Reihenfolge - im Speicher verwalten?
Eine Möglichkeit ist, ein Array mit Ebenenobjekten anzulegen.
Da wir nicht wissen, wieviele Ebenen der Benutzer einfügen wird, kann es uns
aber passieren, dass dieses Array zu klein wird.
Außerdem ist nicht klar, wie wir in dem Array eine neue Ebene
zwischen zwei existierenden Ebenen einfügen können.
Wir verfolgen diese Idee dennoch weiter und untersuchen in Abschnitt die Datenstruktur ArrayListe.
Alternativ kann jede Ebene einen Verweis auf die jeweils darüber- und
darunterliegende Ebene speichern (sofern die darüber- bzw. darunterliegende Ebene
nicht existiert speichern wir einen Dummy-Wert).
Diese Idee führt zu verketteten Listen, die wir in den Abschnitten und  betrachten.

Eine Liste speichert eine endliche Folge von Objekten  und
ihre Reihenfolge.

Wir können also vom ersten, zweiten, dritten usw. Objekt einer Liste
oder von Vorgänger- und Nachfolgerelementen in der gespeicherten
Reihenfolge sprechen.
Wie in unserem einführenden Beispiel kann der Benutzer die Reihnefolge der
Objekte vorgeben bzw. verändern.
Wir nennen die Objekte in der Liste die Elemente der Liste.

Folgen von Objekten treten in ganz verschiedenen Zusammenhängen auf, und die
enthaltenen Objekte können sehr verschiedenartig sein.
Wir werden daher zur Vereinfachung der Darstellung davon ausgehen, dass zu jedem
Objekt in unserer Folge eine (eindeutige) nicht-negative ganze Zahl gehört, die
wir Schlüssel nennen.
Unsere Datenstrukturen verwalten lediglich die Schlüssel der Daten und stellen uns vor,
dass die eigentlichen Informationen unter diesem Schlüssel in der Datenstruktur
abgelegt werden.
In der Praxis müssen natürlich die Schlüssel - etwa wie im folgenden Beispiel -
noch mit den eigentlichen Daten verknüpft werden.

class Item:
    int key          # Zahl als identifizierender Schlüssel
    DataType data    # Zeiger auf die eigentlichen Daten
Häufig lassen sich Objekte auf natürliche Weise in Zahlen umrechnen:
Zum Beispiel den Zeiger auf ein Objekt verwenden, um eine Zahl aus dem Objekt zu generieren.
Dieser ist eindeutig.
Zur Vereinfachung der Notation setzen wir im Folgenden hin und wieder Elemente
mit ihrem Schlüssel gleich und stellen uns vor, dass jedes Element bei Bedarf
einen Zeiger auf das eigentlich (Daten-)Objekt besitzt, auch wenn dieser
Zeiger in den Beispielen nicht explizit angegeben ist.
Anders gesagt: Wir speichern in unseren Datenstrukturen nur Zahlen und behalten
im Hinterkopf, dass wir bei Bedarf z.B. mit der oben gegebenen Klasse Item
auch beliebige Daten abspeichern können.












ArrayListen
Die Datenstruktur ArrayListe verwaltet eine (endliche) Folge
 von  Elementen  und soll die
folgenden Operationen
unterstützen.

  [get(i)] Liefert das Element , also jenes mit Index , zurück.
  
  
  [append(x)] Fügt das Element  an das Ende der Liste, also als
  Element  ein.







  [find(x)] Liefert den Index des linkesten Vorkommens von , d.h. den
  kleinsten Index  mit . Enthält die Liste  nicht, liefert die
  Methode das Dummyelement null zurück.
  [delete(x)] Verändert die Liste nicht, falls .
  Anderenfalls sei  der Index des linkesten Vorkommens von ,
  d.h. der kleinste Index  mit .
  Dann entfernt die Methode  aus der Liste.
  Die Elemente an den Positionen  werden um eine Position
  nach links verschoben, verringern also jeweils ihren Index um eins.
  [size()] Liefert die Anzahl  der Elemente in der Liste zurück.
Abbildung  zeigt einige Beispiele für die Anwendungen der Operationen.


arraylist-example

Beispiel für das Verhalten einer Arrayliste. Wir unterscheiden die Größe der Arrayliste (Anzahl gespeicherter Elemente) und ihre Kapazität (Größe des zugrundliegenden Arrays). Die ArrayListe vergrößert sich, sobald ein Element angefügt wird, aber kein freier Platz mehr im zugrundliegenden Array verfügbar ist.
Löschoperationen führen potentiell zu Lücken im zugrundeliegenden Array, so dass Elemente nach links verschoben werden müssen.


Verwaltung der Folge 
Wir möchten diese Schnittstelle nun implementieren und entscheiden uns, die
Folge  in einem Array  zu verwalten.
Dazu speichern wir  an Position .
Im Allgemeinen muss  also mindestens Größe  besitzen und muss
mitwachsen, wenn wir neue Elemente in  einfügen.
Damit  nicht bei jeder Einfügeoperation wachsen muss, erlauben wir es, dass
 eine Größe von mehr als  besitzt.
Zur besseren Unterscheidung nennen wir die Größe von  im Folgenden die
Kapazität der Liste.
Initial ist unsere Liste leer und wir starten mit einem Array  der Größe 1.



Die elementaren Methoden get und size
Die Methoden get und size in Listing  ergeben sich direkt aus unserer Entscheidung, die Folge  durch  zu repräsentieren.
Sie besitzen eine Laufzeit von , da wir in konstanter Zeit auf die
Elemente von  zugreifen können.
[caption=Methoden get und size einer ArrayListe, label=lst:elemds:arraylist:setgetsize]
class ArrayList:
  A = Array(1) # Array hat zu Beginn Größe 1
  n = 0     # Größe von L (= Anzahl belegter Plätze in A)

  # Liefert das Element an Position i von L
  function get(i):
    if i >= n:
      error "Index out of bounds"

    return A[i]

  # Liefert die Anzahl an Elementen in L
  function size():
    return n

Die Methode find
Listing  beschreibt die Methode find(x).
Wir durchlaufen einmal das Array  auf der Suche nach .
Obwohl wir die Suche abbrechen, sobald wir gesuchte Element gefunden haben,
besitzt die Methode eine asymptotische Worst-Case-Laufzeit von ,
wenn  genau  Elemente enthält.
Das liegt daran, dass das gesuchte Elemente an der letzten Position der Liste
liegen könnte oder vielleicht gar nicht in  gespeichert ist (so dass wir alle
 Elemente von  betrachten müssen).
[caption=Methode find einer ArrayListe, label=lst:elemds:arraylist:find]
class ArrayList:
  ...
  # Liefert die Position von x in L,
  # oder null falls x nicht in L enthalten.
  function find(x):
    for j = 0,...,n-1:
      if A[j] == x:
        return j

    return null

Die Methode append
Die Methode append(x) fügt  hinter dem letzten Listenelement ein.
Dabei müssen wir sicherstellen, dass in  noch Platz für ein weiteres Element ist
und anderenfalls  vergrößern.
Dies erledigen wir mit der Hilfsmethode enlarge().
[caption=Methode append einer ArrayListe, label=lst:elemds:arraylist:append]
class ArrayList:
  ...
  # Hängt x an die Liste an
  function append(x)
    # Vergrößere A, falls kein Platz für neues Element vorhanden
    if n == A.laenge:
      enlarge()

    # Füge x an erster freien Position ein.
    A[n] = x
    ++n
Wird die Methode enlarge() nicht aufgerufen, so arbeitet append
in konstanter Zeit.
Um die Worst-Case-Laufzeit der Methode abschätzen zu können, müssen wir zunächst
enlarge genauer betrachten.
[caption=Hilfsmethode enlarge einer ArrayListe, label=lst:elemds:arraylist:enlarge]
class ArrayList:
  ...
  function enlarge():
    B = Array(2*A.laenge)
    for j=0,...,n-1:
      B[j] = A[j]

    A = B


Im Allgemeinen können wir nicht sicherstellen, dass der Speicherbereich hinter 
frei ist.(Wir erinnern uns, dass Arrays garantieren, dass alle Elemente
in einem zusammenhängenden Speicherbereich abgelegt sind; wir müssen also  auch im
Speicher direkt hinter  ablegen.)
Daher erzeugt enlarge zunächst ein neues (größeres) Array , kopiert die
Einträge von  nach  und ersetzt anschließend  durch .


Dieses Vorgehen benötigt eine Laufzeit von , da alle  Einträge von
 kopiert werden müssen.
Vergrößern wir  nur um eine Position, ist  bei der nächsten append-Operation
wieder an der Kapazitätsgrenze.
In diesem Fall besitzen  Aufrufe von append also eine Gesamtlaufzeit
von .
Verdoppeln wir hingegen bei jedem enlarge-Aufruf die Größe von  müssen wir 
weitere Elemente einfügen müssen, bevor  erneut vergrößert werden muss.

Die Vergrößerung erfordert immer noch eine Laufzeit von , wenn 
Elemente in  gespeichert sind.
Wir können jetzt aber nachweisen, dass  Aufrufe von 
Zeit  benötigen.

Die asymptotische Worst-Case-Laufzeit von  Aufrufen von append
beträgt .

Zur Vereinfachung betrachten wir die Situation, in der keine Elemente aus 
gelöscht werden.
Das ist keine Einschränkung: Wenn auch Elemente gelöscht werden, benötigen wir
weniger Vergrößerungsoperationen und die Gesamtlaufzeit der  Aufrufen
von append kann höchstens geringer ausfallen.

Da wir die Größe von  mit 1 initialisieren und anschließend stets verdoppeln,
ist die Größe von  immer eine Zweierpotenz.
Betrachten wir zunächst eine unendliche Folge von append-Operationen:
Wir verdoppeln für jedes  die Größe von  von  auf ,
wenn das -te Element eingefügt wird.
Der Laufzeit dieser Verdopplung ist proportional zu .
Stoppen wir nach  Elementen, wird die Größe von  genau 
mal verdoppelt.
Damit erhalten wir als Gesamtlaufzeit:
*
Hier haben wir verwendet, dass  ist (s. Lemma  im Anhang.)
























































Die Methode delete
Die Methode delete arbeitet in zwei Schritten:
Zunächst muss sie das linkeste Vorkommen von  finden oder entscheiden, dass
es kein Vorkommen von  in  gibt.
Dazu können wir die Methode find verwenden.
Löschen wir ein Element , das nicht am Ende von  gespeichert ist, entsteht
ein Loch, d.h. ein unbelegter Platz in .
Wir schließen dieses Loch, indem wir im zweiten Schritt alle Elemente rechts von
 um eine Position nach links verschieben.
[caption=Methode delete einer ArrayListe, label=lst:elemds:arraylist:delete]
class ArrayList:
  ...
  function delete(x):
    i = find(x)
    if i == null:
      return

    for j=i+1,...,n-1:
      A[j-1] = A[j]

    -n



  Die gängigen Programmiersprachen besitzen Implementierungen von Arraylisten.
  
  [Java:] java.utils.ArrayList, java.utils.vector
  [C++:]  std::vector aus <vector>
  [Python:] list, []
  



Einfach verkettete Listen
Auch verkettete Listen dienen dazu, Folgen zu speichern. Anstatt aber die
Listenelemente in einem Array zu verwalten, legen wir auch die Listenelemente
an beliebigen Speicherstellen ab und verbinden (verketten) sie mit
Hilfe einer Zeigerstruktur.
Jedes Listenelement zeigt dann auf diejenige Speicherstelle, die das nächste
Listenelement - seinen Nachfolger - enthält (sofern ein solches existiert).
Eine solche Struktur heißt verkettete Liste.

Schnittstelle und Verwaltung der Elemente
Jedes Listenelement muss also einen Schlüssel - hier eine Zahl - und einen
Verweis auf das nächste Listenelement speichern.
[caption=Klasse für die Elemente einer einfach verketteten Liste, label=lst:elemds:linkedlist:elements]
class ListElement(x):
    key = x
    next = null
Die Liste selbst speichert zwei Platzhalterelemente, die Start
(head) und Ende (tail) der Liste repräsentieren.
Initial besteht die Liste aus diesen beiden Elementen, so dass tail
der Nachfolger von head ist.
Alle echten Elemente der Liste werden zwischen head und tail
eingefügt.
[caption=einfach verkettete Liste, label=lst:elemds:linkedlist:init]
class LinkedList:
    head = ListElement(null) # Platzhalter
    tail = ListElement(null) # Platzhalter
    head.next = tail # initial keine Elemente zwischen head/tail
    n = 0            # Groesse der Liste

Wir möchten im wesentlichen die gleiche Schnittstelle unterstützen wie zuvor.

  
  
  
  [append(x)] Fügt ein neues Element mit Schlüssel  an das Ende der
  Liste ein.
  [insert(pred, x)] Fügt ein Element mit Schlüssel  hinter dem
  Listenelement  ein.
  
  
  Es ist ein Fehler, wenn  ist.







  [find(x)] Liefert das linkeste Element der Liste mit Schlüssel .
  Falls kein Element mit Schlüssel  in der Liste enthalten ist, liefert die Methode tail
  zurück.
  [pred(e)] Für ein Listenelement : Liefert den Vorgänger von Element  zurück.
  Falls  das erste Element in der Liste ist, ist sein Vorgänger das Element head.
  Enthält die Liste  nicht, liefert die Methode das Element tail zurück.
  [delete(e)] Für ein Listenelement : Verändert die Liste nicht, falls
  .
  Anderenfalls wird  aus  entfernt.
  [size()] Liefert die Anzahl der Elemente in der Liste zurück.
  [get(i)] Liefert das -te Listenelement zurück, für .


[T]
    
      linkedlist-in-memory
      
        Eine verkettete Liste im Speicher; Speicherzellen sind
    schematisch () dargestellt.
    Jedes Listenelement speichert seinen Schlüssel (Zahl) und eine
    Referenz (Pfeil) auf die Speicherzelle mit dem nächsten Listenelement,
    seinem Nachfolger.
    Die Liste beginnt mit dem Platzhalterelement head und endet
    mit dem Platzhalterelement tail.
    Der Nachfolger von tail ist null und nicht eingezeichnet.
    Abgesehen von head besitzt jedes Listenelement einen eindeutigen
    Vorgänger in der Verkettung, auf den jedoch keine Referenz gespeichert wird.
    Der verwendete Speicher ist im Allgemeinen nicht zusammenhängendend.
  
  [T]
    
      linkedlist-example
      
        Vereinfachte Darstellung der gleichen verketteten Liste.
    Eine beispielhafte verkettete Liste.

Implementierung einer einfach verketteten Liste
Im Folgenden implementieren wir die Methoden einer einfach verketteten Liste.
Zur einfacheren Übersicht verwenden wir dabei die Variablen  und 
um Listenelemente zu kennzeichen und die Variable , um Schlüssel zu
bezeichnen.

Elemente suchen: Die Methoden find und pred
Um ein Gefühl für die Funktionsweise der Liste zu bekommen, betrachten wir
in Listing  zunächst die Methode find.
Hier starten wir mit dem ersten echten Element der Liste
(dem Nachfolger von head) und überprüfen, ob es den gesuchten
Schlüssel  besitzt. Falls ja, haben wir das gesuchte linkeste Element
der Liste gefunden; anderenfalls suchen wir bei seinem Nachfolger weiter.
Wir stoppen unsere Suche, wenn wir das Endelement tail erreichen.
[caption=Methode find einer einfach verketteten Liste, label=lst:elemds:linkedlist:find]
class LinkedList:
    ...
    # finde linkestes Element mit Schluessel x
    function find(x):
      # erstes echtes Listenelement
      e = head.next

      # suche von links nach rechts
      while e != tail and e.key != x:
        e = e.next

      return e
Im schlimmsten Fall ist kein Element mit Schlüssel  in der Liste enthalten.
Wir müssen die gesamte Liste durchlaufen, um uns von diesem Umstand zu überzeugen,
so dass die Methode find eine Worst-Case-Laufzeit von  besitzt,
wenn die Liste  Elemente enthält.
Analog können wir auch den Vorgänger eines Elementes bestimmen und die Methode
pred in Listing  implementieren.
[caption=Methode pred einer einfach verketteten Liste, label=lst:elemds:linkedlist:pred]
class LinkedList:
    ...
    # finde Vorgaenger von Listenelement e
    function pred(e):
      # Kandidat fuer Vorgaenger
      p = head

      # suche von links nach rechts
      while p != tail and p.next != e:
        p = p.next

      return p

Elemente einfügen: Die Methoden insert und append
Das Einfügen eines neuen Elementes in eine verkettete Liste muss die
Verkettungsstruktur aufrecht erhalten.
Fügen wir ein neues Element  hinter einem Element  ein, so muss nach
der Einfügeoperation  auf  und  auf den früheren Nachfolger
von  zeigen. Dies erreichen wir in zwei Schritten (s. Abbildung ):

 Wir setzen den Nachfolger  von  auf .
 Wir setzen den Nachfolger  von  auf .
Die Reihenfolge der Schritte ist wichtig damit nicht
 überschrieben wird und wir die Information verlieren, wo
sich der (frühere) Nachfolger von  im Speicher befindet.
[caption=Methode insert einer einfach verketteten Liste, label=lst:elemds:linkedlist:insert]
class LinkedList:
    ...
    function insert(p, x):
        if p == tail:
            error "Cannot insert after end of list."

        # neues Listenelement mit Schluessel x
        e = ListElement(x)
        # Schritt 1
        e.next = p.next
        # Schritt 2
        p.next = e
        # Groesse der Liste waechst um 1
        n++
        return e
Die Methode gibt das erzeugte Listenelement zurück, da der Benutzer eine Referenz
auf das Listenelement  (und nicht den Schlüssel ) benötigt, um
ein Element hinter  einzufügen.
Die Worst-Case-Laufzeit von insert ist .
Würden wir der Methode allerdings nicht  sondern lediglich den Schlüssel
 von  übergeben, müssten wir zunächst mit pred in Zeit 
das zugehörige Listenelement suchen.
Außerdem wäre die Einfügeposition von  nicht eindeutig, wenn es
mehrere Elemente mit Schlüssel  gibt.


fig-linkedlist-insert

In eine verkettete Liste soll ein neues Element mit Schlüssel 5 zwischen
den Elementen 1 und 7 eingefügt werden.
Zunächst setzen wir den Nachfolger des neuen Elements 5 auf 7, den bisherigen
Nachfolger von 1.
Anschließend können wir den Nachfolger von 1 auf das neue Element 5 umsetzen.
Das neue Element 5 ist nun in die verkettete Struktur integriert.


Die Methode append kann insert aufrufen, muss allerdings
zunächst das letzte Element der Liste - also den Vorgänger von tail
- mit Worst-Case-Laufzeit  suchen.
[caption=Methode append einer einfach verketteten Liste, label=lst:elemds:linkedlist:append]
class LinkedList:
    ...
    function append(x):
        # suche letztes echtes Element der Liste
        p = pred(tail)
        # fuege zwischen p und tail ein
        insert(p, x)




Elemente löschen: Die Methode delete


fig-linkedlist-delete

Das Element mit Schlüssel 1 soll aus der verketteten Liste entfernt
werden. Wir setzen den Nachfolger von 8 auf 7, den bisherigen Nachfolger von 1.
Die verkettete Struktur überspringt nun das Element mit Schlüssel 1, so dass
dessen Speicher freigegeben werden kann.
Um ein Element  aus der Liste zu löschen muss es aus der Verkettungsstruktur
ausgehakt werden.
Dazu müssen wir zunächst den Vorgänger  von  bestimmen, denn wenn wir
 entfernen, wird der frühere Nachfolger  von  zum Nachfolger von ,
so dass wir  aktualisieren müssen.
Wir setzen diese Idee in Listing  um.
[caption=Methode delete einer einfach verketteten Liste, label=lst:elemds:linkedlist:delete]
class LinkedList:
    ...
    function delete(e):
        # suche Vorgaenger von e
        p = pred(e)
        # ist e in der Liste enthalten?
        if p == tail:
            return

        # Nachfolger von e wird Nachfolger von p
        p.next = e.next
        # ggf. Speicher von e freigeben
        delete e
        # Groesse der Liste sinkt um 1
        n-
Im Worst-Case arbeitet die Methode delete in Zeit , da das Suchen des
Vorgängers von  Zeit  in Anspruch nimmt und die restliche
Methode in Zeit  abläuft.
Im Gegensatz zur Methode delete der Arraylisten arbeitet die Methode
schneller, wenn zu Beginn der Liste gelöscht wird.
Die Methode lässt sich mit Worst-Case-Laufzeit  implementieren, wenn wir
anstatt  den Vorgänger von  übergeben.
Dann obliegt es aber dem Benutzer, den Vorgänger von  zu finden.


Die Methode get

Da wir die Listenelemente nicht mehr notwendigerweise in einem zusammenhängenden
Speicherbereich ablegen, ist es nun schwieriger, die Position des -ten
Elementes im Speicher zu bestimmen.
Da wir nicht mehr direkt auf das -te Element zugreifen können, starten wir bei
head und folgen  mal der Nachfolgerreferenz.
Eine Implementierung der Methode befindet sich in Listing .
[caption=Methode get einer einfach verketteten Liste, label=lst:elemds:linkedlist:get]
class LinkedList:
    ...
    function get(i):
        if i >= n:
            error "Index out of bounds."

        e = head
        for j=0,...,i:
            e = e.next

        return e

    function size()
        return n
Die Methode läuft in Zeit .

Doppelt verkettete Listen
Die Laufzeit der Methoden append, insert und delete
hängt davon ab, ob wir den Methoden neben dem einzufügenden bzw. zu löschendem
Element auch noch dessen Vorgängerelement übergeben.
Sofern wir das Vorgängerelement übergeben, können alle drei Methoden in Worst-Case-Laufzeit
 implementieren; muss das Vorgängerelement erst gesucht werden, steigt
die Worst-Case-Laufzeit auf .
Wir schließen, dass es sich lohnt, in jedem Listenelement Nachfolger- und
Vorgänger zu speichern.
Auf diese Weise erhalten wir eine doppelt verkettete Liste.
[caption=Klasse für die Elemente einer doppelt verketteten Liste, label=lst:elemds:doublylinkedlist:elements]
class DListElement(x):
    key = x
    next = null
    prev = null
Die doppelt verkettete Liste implementiert die gleiche Schnittstelle wie die
einfach verkettete Liste; lediglich die Methode pred wird nicht mehr
benötigt.
In der Initialisierung müssen wir nun zusätzlich den Vorgänger des
tail-Platzhalterelements auf head setzen.

[caption=Initialisierung einer doppelt verketteten Liste, label=lst:elemds:doublylinkedlist:init]
class DLinkedList:
    head = DListElement(null) # Platzhalter
    tail = DListElement(null) # Platzhalter

    head.next = tail # initial keine Elemente zwischen head/tail
    tail.prev = head

    n = 0            # Groesse der Liste

    function size():
        return n
Wie bei der einfach verketteten Liste arbeitet die Initialisierung in Zeit .
Die Implementierungen der Methoden find, get und size
können wir von der einfach verketteten Liste mit unveränderten Laufzeiten übernehmen.

Einfügen von Elementen: Die Methoden append und insert

Die Einfügeoperationen müssen nun zusätzlich Verweise auf die Vorgänger aktualisieren.
Dazu führen wir folgende Schritte aus, wenn wir ein neues Element  hinter Element 
einfügen möchten. Das Element  sei hier der Nachfolger  von .

 Wir aktualisieren zunächst den Vorgänger und Nachfolger von , indem wir 
 und  setzen.
 Damit sichern wir gleichzeitig die Information, wo sich  und  im Speicher befinden,
 ohne eine zusätzliche Variable zu benötigen.
 Jetzt können wir den Nachfolger von  aktualisieren, indem wir 
 setzen.
 Es bleibt, den Vorgänger  von  auf  zu aktualisieren. Da wir 
 nur in  gespeichert haben, setzen wir dazu .
Als Pseudocode der Methode ergibt sich Listing .
[caption=Methode insert der doppelt verketteten Liste, label=lst:elemds:doublylinkedlist:insert]
class DLinkedList:
    ...
    function insert(p, x):
        if p == tail:
            error "Cannot insert after end of list."

        # neues Listenelement mit Schluessel x
        e = DListElement(x)

        # Schritt 1
        e.prev = p
        e.next = p.next

        # Schritt 2
        p.next = e

        # Schritt 3
        e.next.prev = e

        # Groesse der Liste waechst um 1
        n++
        return e
Wie zuvor arbeitet die Methode insert im Worst-Case in Laufzeit .

Um die Methode append zu implementieren müssen wir ein neues Element hinter
dem Vorgänger von tail einfügen.
Diesen Vorgänger erhalten wir als tail.prev.
[caption=Methode append der doppelt verketteten Liste, label=lst:elemds:doublylinkedlist:append]
class DLinkedList:
    ...
    function append(x):
        return insert(tail.prev, x)
Wir erinnern uns, dass die Methode append einer einfach verketteten Liste
mit  Elementen eine Worst-Case-Laufzeit von  aufweist, da zunächst
in Zeit  das Vorgängerelement von tail gesucht werden muss.
Diese zeitaufwändige Suche entfällt hier und die Methode arbeitet auch im
Worst-Case in Zeit .

Löschen von Elementen: Die Methode delete

Bevor wir ein Listenelement löschen können, müssen wir es aus der vorhandenen
Listenstruktur ausklinken. Dazu sind zwei Schritte notwendig, in denen wir
jeweils ein zu löschendes Listenelement  sowie seinen Vorgänger  und
seinen Nachfolger  betrachten.

 Den Nachfolger von  aktualisieren wir so, dass er auf den
 bisherigen Nachfolger  von  zeigt. Wir erhalten  als ,
 so dass wir  setzen müssen.
 Den Vorgänger von  aktualisieren wir so, dass er auf den bisherigen
 Vorgänger  von  zeigt.
 Wir erhalten  als , so dass wir  setzen
 müssen.
Damit ergibt sich der Pseudocode aus Listing .

[caption=Methode delete der doppelt verketteten Liste, label=lst:elemds:doublylinkedlist:delete]
class DLinkedList:
    ...
    function delete(e):
        if e == head or e == tail:
            error "Cannot delete head/tail."

        # Schritt 1 mit p=e.prev
        e.prev.next = e.next

        # Schritt 2 mit s=e.next
        e.next.prev = e.prev

        # ggf. Speicher freigeben
        delete e

        # Verringere Listengroesse um 1
        n-
Wir erhalten eine Methode, die im Worst-Case in Laufzeit  arbeitet.
Auch hier profitieren wir davon, dass wir das Vorgängerelement von  nicht
suchen müssen.

Verbinden von Listen: Die Methode concat

Wir erweitern unsere Listenschnittstelle um eine Methode, die es uns erlaubt,
eine Listen  an die aktuelle Liste  anzuhängen.
Dabei möchten wir nicht die Elemente aus  nach  kopieren, sondern
alle Elemente von  nach  verschieben (so dass  im Anschluss eine
leere Liste ist).

 [concat()] Hängt alle Elemente der Liste  am Ende von  an und leert .
Gegeben  könnten wir concat(L') implementieren,
indem wir für  die Methode  aufrufen danach alle Elemente
aus  entfernen.
Diese Implementierung besitzt jedoch eine Worst-Case-Laufzeit von .
Wir können die Methode auch in Worst-Case-Laufzeit  implementieren, indem
wir ausnutzen, dass die Elemente in  bereits korrekt verkettet sind; wir müssen
lediglich das erste Elemente von  zum Nachfolger des letzten Elementes von 
machen und L.tail als Nachfolger des letzten Elementes von 
einsetzen.
Um dabei die Listenstruktur aufrecht zu erhalten führen wir drei Schritte durch:

 Das erste Element von  wird Nachfolger des letzten Elements von ;
 umgekehrt wählen wir als Vorgänger des ersten Elements von  das letzte Element
 von .
 Das tail-Element von  wird der Nachfolger des letzten Elements von ;
 umgekehrt wählen wir als Vorgänger von L.tail das letzte Element von .
 Der Nachfolger von L'.head wird L'.tail; umgekehrt wählen wir als
 Vorgänger von L'.tail nun L'.head.
[caption=Methode concat einer doppelt verketteten Liste]
class DLinkedList:
  ...
  function concat(l):
    # Stoppe falls l leer ist
    if l.head.next == l.tail:
      return

    # haenge letztes Element vor erstes Element von l
    tail.prev.next = l.head.next
    l.head.next.prev = tail.prev

    # haenge letztes Element von l vor tail
    l.tail.prev.next = tail
    tail.prev = l.tail.prev
    n = n + l.n

    # leere l
    l.head.next = l.tail
    l.tail.prev = l.head
    l.n = 0


Übersicht: Laufzeiten von Listenoperationen



Stacks

Die Datenstruktur Stack (deutsch: Stapel) erlaubt es, Schlüssel einzufügen
und wieder zu entnehmen. Dabei wird die Einfügereihenfolge gespeichert und die Schlüssel
können nur in der umgekehrten Einfügereihenfolge wieder entnommen werden: Um auf einen
Schlüssel  im Stapel zugreifen zu können, müssen wir zunächst alle Schlüssel entfernen,
die nach  eingefügt wurden.
Dieses Prinzip nennt man LIFO: Last-in, first-out.
Wir können uns den Stack als einen Tellerstapel im Geschirrschrank vorstellen, von
dem immer jeweils nur der oberste Teller - der zuletzt eingefügte Schlüssel -
entfernt werden kann.
Abbildung  und das Beispielprogramm in Listing  veranschaulichen die Funktionsweise.

Der Stack unterstützt folgende Schnittstelle:

  [push(x)] Legt den Schlüssel  oben auf den Stack.
  [pop()] Entfernt den obersten Schlüssel des Stacks und gibt ihn zurück.
  [top()] Liefert den obersten Schlüssel des Stacks zurück.
  [empty()] Liefert false, falls sich mindestens ein Schlüssel auf
  dem Stack befindet, sonst true.


fig-stack-example

Funktionsweise eines Stacks nach dem Last-In-First-Out-Prinzip.
Schlüssel können nur von oben entnommen werden.
Neue Schlüssel können nur oben auf den Stack gelegt werden.

[caption=Beispielprogramm mit einem Stack, label=lst:elemds:stack:example]
S = Stack()
S.push(1)
S.push(2)
S.push(3)
S.push(4)

while not S.empty():
  print S.pop()

#Ausgabe: 4 3 2 1

Wir untersuchen im Folgenden, wie wir einen Stack mit Hilfe von verschiedenen
Listen implementieren können.


Implementierung mit ArrayListen

Die Stack-Schnittstelle lässt sich implementieren, indem wir die Schlüssel im Stack
in einer ArrayListe verwalten.
Wird ein Schlüssel auf den Stack gelegt, fügen wir ihn am Ende der Arrayliste ein.
Um einen Schlüssel vom Stack herunterzunehmen, entfernen wir ihn vom Ende der Liste.
Um den Stack implementieren zu können, erweitern wir die ArrayListe geringfügig, indem
wir eine Methode deleteat(i) hinzufügen:

 [deleteat(i)] Löscht das Element an Position  aus der Arrayliste.
 Die Elemente mit den Indizes  werden um eine Position nach links verschoben,
 d.h. sie verringern ihren Index um 1. Die Methode verringert die Größe der Arrayliste um 1.
Die Methode deleteat(i) hat wie delete im Worst-Case eine Laufzeit von
, da sie im schlimmsten Fall  Elemente verschieben muss.

Aus dieser Idee ergibt sich die Implementierung in Listing .
Die Stack-Operationen eines Stacks mit  Schlüsseln erben die Laufzeiten derjenigen
Arraylisten-Operation, die sie aufrufen:
Die Methode push ruft ArrayList.append mit einer
Worst-Case eine Laufzeit von  auf.
Der Worst-Case besteht darin, dass die Arrayliste vergrößert werden muss und kann
auch beim Aufruf durch push auftreten.
Wir haben also push mit einer Worst-Case-Laufzeit von 
implementiert.
Die amortisierte Laufzeit von push beträgt wie diejenige von ArrayList.append .
Die Methode top arbeitet wie ArrayList.get auch im Worst-Case in Zeit .
Lediglich bei der Methode pop() müssen wir genauer hinschauen:
Sie ruft ArrayList.deleteat mit einer Worst-Case-Laufzeit von  auf.
Wir erinnern uns aber, dass diese Worst-Case-Laufzeit daher stammt, dass im schlimmsten Fall am
Anfang der Arrayliste entfernt wird und  Elemente verschoben werden müssen.
Da wir aber nur am Ende der Arrayliste löschen, tritt der schlimmste Fall nicht ein.
Tatsächlich können wir das Element am Ende der Arrayliste in konstanter Zeit entfernen,
und haben damit die Methode pop() mit Worst-Case-Laufzeit 
implementiert.
[caption=Implementierung eines Stapels mit einer Arrayliste, float, label=lst:elemds:stack:arraylist]
class Stack:
  A = ArrayList()

  # fuege x am Ende der Arrayliste ein
  function push(x):
    A.append(x)

  # liefert das letzte Element der Arrayliste
  function top():
    if empty():
      error "Empty stack has no first element."

    last = A.size-1
    return A.get(last)

  # liefert das letzte Element der Arrayliste
  # und entfernt es.
  function pop():
    if empty():
      error "Empty stack has no first element."

    last = A.size-1
    x = A.get(last)
    A.delete_at(last) # loescht Element mit Index last
    return x

  # Der Stack ist leer, wenn die Arrayliste
  # keine Elemente enthält.
  function empty():
    return A.size() == 0

Implementierung mit einfach verketteten Listen
Um alle Stack-Operationen auch im Worst-Case in konstanter Zeit umzusetzen,
verwalten wir in Listing  die Schlüssel auf dem
Stack in einer einfach verketteten Liste.
Hier fügen ein neues Element am Anfang der Liste ein, wenn ein neuer
Schlüssel auf den Stack gelegt wird und entfernen auch dort wieder, wenn ein
Schlüssel vom Stack genommen wird.
[caption=Implementierung eines Stapels mit einer verketteten Liste, label=lst:elemds:stack:linkedlist]
class Stack:
  L = LinkedList()

  # fuege x am Anfang der Liste ein
  function push(x):
    L.insert(L.head, x)

  # liefert den Schluessel des ersten Listenelements
  function top():
    if empty():
      error "Empty stack has no first element."

    return L.head.next.key

  # Entfernt das erste Listenelement und liefert
  # dessen Schluessel.
  function pop():
    x = top()
    L.delete(L.head.next)
    return x

  # Der Stack ist leer, wenn L keine Elemente
  # enthaelt.
  function empty():
    return L.size() == 0
Da LinkedList.insert und LinkedList.size eine
Worst-Case-Laufzeit von  besitzen, arbeiten die Methoden push,
top und empty ebenfalls in Zeit .
Die Methode pop ruft LinkedList.delete auf, so dass wir
vermuten könnten, dass die Worst-Case-Laufzeit der Methode 
beträgt.
Da wir jedoch nur am Anfang von  löschen, tritt der Worst-Case der Methode
 nicht ein; die Methode arbeitet im Worst-Case in Zeit .

Queues
Eine Queue (Warteschlange) erlaubt wie ein Stack das Einfügen
und Löschen von Schlüsseln.
Sie setzt aber das First-In-First-Out-Prinzip (FIFO) um: Damit wir auf einen
Schlüssel  zugreifen können, müssen zunächst alle Schlüssel entfernt werden, die
vor  eingefügt wurden.
Der Zugriff auf die Schlüssel ist also nur in der gleichen
Reihenfolge erlaubt, in der sie eingefügt wurden.
Genauer unterstützt eine Queue die Operationen:

  [push(x)] Reiht den Schlüssel  am Ende der Queue ein.
  [top()] Liefert den aktuell vordersten Schlüssel der Queue.
  [pop()] Entfernt den vordersten Schlüssel aus der Queue und liefert ihn zurück.
  [empty()] Liefert false, falls die Queue mindestens einen Schlüssel
  enthält, sonst true.

fig-example-queue

Eine Queue nach dem First-In-First-Out-Prinzip. Neue
Schlüssel werden am Ende der Queue angehangen. Schlüssel können nur am
Anfang entnommen werden.
Abbildung  zeigt die gleiche Befehlssequenz für einen Stack.
Abbildung  und das Beispielprogramm in Listing illustrieren die Funktionsweise einer Queue (vgl. mit dem Beispielprogramm für einen Stack
in Listing ).
[caption=Beispielprogramm mit einer Queue, label=lst:elemds:queue:example]
Q = Queue()
Q.push(1)
Q.push(2)
Q.push(3)
Q.push(4)

while not Q.empty():
  print Q.pop()

#Ausgabe: 1 2 3 4

Implementierung mit doppelt verketteten Listen

Unsere ArrayListen erlauben ein schnelles Einfügen und Löschen nur am Ende der Liste.
Einfach verkettete Listen erlauben ein schnelles Einfügen und Löschen nur am Anfang der
Liste.
Um eine Queue zu implementieren, müssen wir aber am Anfang der Liste löschen und
an ihrem Ende einfügen (oder umgekehrt).
Wir verwenden daher in Listing  eine doppelt
verkettete Liste, um unsere Queue zu implementieren.
Alle Operationen der Queue arbeiten bei dieser Implementierung in konstanter Zeit.

[caption=Implementierung einer Queue mit einer doppelt verketteten Liste, label=lst:elemds:queue:dlinkedlist]
class Queue:
  L = DLinkedList()

  # fuegt x am Ende der Liste ein
  function push(x):
    L.append(x)

  # liefert den Schluessel des erstes Listenelements
  function top():
    if empty():
      error "Empty queue has no first element."
    return L.head.next.key

  # entfernt das erste Listenelement und liefert dessen Schluessel
  function pop():
    if empty():
      error "Empty queue has no first element."

    x = top()
    L.delete(L.head.next)
    return x

  # die Queue ist leer, wenn L keine Elemente enthaelt
  function empty():
    return L.size == 0


Eine Queue, die zusätzlich die Stack-Schnittstelle unterstützt
heißt Double-Ended Queue oder Dequeue.
Auch sie lässt sich mit Hilfe einer doppelt verketteten Liste so implementieren,
dass alle Operationen im Worst-Case in konstanter Zeit arbeiten.


Die Standardbibliotheken enthalten Implementierungen von Listen, Stacks und Queues.

 [Java] Implementiert eine (doppelt) verkettete Liste in java.util.LinkedList.
 Alle Klassen, die das Interface java.util.Deque-Interface implementieren sind
 Double-Ended-Queues und damit auch Stacks/Queues.
 Die Liste java.util.LinkedList liefert eine Implementierung des Interfaces auf
 Basis einer verketteten Liste; java.util.ArrayDeque implementiert das Interface auf Basis
 einer Arrayliste. Bei dieser Implementierung sind alle Queue/Stack-Operationen
 in amortisiert konstanter Zeit umgesetzt.
 [C++] Implementiert eine doppelt verkettete Liste in std::list in
 der Headerdatei <list>.
 Vorsicht: Die Methode size() läuft erst ab Version C++11 in
 garantiert in konstanter Zeit, davor erlaubt der Standard eine Implementierung
 mit linearer Worst-Case-Laufzeit.
 Eine einfach verkettete Liste ist ab C++11 in std::forward_list
 aus <forward_list> implementiert.
 Die Liste std::list ist gleichzeitig eine Double-Ended-Queue und kann als
 Stack/Queue verwendet werden. Das gleiche gilt für die Arrayliste std::vector
 aus <vector>; diese benötigt aber lineare Zeit für das Einfügen bzw.
 Löschen am Anfang der Liste.
 Darüberhinaus gibt es in std::deque aus <deque> eine Implementierung
 einer Double-Ended-Queue, die auf einer Kombination von Arrays und verketteten Listen
 besteht und das Einfügen und Löschen am Anfang und am Ende in konstanter Zeit erlaubt.
 Als Liste erlauben std::list und std::vector auch Zugriffe,
 die über pop und push hinausgehen; diese zusätzlichen
 Zugriffsmethoden können mit den mit den Adaptern std::stack bzw.
 std::queue vor dem Benutzer versteckt werden.
 [Python] Implementiert eine verkette Liste in der Klasse deque
 aus dem Modul Collections. Die verkettete Liste ist eine Double-Ended-Queue
 und kann daher als Stack oder Queue verwendet werden.
 Die Standard-Listenklasse list bzw. [] ist eine Arrayliste.




Bäume als Datenstruktur für dynamische Mengen
Datenstrukturen für dynamische Mengen

In Kapitel  haben wir uns damit beschäftigt, wie wir dynamische Folgen - Folgen zu denen wir Elemente hinzufügen und aus denen wir vorhandene Elemente entfernen können - mit Hilfe von Listen verwalten können.
Wir hatten es geschafft, Einfüge- und Löschoperationen mit konstanter Worst-Case-Laufzeit zu implementieren.
Lediglich die Laufzeit der Suchoperation find war in allen Fällen linear in der Länge der Folge.
Der Grund dafür ist, dass wir davon ausgegangen sind, dass der Benutzer die Reihenfolge der Elemente in der Datenstruktur bestimmen kann.
Suchen wir dann ein Element, so kann es der Benutzer an jeder beliebigen Position der Datenstruktur gespeichert haben.
Wir sind daher im Worst-Case gezwungen, alle Elemente der Datenstruktur zu betrachten - sonst können wir nicht sicher entscheiden, ob und wo sich das gesuchte Element in der Datenstruktur befindet.

Das Ziel in diesem und im nächsten Kapitel wird es sein, die Laufzeit der find-Operation zu verbessern.
Dazu gehen wir davon aus, dass nun die Datenstruktur und nicht der Benutzer die Reihenfolge der Elemente festlegt.
Diese neue Voraussetzung erlaubt es uns, die Elemente so anzuordnen, dass wir sie schnell wiederfinden können.
Wir modellieren diese Voraussetzung formal, indem wir unsere Datenstruktur nun eine (Multi-)Menge  anstelle einer Folge
verwaltet.(Formal besitzen die Elemente einer Folge eine Reihenfolge, die Elemente einer Menge nicht.)
Die Schnittstelle unserer Datenstruktur ist aber im Wesentlichen die gleiche wie im vorherigen Kapitel,
allerdings mit einem wichtigen Unterschied:
Der Einfügeoperation wird keine Einfügeposition übergeben, da diese nun von der Datenstruktur selbst bestimmt werden soll.
Folglich gibt es auch keine append-Operation mehr.
Um den Unterschied zu verdeutlichen nennen wir die Einfügeoperation hier add.

  [add(x)] Füge den Schlüssel  zu  hinzu.
  [find(x)] Wenn , liefere das (eindeutige) Element mit Schlüssel  zurück.
  Ist , gibt null zurück.
  [delete(x)] Wenn , lösche das Element mit Schlüssel  aus .
  Ist , verändere  nicht.
  [size()] Liefert die Anzahl der Elemente in  zurück.














Wir werden sehen, dass die Datenstruktur binärer Suchbaum in diesem Kapitel auch die Realisierung folgender Operation erlaubt:


  [sorted()] Gib alle Elemente aus  in aufsteigender Reihenfolge aus.


Elementare Datenstrukturen








Wir haben nun also die Wahl, welche Reihenfolge wir für die Listenelemente verwenden.
Geschickt ist es etwa, die Elemente sortiert abzuspeichern, also so, dass ein Element mit größerem Index auch einer größeren Zahl entspricht als ein Element mit kleinerem Index.
Siehe dazu Abbildung .
In der oberen Liste haben wir keinen Anhaltspunkt, wo sich Elemente befinden, und müssen im schlechtesten Fall die ganze Liste durchsuchen (z.B., um sicherzugehen, dass ein Element nicht enthalten ist). In der unteren Liste können wir die Methode find(x) so abwandeln, dass sie die Suche abbricht, sobald wir das gesuchte Element oder ein größeres Element gefunden haben. Auch mit dieser Verbesserung bleibt es jedoch so, dass die Methode find(x) im schlechtesten Fall eine Laufzeit von  hat. Damit hat auch die Methode find(i) unserer dynamischen Menge eine Laufzeit von  im schlechtesten Fall. Und da wir auch beim Einfügen oder Löschen eines Elementes zunächst herausfinden müssen, ob das Element in der Datenstruktur enthalten ist, erhalten wir für alle Methoden außer size() eine Laufzeit von .
Verwenden wir anstatt einer verketteten Liste eine Arrayliste, können wir find mit einer binären Suche mit Worst-Case-Laufzeit  implementieren.
Dennoch benötigt auch hier das Einfügen und Löschen weiterhin eine Worst-Case-Laufzeit von 


fig-list-as-set

Zwei doppelt verkettete Listen, 

Binäre Suchbäume
Binäre Suchbäume verwenden eine Verkettungsstruktur ähnlich zu verketteten Listen und
haben ihren Namen daher, dass man in ihnen sehr viel besser suchen kann.
Außerdem erinnert der Name an die binäre Suche, die wir im Einleitungskapitel kennengelernt haben. Was haben binäre Suchbäume und binäre Suche also gemeinsam?

Die Idee ist, dass wir die Elemente in einer Baumstruktur speichern, die es erlaubt, auf ähnliche Weise zu suchen wie bei der binären Suche. Wir betrachten zunächst in Abbildung  ein Beispiel, in dem das sehr gut funktioniert. Wir haben hier Elemente mit der Schlüsselmenge  abgespeichert. Um in dieser Menge gut suchen zu können, haben wir die Elemente nicht doppelt verkettet, sondern die Zeiger so gesetzt, dass man sich in der Menge gut zurechtfinden kann. Wir sehen ganz oben im Bild einen Zeiger root, der auf das Element mit dem Schlüssel  zeigt.
Wir bezeichnen dieses Element auch als Wurzel.
Von diesem Element gehen nun zwei Zeiger aus: Einer zeigt auf ein Element mit kleinerem Schlüssel, einer auf ein Element mit größerem Schlüssel.
Auch bei allen anderen Elementen im Suchbaum sind die beiden Zeiger auf diese Weise gesetzt.
Betrachten wir den Suchbaum genauer, so stellen wir fest, dass tatsächlich alle Elemente im linken Teilbaum unter der Wurzel einen Schlüssel besitzen, der kleiner als der Schlüssel 6 der Wurzel ist.
Alle Elemente im rechten Teilbaum unter der Wurzel besitzen hingegen einen Schlüssel, der größer als der Schlüssel 6 der Wurzel ist.
Mehr noch: Diese Eigenschaft gilt nicht nur für die Wurzel des Baumes, sondern für jedes beliebiges Element.




  Suchen wir nun also ein Element - zum Beispiel das mit Schlüssel  - müssen wir nicht mehr den gesamten Suchbaum durchsuchen: Da der Schlüssel des Wurzelknotens größer als 4 ist, wissen wir, dass das Element mit Schlüssel 4 sich entweder im linken Teilbaum unter der Wurzel befinden muss oder gar nicht im Suchbaum enthalten ist.
  Wir folgen also dem linken Zeiger und suchen dann rekursiv weiter.
  In diesem Sinne ähnelt das Vorgehen stark einer binären Suche.









  fig-bin-tree-example-complete-tree
 
 Ein vollständiger binärer Suchbaum.

Etwas Notation
Im Folgenden bezeichnen wir die Elemente des Suchbaumes als Knoten, und das root-Element als Wurzel.
Wir betrachten hier nur binäre Bäume, d.h. Bäume in denen jeder Knoten maximal zwei Nachfolger - einen linken und einen rechten - besitzt.
Zu dem linken Nachfolger  und dem rechten Nachfolger  eines Knotens  sagen wir Kinderknoten von  und nennen  den Elternknoten von  und .
Die Anzahl ,  oder  der Kinderknoten eines Knotens  nennen wir den Grad von .
Ein Knoten, der keine Kinderknoten besitzt heißt Blatt; alle anderen Knoten nennen wir innere Knoten des Baumes.
Jeder Knoten  ist die Wurzel eines Teilbaumes, der aus dem Knoten  selbst und allen anderen Knoten besteht, die entlang der Verzeigerung von  aus
erreicht werden können.
Wir nennen einen Weg entlang der Zeiger im Baum einen Pfad; die Länge eines Pfades ist die Anzahl der Zeiger, die er überquert (entlang geht).
Die Höhe eines Baumes ist die Länge eines längsten Pfades von der Wurzel zu einem Blatt.
Ein Baum mit Höhe  besitzt also  (Knoten-)Ebenen, die wir von  bis  indizieren.
Ein binärer Baum mit Höhe  heißt vollständig, wenn jeder innere Knoten zwei Kinder besitzt und alle Blätter auf Ebene  liegen.
Da sich die Anzahl der Knoten eines vollständigen binären Baumes auf jeder Ebene verdoppelt, besitzt er auf Ebene  gerade  Knoten und damit
insbesondere  Blätter.
Nach Formel  beträgt die Gesamtanzahl an Knoten in einem vollständigen binären Baum also .

Damit können wir nun den wichtigsten Begriff dieses Kapitels definieren: Ein binärer Baum heißt binärer Suchbaum, falls er
folgende Eigenschaft erfüllt.
Wenn wir einen beliebigen Knoten  des Suchbaumes mit einem beliebigen Schlüssel  betrachten, dann:

  besitzen alle Knoten, die im linken Teilbaum unter  gespeichert sind einen Schlüssel, der (echt) kleiner als  ist.
  besitzen alle Knoten, die im rechten Teilbaum unter  gespeichert sind einen Schlüssel, der (echt) größer als  ist.
Alle Operationen des binären Suchbaums erhalten diese beiden Eigenschaften als Invariante.

Implementierung von binären Suchbäumen

Ähnlich zu den verketten Listen verwenden wir Baumelemente, die Schlüssel und Nachfolgezeiger verwalten.
[caption=Elemente (Knoten) eines binären Baumes, label=trees:bintrees:node]
class Node(x):
  key = x
  left = null
  right = null
Der binäre Baum selbst enthält nur einen Zeiger auf das Wurzelelement.
Ist der Baum leer, so ist dieser Zeiger null.
[caption=Initialer binärer Baum, label=trees:bintrees:init]
class BinaryTree:
  root = null
  n = 0


Suchen in binären Suchbäumen
Wir präzisieren nun unser Vorgehen zum Suchen eines Schlüssels in einem binären
Suchbaum und suchen eine Implementierung für die Methode find(x).
Dabei verwenden wir unsere beiden Invarianten.

Die Idee der Methode find(x) ist folgende.
Wir vergleichen unseren Suchschlüssel  zunächst mit dem Schlüssel  des
Wurzelknotens (ist der Baum leer und besitzt daher keine Wurzel können wir sofort stoppen).
Ist  muss das Element mit Schlüssel  (wegen unserer Invarianten) im linken
Teilbaum unter der Wurzel liegen, sofern es überhaupt im Suchbaum enthalten ist.
Besitzt der Wurzelknoten kein linkes Kind, wissen wir also, dass  nicht im Suchbaum
gespeichert ist und wir stoppen.
Anderenfalls suchen rekursiv im linken Teilbaum unter der Wurzel weiter.
Analog gehen wir vor wenn  ist.
Dann wissen wir, dass sich das Element mit Schlüssel  wenn überhaupt im
rechten Teilbaum unter der Wurzel befinden kann.
Besitzt die Wurzel aber kein rechtes Kind, so befindet sich  nicht im
Suchbaum und wir stoppen.
Ansonsten suchen rekursiv in im rechten Teilbaum unter der Wurzel  weiter.
Im dritten Fall ist  und wir haben den gesuchten Knoten gefunden.
Abbildung  zeigt ein Beispiel für eine
erfolgreiche und eine erfolglose Suche.

  fig-example-bintree-search
 
 
 Im linken Bild ist der Suchpfad der (erfolgreichen) Suche nach Schlüssel  eingezeichnet, im rechten Bild der Suchpfad der (erfolglosen) Suche nach Schlüssel .

Zusammengefasst definieren wir eine Hilfsmethode find(x, r), die
 im Teilbaum unter  sucht. Wenn wir  im gesamten Baum suchen möchten, rufen
wir also find(x, root) auf.

  Ist  echt kleiner als r.key: Falls  ein linkes Kind
  besitzt, suche mit find(x, r.left) rekursiv im linken Teilbaum
  weiter. Anderenfalls gib null zurück.
  Ist  echt größer als r.key: Falls  ein rechtes Kind
  besitzt, suche mit find(x, r.right) rekursiv im rechten Teilbaum
  weiter. Anderenfalls gib null zurück.
  Ist  gleich r.key: Gib r zurück.
Der Vollständigkeit halber formulieren wir die Methode auch als Pseudocode in Listing .
[caption=Methode find eines binären Suchbaums, float, label=lst:trees:bintrees:find]
class BinaryTree:
  ...
  # Suche x im gesamten Suchbaum
  function find(x):
    if root == null:
      return null

    return find(x, root)

  # Suche x im Suchbaum unter r
  function find(x, r):
      if x < r.key:
        if r.left == null:
          return null
        else:
          return find(x, r.left)

      else if x > r.key:
        if r.right == null:
          return null
        else:
          return find(x, r.right)

      else:
        return r
Wir beobachten, dass wir in mit jedem rekursiven Aufruf um eine Ebene weiter im
Baum absteigen.
Besitzt der Baum Höhe , so stoppt die Rekursion also spätestens nach 
Aufrufen.
Es kann auch tatsächlich passieren, dass die Rekursion  Aufrufe durchführt;
nämlich z.B. dann, wenn sich das gesuchte Element auf Ebene  befindet.
Da der Aufwand pro einzelnem Aufruf konstant ist, besitzt die Methode also eine
Worst-Case-Laufzeit von .

Einfügen in binäre Suchbäume
Wir überlegen uns nun, wie wir einen Schlüssel  in einen binären Suchbaum
einfügen können, ohne die Invarianten zu verletzen.
Dazu legen wir uns darauf fest, dass neue Elemente immer als Blätter eingefügt
werden.
Anschließend gehen wir analog zur Methode find vor:
Ist der Suchbaum aktuell leer, erzeugen wir ein neues Element mit Schlüssel 
und machen es zur Wurzel des Baumes.
Anderenfalls betrachten wir zunächst den Schlüssel  der Wurzel root.
Ist , gehört  in den linken Teilbaum unter root.
Besitzt root kein linkes Kind, können wir  als linkes Kind von root
einfügen.
Anderenfalls suchen wir rekursiv im linken Teilbaum unter root weiter.
Ist  verfahren wir analog im rechten Teilbaum.
Abbildung  zeigt ein Beispiel dieses Vorgehens.

  fig-bintrees-add-example
 
 In den oben links abgebildeten binären Suchbaum werden nacheinander die Schlüssel ,  und  eingefügt (add(60), add(9), add(16)).
Präziser definieren wir uns eine Hilfsmethode add(x, r), die 
im Teilbaum unter  einfügt.
Wir können also add(x) implementieren, indem wir add(x, root)
aufrufen.
Die Hilfsmethode add(x,r) arbeitet nun wie folgt.

  Ist  echt kleiner als r.key und besitzt r
  ein linkes Kind, füge  mit add(x, r.left) rekursiv im linken Teilbaum ein.
  Anderenfalls erzeuge ein neues Element mit Schlüssel  und mache es
  zum linken Kind von r.
  Ist  echt größer als r.key und besitzt r
  ein rechtes Kind, füge  mit add(x, r.right) rekursiv im rechten Teilbaum ein.
  Anderenfalls erzeuge ein neues Element mit Schlüssel  und mache es
  zum rechten Kind von r.
  Ist  gleich r.key enthält der Baum Schlüssel 
  bereits und wir stoppen.
Listing  zeigt den Pseudocode dieser Methode.
[caption=Methode add eines binären Suchbaums, label=lst:trees:bintrees:add]
class BinaryTree:
  ...
  # Fuege Schluessel x in den Suchbaum ein
  function add(x):
    if root == null:
      root = Node(x)
    else:
      add(x, root)

  # Versuche x im Suchbaum unter r einzufuegen
  function add(x, r):
      if x < r.key:
        if r.left != null:
          add(x, r.left)
        else:
          r.left = Node(x)

      else if x > r.key:
        if r.right != null:
          add(x, r.right)
        else:
          r.right = Node(x)
Auch diese Methode hat im Worst-Case eine Laufzeit von  wenn wir in
einen Baum mit Höhe  einfügen.

Löschen in binären Suchbäumen

Das Löschen eines Knotens aus einem binären Suchbaum stellt sich als kompliziert
heraus.
Wir betrachten zunächst zwei einfache Fälle:

[Fall 1:] Ist  ein Blatt mit Elternknoten , können wir r.left bzw. r.right auf null
setzen, je nachdem ob  ein linkes bzw. rechtes Kind von  ist.
Ist  die Wurzel (besitzt also keinen Elternknoten), verwenden wir anstelle
von r.left und r.right den Wurzelzeiger root.
Die Situation ist in Abbildung  dargestellt.

  fig-bintree-delete-case-1
 
 Löschen in binären Suchbäumen, Fall 1: Blätter können einfach entfernt werden.
[Fall 2:] Besitzt  ein einziges Kind , setzen wir r.left bzw. r.right
auf , je nachdem ob  ein linkes bzw. rechtes Kind von  ist.
Ist  die Wurzel und besitzt also keinen Elternknoten, verwenden wir anstelle
von r.left und r.right den Wurzelzeiger root unseres
Baumes.
Diese Situation sehen wir in Abbildung .

  fig-bintree-delete-case-2
 
 Löschen in binären Suchbäumen, Fall 2: Knoten von Grad  werden durch ihr einziges Kind ersetzt.
Besitzt  allerdings zwei Kinder, kommen wir mit unserer Strategie nicht weiter:
Wir können den linken bzw. rechten Zeiger von  (und auch den Wurzelzeiger root)
nicht auf beide Kinder gleichzeitig zeigen lassen.
Der Trick ist hier, dass wir diesen schwierigen Fall auf die beiden einfachen
Fälle zurückführen können.
Dazu möchten wir  mit einem anderen Knoten  vertauschen, der höchstens ein
Kind besitzt.
Dabei dürfen wir allerdings  nicht mit einem beliebigen Knoten vertauschen:
Zum Beispiel muss der Schlüssel des neuen Knotens größer sein als alle Schlüssel
im linken Teilbaum unter  (ansonsten würden wir unsere Invariante verletzen).
Dies erreichen wir, indem wir einen Schlüssel aus dem rechten Teilbaum unter  wählen;
dieser Teilbaum enthält mindestens einen Knoten, nämlich das rechte Kind von .
Aber auch im rechten Teilbaum unter  dürfen wir nicht beliebig wählen: Wenn
wir unsere Invariante erhalten wollen, muss der neue Knoten einen Schlüssel besitzen,
der kleiner ist als alle anderen Schlüssel im rechten Teilbaum unter .
Wir wählen also den Knoten  mit dem kleinsten Schlüssel im rechten Teilbaum von .
Diese Wahl garantiert, dass nach dem Löschen von  die Invarianten gelten.
Außerdem können wir garantieren, dass  kein linkes Kind besitzt: Denn hätte 
ein linkes Kind ,  so hätte  einen kleineren Schlüssel als  und wir hätten  gewählt.
Folglich hat  höchstens ein Kind.
Indem wir anstatt  und  zu vertauschen lediglich die Schlüssel von  und  vertauschen,
ersparen wir uns das komplizierte Aktualisieren der Zeigerstruktur.
Abbildung  zeigt die Situation.

  fig-bintree-delete-case-3
 
 Löschen in binären Suchbäumen, Fall 3: Der zu löschende Knoten  mit Schlüssel  hat Grad .
 Dann ersetzen wir  zunächst durch den kleinsten Schlüssel , der größer ist als .
 Dieser Knoten hat höchstens Grad 1.
 Anschließend wird der Knoten, der nun Schlüssel  enthält, wie in Fall 1 oder Fall 2 entfernt.

Wir fassen unsere Löschmethode zusammen:
Suche zunächst den Knoten  mit Schlüssel  im Suchbaum.
Enthält der Suchbaum keinen solchen Knoten, stoppe.
Sonst:

  [1. Fall:] Der Grad von  ist 0, d.h.  ist ein Blatt.
  Ist  die Wurzel, setze root auf null.
  Anderenfalls sei  der Elternknoten von . Ist  ein linkes Kind von ,
  setze r.links auf null, sonst setze r.rechts auf null.
  [2. Fall:] Der Grad von  ist 1, d.h.  besitzt genau ein Kind .
  Ist  die Wurzel, mache  zur neuen Wurzel (d.h., setze root auf ).
  Anderenfalls sei  der Elternknoten von .
  Ersetze  durch , d.h.:
  Ist  ein linkes Kind von , setze r.links auf , sonst setze
  r.rechts auf .
  [3. Fall:] Der Grad von  ist 2, d.h.  besitzt genau zwei Kinder.
  Suche zunächst den Knoten  mit dem kleinsten Schlüssel im rechten Teilbaum
  von  (s. nächster Abschnitt). Dieser Knoten existiert, weil  zwei Kinder besitzt und daher der rechte
  Teilbaum nicht leer ist.
  Vertausche die Schlüssel von  und .
  Da  höchstens ein Kind besitzen kann, lösche  anschließend wie in Fall 1 oder Fall 2.
Abbildung  zeigt einen binären Suchbaum, in dem beispielhaft Schlüssel
gelöscht werden.

  fig-bintree-delete-example
 
 In dem oben links abgebildeten binären Suchbaum werden nacheinander die Schlüssel , , ,  und  gelöscht (delete(1), delete(3), delete(5), delete(42), delete(12)).


  fig-bintree-delete-detailed
 
 Ergänzung zu Abbildung : Ein Zwischenschritt beim Löschen des Elements mit Schlüssel  aus dem linken Suchbaum.

Wie wir in Fall 3 den kleinsten Schlüssel des rechten Teilbaums finden überlegen
wir uns im nächsten Abschnitt.
Dabei werden wir sehen, dass wir diese Aufgabe in einem Baum mit Höhe  in Zeit
 bestimmen können, so dass auch die Methode delete in Zeit
 arbeitet.

Zunächst formulieren wir noch die Methode delete als Pseudocode in
Listing .
Der Pseudocode dient hier nur dazu, interessierte Leser:innen zu überzeugen, wie
die Datenstruktur vollständig implementiert werden kann.
Er verwendet einige Hilfsmethoden, die im Anhang des Skriptes (Listing ) nachgeschlagen werden können.

[caption=Methode delete eines binären Suchbaums, label=lst:trees:bintrees:delete]
  class BinTree:
    ...
    function delete(x):
      delete(x, root, null)

    # Loescht den Knoten mit Schluessel x (falls vorhanden) aus
    # dem Teilbaum unter r. In p speichern wir den Elternknoten
    # von r oder null, falls r die Wurzel ist
    function delete(x, r, p):
      if x < r.key:
        # x ist nicht im Baum gespeichert
        if r.left == null:
          return

        # suche links rekursiv weiter
        delete(x, r.left, r)

      else if x > r.key:
        # x ist nicht im Baum gespeichert
        if r.right == null:
          return

        # suche rechts rekursiv weiter
        delete(x, r.right, r)

      else:
        # Knoten r hat Schluessel x
        if r.left != null and r.right != null:
          delete_deg2_node(r, p)   # r hat Grad 2

        else:
          delete_deg01_node(r, p) # r hat Grad 0 oder 1

      n -= 1

[caption=Löschen von Grad-2-Knoten in einem Suchbaum, float, label=lst:trees:bintrees:delete-2]
  class BinTree:
    ...
    # loesche den Grad 2 Knoten r mit Elternknoten p
    function delete_deg2_node(r, p):
      # finde Knoten s mit kleinstem Schluessel (und
      # dessen Elternknoten s_p) im rechten Teilbaum von r
      s, s_p = minimum(r.right, r)
      # tausche Schluessel von s und r
      swap(s.key, r.key)
      delete_deg01_node(s, s_p)


Finden des kleinsten Schlüssels eines Teilbaumes
Betrachten wir den Teilbaum  unter einem beliebigen Knoten  eines binären Suchbaums,
so erreichen wir den Knoten mit dem kleinsten Schlüssel in , indem
wir von  aus so lange wie möglich nach links laufen.
Um einzusehen warum das so ist, nehmen wir kurz an, dass wir den kleinsten Schlüssel 
und seinen Knoten  in  schon kennen.
Wenn wir  mit find(x, r) suchen, werden wir an jedem Knoten von 
nach links gehen, weil  kleiner ist als die Schlüssel aller Knoten (außer ) in .

Damit erhalten wir die folgende Methode:
Falls  kein linkes Kind besitzt, gibt  zurück. Anderenfalls suche rekursiv im Teilbaum unter r.left
weiter.
[caption=Finden des kleinsten Knotens in einem binären Suchbaum, label=lst:trees:bintrees:minimum]
class BinTree:
  ...
  function minimum():
    return minimum(root, null)

  # liefert den Knoten mit dem kleinsten Schluessel im Teilbaum
  # unter r und dessen Elternknoten p
  function minimum(r, p):
    if r.left != null:
      return minimum(r.left, r)

    else:
      return r, p
Sortierte Ausgabe der Menge

Indem wir den Suchbaum geschickt rekursiv durchlaufen erhalten wir die Schlüssel
in  in sortierter Reihenfolge.
Wir beobachten, dass für einen beliebigen Schlüssel  der linke Teilbaum 
unter  genau die Schlüssel aus  und  enthält, die kleiner als  sind.
Analog enthält der rechte Teilbaum  unter  genau die Schlüssel aus  und , die
größer als  sind.
Wir erhalten also die Schlüssel in  in sortierte Reihenfolge, wenn wir bei der Wurzel startend zunächst
rekursiv die Schlüssel in  sortiert ausgeben, dann  und dann rekursiv die Schlüssel
in  sortiert ausgeben.
Dieses Vorgehen nennt man Inorder-Traversierung des Suchbaumes.
Definieren wir also ein Hilfsmethode sorted(r), die die Schlüssel im
Teilbaum unter  in sortierter Reihenfolge ausgibt:

  Falls  ein linkes Kind  besitzt, rufe sorted(r.left) auf.
  Gib  aus.
  Falls  ein rechtes Kind  besitzt, rufe sorted(r.right()) auf.
Um alle Schlüssel in sortierter Reihenfolge zu erhalten, rufen wir sorted(root) auf.
Abbildung  zeigt ein Beispiel.

  
 Beispiel für einen Inorder-Durchlauf.
Wir erhalten den Pseudocode der Methode in Listing .
Da die Methode für jeden Knoten genau einmal aufgerufen wird und jeder Aufruf
abgesehen von der Rekursion in konstanter Zeit arbeitet beträgt die Worst-Case-Laufzeit der
Methode , wobei  die Anzahl der im Baum gespeicherten Schlüssel ist.
[caption=Ausgabe der Suchbaumschlüssel in sortierter Reihenfolge, label=lst:trees:bintrees:sorted]
class BinTree:
  ...
  function sorted():
    return sorted(root)

  function sorted(r)
    if r.left != null:
      sorted(r.left)

    print(r.key)

    if r.right != null:
      sorted(r.right)

Höhe von binären Suchbäumen
Die Worst-Case-Laufzeit der Operationen add, delete und find
unseres binären Suchbaumes beträgt , d.h. die Laufzeit ist linear in der Höhe  des Baumes.
Was ist aber die maximal mögliche Höhe für einen binären Suchbaum mit  Knoten?
In Kapitel  haben wir uns schon überlegt, dass die Höhe eines
binären Baumes mit  Knoten mindestens  beträgt.
Dieser Fall tritt ein, wenn der Baum - wie z.B. in Abbildung  -
vollständig ist, also jeder Knoten zwei Kinder hat und die unterste Ebene mit Blättern gefüllt ist.
Bei einer ungünstigen Einfügereihenfolge kann es aber passieren, dass der Baum entartet
und (fast) ausschließlich aus Grad-1-Knoten besteht.
In diesem Fall beträgt die Höhe des Baumes - und damit auch die
Worst-Case-Laufzeit der add-, delete- und find-Operationen -
gerade , s. Abbildung .

0.45
  fig-bin-tree-balanced-example
  
  Ein binärer Suchbaum mit den Schlüsseln  und minimaler Höhe.
  Der Baum resultiert z.B. aus der Einfügereihenfolge
  add(4),
  add(2),
  add(6),
  add(1),
  add(3),
  add(5),
  add(7).
0.45
  fig-bin-tree-unbalanced-example
  
  Ein binärer Suchbaum auf der gleichen Schlüsselmenge , aber mit
  maximaler Höhe.
  Dieser Baum entsteht etwa, wenn die Schlüssel in sortierter Reihenfolge
  add(1),
  add(2),
  add(3),
  add(4),
  add(5),
  add(6),
  add(7)
  eingefügt werden.
  
Zwei binäre Suchbäume mit der gleichen Schlüsselmenge .
Im Allgmeinen besitzt ein binärer Suchbaum mit  Schlüsseln im besten Fall eine Höhe
von  und im schlimmsten Fall eine Höhe von .


Balancierte binäre Suchbäume: Rot-Schwarz-Bäume

Unser Fazit aus dem vorherigen Abschitt ist, dass binäre Suchbäume eine schnelle
add-, delete- und find-Operation besitzen, wenn ihre
Höhe gering ist.
Dieser günstige Fall tritt ein, wenn der Baum balanciert ist, d.h.
wenn für jeden inneren Knoten gilt, dass der linke und der rechte Teilbaum
unter ihm in etwa die gleiche Höhe besitzen.
Im schlimmsten Fall allerdings verhalten sich binäre Suchbäume nicht besser
als einfach verkettete Listen; dies ist der Fall, wenn der Baum maximal
unbalanciert ist, d.h. wenn kein linker Teilbaum unter der Wurzel existiert,
aber der rechte Teilbaum unter der Wurzel Höhe  besitzt.

In diesem Abschnitt betrachten wir einen verbesserten binären Suchbaum, der
garantiert, dass seine Höhe beim Speichern von  Elementen schlimmstenfalls
 ist.
Dieser verbesserte Suchbaum heißt Rot-Schwarz-(Such-)baum.
Der Name des Baumes stammt daher, dass seine Knoten ein zusätzliches Attribut
verwalten, mit dem sie rot oder schwarz gefärbt werden.
Der Baum verwendet diese Färbung um bei Einfüge- und Löschoperationen
unbalancierte Teilbäume zu erkennen und sich durch das geschickte Verschieben von
Knoten auszubalancieren.

Da ein Rot-Schwarz-Baum insbesondere die Suchbaumeigenschaft besitzt, können wir
die sorted- und die find-Methode wie gehabt weiter verwenden.
Die Worst-Case-Laufzeit find-Methode eines Rot-Schwarz-Baumes ist also
weiterhin proportional zur Höhe des Baumes.
Auch die add- und delete-Operation können wir weiterverwenden.
Wir werden sie allerdings so erweitern, dass sie einen stets gut balancierten
Suchbaum garantieren und werden uns davon überzeugen müssen, dass diese
Erweiterungen keinen Einfluss auf die Worst-Case-Laufzeit haben.

Rotationen zur Reparatur eines unbalancierten Suchbaumes

In diesem Abschnitt sehen wir eine lokale Operation, die die Balancierung eines
Suchbaumes durch geschicktes Verschieben der vorhandenen Knoten verbessern kann,
d.h. ohne Knoten einzufügen oder zu löschen.



Schematische Darstellung einer Rechts-Rotation:
Wir stellen uns vor, dass der Wurzelknoten des Teilbaums , der Knoten , der Knoten  und der Wurzelknoten des Teilbaums  auf einer Kette liegen.
An dieser Kette ziehen wir im Uhrzeigersinn, so dass der rechte Baum entsteht.
Wir bemerken, dass sich dabei die Richtung der Kanten an  ändert, und dass der Teilbaum  umgehängt werden muss.
Zur Veranschaulichung der Änderung der Zeiger siehe auch Abbildung .



Ergänzende Abbildung zur Rechts-Rotation. Hier verlassen wir die Intuition aus Abbildung und sehen uns konkret an, wie die Zeiger verändert werden. Wir können uns dies in zwei
Schritten vorstellen: Zuerst setzen wir den linken Kindzeiger von  auf die Wurzel von ,
so dass temporär zwei Zeiger auf  zeigen.
Dann setzen wir den rechten Kindzeiger von  um, so dass dieser auf  zeigt und wieder
ein korrekt aufgebauter Binärbaum entsteht.




Darstellung einer Links-Rotation: Wir verbinden die Abbildungen  und ,
um die Links-Rotation zu veranschaulichen. Hier wird die Kette nach links gezogen.
Wieder gibt es nur zwei Zeiger, die verändert werden müssen, um die Rotation auszuführen.



Die Rechtsrotation um einen Knoten  dient dazu, die Balancierung des Teilbaums
unter  nach rechts zu verschieben (s. Abbildung und Abbildung ).
Dazu vertauscht sie die Rollen von  und seinem linken Kind : Nach der Rotation
ist  die Wurzel des rotierten Teilbaums, und  ist nun das rechte Kind von .
Besitzt  vor der Rotation bereits ein rechtes Kind, so wird dieses (inkl. des möglicherweise
daran befestigten Teilbaumes) als linkes Kind an  angehangen.
Alle anderen Eltern-Kind-Beziehungen ändern sich nicht.
Die Rotation führt dazu, dass die Länge jedes Pfades von der Wurzel zu einem
Blatt im linken Teilbaum unter  um eins verringert wird.
Im Gegenzug vergrößert sich die Länge jedes Pfades von der Wurzel zu einem
Blatt im rechten Teilbaum unter  um eins.

Wir überzeugen uns, dass die Rechtsrotation die Suchbaumeigenschaft erhält.
Dazu betrachten wir einen beliebigen Knoten  mit linkem Kind 
in einem binären Suchbaum mit den Bezeichnungen aus
Abbildung .
  Damit wir nach der Rechtsrotation  als rechtes Kind von 
  verwenden dürfen, muss  gelten.
  Das ist der Fall, denn vor der Rechtsrotation ist  ein linkes Kind von .
  Nach der Rotation ist  linker Teilbaum unter .
  Alle Schlüssel  im Teilbaum  erfüllen, dass , denn  war
  auch schon vor der Rotation linker Teilbaum unter .
  Damit wir den Teilbaum  nach der Rotation als linken
  Teilbaum von  verwenden dürfen, muss für alle Schlüssel  in diesem
  Teilbaum  gelten.
  Das ist der Fall, da der Teilbaum  vor der Rotation im linken
  Teilbaum unter  lag.
  Der Teilbaum , der nach der Rechtsrotation rechter
  Teilbaum unter  ist, war auch schon vor der Rotation rechter Teilbaum
  unter .
  Folglich erfüllen alle Schlüssel  in , dass .

Die analoge Linksrotation ist genau die Umkehroperation der Rechtsrotation, d.h.
sie macht die Rechtsrotation wieder rückgängig, s. Abbildung .

Die Links- und Rechtsrotationen sind lokale Operationen in dem Sinn, dass sie
nur die Eltern-Kind-Beziehungen von wenigen Knoten ändern.
Im Allgemeinen brauchen wir daher mehrere Links- und Rechtsrotationen um
einen unbalancierten Suchbaum wieder auszubalancieren.
Außerdem ist nicht offensichtlich, wie wir schnell erkennen können, wo
Links- oder Rechtsrotationen ausgeführt werden sollten.

Idee von Rot-Schwarz-Bäumen
Rot-Schwarz-Bäume verwenden eine Knotenfärbung in rote und schwarz Knoten.
Die Färbung muss gewisse Eigenschaften (Regeln) erfüllen, und anhand der Eigenschaften können wir
sicherstellen, dass der Baum stets ungefähr balanciert ist.
Immer wenn wir einen Knoten in unseren Suchbaum einfügen oder entfernen laufen
wir jedoch Gefahr, diese Balance zu zerstören.
Der Rot-Schwarz-Baum erkennt das daran, dass nach dem Einfügen oder Löschen
Eigenschaften der Knotenfärbung verletzt sind.
Er verwendet anschließend Rechts- und Linksrotationen um die Eigenschaften
der Färbung - und damit auch die Balancierung - wiederherzustellen.

Genauer ist ein Rot-Schwarz-Baum ein binärer Suchbaum der zusätzlich die folgenden fünf
Eigenschaften erfüllt:

  Jeder Knoten ist entweder rot oder schwarz gefärbt.
  Die Wurzel ist schwarz.
  Alle Blätter sind schwarz.
  Jeder rote Knoten besitzt ausschließlich schwarze Kinder.
  Für jeden Knoten  gilt:
  Alle Pfade, die von  zu einem Blatt im Teilbaum unter  laufen besitzen
  die gleiche Anzahl an schwarzen Knoten.


Ein Rot-Schwarz-Baum.

Wir erinnern uns hier, dass die Höhe eines Suchbaumes die Länge eines längsten Pfades
von der Wurzel zu einem Blatt ist und machen zwei Beobachtungen:

  Wegen Eigenschaft 4 kann es auf keinem Wurzel-Blatt-Pfad zwei aufeinanderfolgende
  rote Knoten geben.
  Jeder Wurzel-Blatt-Pfad beginnt bei einem schwarzen Knoten (der Wurzel, Eigenschaft 2)
  und endet bei einem schwarzen Knoten (einem Blatt, Eigenschaft 3).
  Folglich enthält jeder Wurzel-Blatt-Pfad höchstens so viele rote Knoten wie
  schwarze (genauer enthält ein Wurzel-Blatt-Pfad mit  schwarzen Knoten höchstens
   rote Knoten).
  Wegen Eigenschaft 5 besitzen alle Wurzel-Blatt-Pfade die gleiche Anzahl
  an schwarzen Knoten.
Ein längster Wurzel-Blatt-Pfad in einem Rot-Schwarz-Baum ist also höchstens
doppelt so lang wie ein kürzester Wurzel-Blatt-Pfad im gleichen Baum und wir
einsehen intuitiv ein, dass ein Rot-Schwarz-Baum immer nahezu ausbalanciert
sein muss.


Implementierung von Rot-Schwarz-Bäumen
Die Knotenobjekte von Rot-Schwarz-Bäumen sind fast identisch zu denen unseres
allgemeinen binären Suchbaums.
Zuätzlich zu den beiden Kindzeigern links und rechts
speichern wir hier noch einen Zeiger auf Elternknoten sowie eine Knotenfarbe.
[caption=Elemente (Knoten) eines binären Baumes, label=lst:trees:rbtrees:node]
class RBNode(x):
  key = x
  left = null
  right = null
  parent = null
  color = red
Knoten werden immer als rote Knoten erzeugt.

Wir haben bereits bei den verketteten Listen festgestellt, dass es nützlich ist,
das Ende der Verkettung mit einem Platzhalterelement tail zu markieren,
das keinen Schlüssel speichert.
Wir gehen in unserer Implementierung ähnlich vor und beenden jeden
Ast des Baumes mit einem Platzhalterknoten nil,
s. Abbildung :
Fehlt einem Nicht-Platzhalter-Knoten ein Kind oder fehlen beide Kinder so bekommt
er stattdessen einen bzw. zwei nil-Knoten als Kinder.
Die nil-Knoten sind also genau die Blätter des Baumes und nur innere
Knoten speichern Schlüssel.
Außerdem besitzt jeder innere Knoten genau zwei Kinder (der Baum ist allerdings
nicht notwendigerweise vollständig, da wir nicht erzwingen, dass alle Blätter
auf der untersten Ebene liegen).
Die nil-Knoten sind immer schwarz und enthalten stets den
Schlüssel null.

Für die Implementierung ist es nicht erforderlich, mehrere Kopien des nil-Knotens
zu erzeugen, es genügt wenn alle Zeiger auf das gleiche globale Knotenobjekt zeigen.


Wir halten aber fest, dass die Rot-Schwarz-Baum-Eigenschaft 3 automatisch
erfüllt ist, da unsere Blätter als nil-Knoten immer schwarz sind.
Außerdem modifizieren wir unsere Einfügeoperation so, dass sie nil-Knoten
berücksichtigt.
Wir initialisieren den Rot-Schwarz-Baum wie folgt.
[caption=Initialer Rot-Schwarz-Baum, label=lst:rees:rbtrees:init]
class RBTree:
  nil = Node(null)
  nil.color = black
  root = nil
  n = 0

Rot-Schwarz-Bäume besitzen höchstens eine logarithmische Höhe

Wir möchten jetzt nachweisen, dass die Höhe eines Rot-Schwarz-Baumes mit 
Knoten in  liegen muss.
Die Intuition hinter dieser Behauptung ist folgende:
Betrachten wir einen Rot-Schwarz-Baum ohne roten Knoten.
Wegen Eigenschaft 5 liegen alle Blätter dieses Baumes auf der gleichen Ebene.
Wir hatten außerdem den Baum so implementiert, dass jeder innere Knoten genau zwei
Kinder besitzt - gibt es also wie angenommen keine roten Knoten, so ist der Baum vollständig.
Wir hatten aber schon im vorherigen Abschnitt beobachtet, dass ein vollständiger
Binärbaum mit  Knoten eine Höhe von  besitzt.
Schauen wir genauer hin, so stellen wir aber fest, dass  auch eine gewisse
Anzahl an nil-Knoten enthält. Es wäre denkbar, dass die Anzahl 
tatsächlich gespeicherter Schlüssel im Baum deutlich kleiner als  ist und
die Beziehung  nicht gilt.
Da nil-Knoten aber immer Blätter sind können sie die Höhe des Baumes um
höchstens  vergrößern.

Wie können wir das Argument so verallgemeinern, dass es auch für Rot-Schwarz-Bäume
gilt, die tatsächlich rote Knoten enthalten? Unsere anfängliche Intuition sagt
uns schließlich, dass die roten Knoten die Balance des Baumes - und damit auch die
Höhe im schlimmsten Fall - nicht zu negativ beeinflussen sollten.
Das Konzept, was es uns erlaubt, einen beliebigen Rot-Schwarz-Baum auf unser
Argument mit ausschließlich schwarzen Knoten zurückzuführen heißt
Schwarzhöhe.
Wir definieren als Schwarzhöhe  eines Knotens  die Anzahl der
schwarzen Knoten auf einem Pfad von  zu einem Blatt im Teilbaum unter .
Hierbei zählen wir  selbst nicht mit.
Da wegen Eigenschaft 5 alle diese Pfade die gleiche Anzahl an schwarzen Knoten
besitzen, müssen wir uns in der Definition nicht für einen Pfad entscheiden und
der Begriff Schwarzhöhe ist wohldefiniert.
Wir formulieren unser Lemma direkt für die Anzahl  der gespeicherten Schlüssel
im Baum und umgehen so elegant die nil-Knotenproblematik.

  Ein Rot-Schwarz-Baum mit  Schlüsseln - also  inneren Knoten -
  hat höchstens Höhe .

  Wir zeigen zunächst eine Hilfsbehauptung.

  Behauptung: Der Teilbaum unter einem beliebigen Knoten  hat
  mindestens  innere Knoten.

  Wir zeigen die Behauptung per Induktion über die Höhe  von , d.h. über die
  Länge eines längsten Pfades von  zu einem Blatt im Teilbaum unter .

  Induktionsanfang ().
  Falls , so ist  ein Blatt (d.h. ein nil-Knoten).
  Der Teilbaum unter  enthält also nur das Blatt  und damit 0 innere Knoten.
  Andererseits ist  ( ist zwar schwarz, aber in der Definition der
  Schwarzhöhe wird  selbst nicht mitgezählt) und daher
  .
  Unsere Behauptung stimmt also, wenn  ist.

  Induktionsschritt ().
  Betrachten wir also den Fall, dass die Höhe von  echt größer als 0 ist und
  nehmen wir per Induktion an, dass die Behauptung für alle Knoten mit kleinerer
  Höhe als  gilt.
  Da  ist  jedenfalls ein innerer Knoten und besitzt aufgrund
  unserer nil-Konstruktion genau zwei Kinder.
  Ist das linke Kind von  rot, so besitzt es die gleiche Schwarzhöhe 
  wie . Ist das linke Kind von  schwarz, so besitzt es Schwarzhöhe .
  Das Gleiche gilt für das rechte Kind von .
  Die Schwarzhöhe beider Kinder ist also mindestens .
  Für beide Kinder gilt aber auch, dass ihre Höhe geringer als die Höhe 
  von  ist.
  Wir dürfen auf sie also die Induktionsvoraussetzung anwenden und schließen,
  dass der Teilbaum unter dem linken Kind sowie der Teilbaum unter dem rechten
  Kind jeweils mindestens  innere Knoten enthält.
  Also enthält der Teilbaum unter  mindestens
  *
  innere Knoten und wir haben unsere Behauptung gezeigt.

  Nachweis der Aussage des Lemmas.
  Es bleibt noch, die Aussage des Lemmas aus der Hilfsbehauptung herzuleiten,
  d.h. wir müssen noch die Schwarzhöhe auf die tatsächliche Höhe zurückführen.
  Betrachten wir also einen Rot-Schwarz-Baum mit Höhe .
  In diesem Baum liegen auf dem längsten Pfad von der Wurzel  zu einem Blatt
   Knoten, wenn wir die Wurzel selbst nicht mitzählen.
  Aus Eigenschaft 4 folgt, dass höchstens die Hälfte dieser Knoten rot sein kann
  (denn der Pfad endet wegen Eigenschaft 3 in einem schwarzen Knoten).
  Die Schwarzhöhe  der Wurzel beträgt also mindestens .
  Unsere Hilfsbehauptung sagt uns also, dass der Baum
  *
  innere Knoten (d.h. Schlüssel) enthält.
  Durch Umformen erhalten wir
  *
  was die Behauptung des Lemmas zeigt.

Einfügen in Rot-Schwarz-Bäume

Da Rot-Schwarz-Bäume auch binäre Suchbäume sind, können wir einen neuen
Schlüssel  zunächst mit der Suchbaum-Methode add einfügen.
Die unmodifizierte Methode fügt einen neuen Knoten für  als Blatt in den
Suchbaum ein.
Wir haben den Suchbaum allerdings so modifiziert, dass alle Blätter nil-Knoten
sind und möchten nun den Knoten für  nicht als Kind eines nil-Knotens
einfügen. Stattdessen fügen wir  als Blatt des Baumes ein, der nur aus den inneren
Knoten (den nicht-nil-Knoten) besteht.
Außerdem müssen wir den neuen Knoten mit zwei nil-Kindern versehen.

Der neu eingefügte Knoten  muss außerdem mit einer Farbe gefärbt werden.
Wir entscheiden uns, neue Knoten stets rot zu färben.
Dabei nehmen wir in Kauf, dass wir  eventuell als rotes Kind eines roten
Knoten einfügen (und damit Eigenschaft 4 verletzen) oder  als rote Wurzel
eingefügt wird (das verletzt Eigenschaft 2).
Beide Verletzung müssen wir im Folgenden reparieren.
Wir stellen aber fest, dass die Eigenschaften 1, 3 und 5 weiterhin erfüllt sind:
Vor der Einfügeoperation war unser Baum ein gültiger Rot-Schwarz-Baum.

 Nach dem Einfügen haben wir potentiell neue nil-Knoten (schwarz)
 und den neuen Knoten  (rot).
 Damit sind weiterhin alle Knoten entweder rot oder schwarz und Eigenschaft 1 gilt.
 Wir haben lediglich nil-Knoten als neue Blätter erhalten und diese
 sind nach Definition schwarz. Eigenschaft 3 gilt also weiterhin.
 Wir haben außer den nil-Blatt-Knoten nur einen roten Knoten
 eingefügt. Die Anzahl an schwarzen Knoten auf Wurzel-Blatt-Pfaden kann sich
 also nicht geändert haben und Eigenschaft 5 gilt weiterhin.

Im Folgenden werden wir sehen, wie die Eigenschaften 2 und 4 wiederhergestellt werden
können.
Die Idee der Reparatur ist, dass wir den Fehler entlang des Einfügepfades von 
im Baum nach oben schieben, bis wir die Wurzel erreichen.
Dabei stellen wir sicher, dass wir die Eigenschaften 1, 3 und 5 erhalten und der
Baum unterhalb des aktuell fehlerhaften Knotens fehlerfrei ist.
Erreicht der Fehler die Wurzel wird von der Methode garantiert, dass als einziger Fehler
noch eine Verletzung der Eigenschaft 2 (Wurzel muss schwarz sein) auftreten kann.
In diesem Fall können wir die Wurzel schwarz färben und haben den Baum damit
vollständig repariert.
Abbildung  zeigt den beispielhaften Ablauf
einer Reparatur.

Wir formulieren diese Idee noch einmal präziser als Methode rbfixup.
Die Methode verwaltet einen Knotenzeiger  und setzt initial  auf den gerade
eingefügten Knoten.
Die Methode führt Reparaturschritte aus und garantiert, dass nach jedem Schritt
folgende Eigenschaften (Invarianten) gelten.

 [Invariante 1.] Die Rot-Schwarz-Baum-Eigenschaften 1, 3 und 5 sind erfüllt.
 [Invariante 2.] Wenn Eigenschaft 4 verletzt ist, dann nur, weil  und
 sein Elternknoten  beide rot sind.
 Überall sonst ist Eigenschaft 4 erfüllt.
 [Invariante 3.] Wenn Eigenschaft 2 verletzt ist, dann ist  die Wurzel
 des Rot-Schwarz-Baumes und Eigenschaft 4 gilt überall.
Wir haben uns bereits überlegt, dass diese Invarianten direkt nach dem Einfügen
von  gelten.

Die Methode rbfixup wiederholt nun den folgenden Reparaturschritt, bis

  die Wurzel des Baumes ist. Dann färben wir  schwarz und folgern
 aus Invariante 3, dass wir damit den Baum vollständig repariert haben.
  nicht die Wurzel ist, aber Eigenschaft 4 überall gilt. In diesem
 Fall folgt ebenfalls aus den Invarianten, dass der Baum alle
 fünf Rot-Schwarz-Baum-Eigenschaften erfüllt.

Sofern nun  also nicht der Wurzelknoten ist, existiert sein Elternknoten  und
dieser ist auch rot (ansonsten hätte der Reparaturschritt gestoppt, weil dann
wegen Invariante 2 der Baum repariert wurde).
Als roter Knoten kann  nicht die Wurzel sein, so dass auch der Großelterknoten
 von  existieren muss.
Der Großelterknoten besitzt zwei Kinder: Eines davon ist , das andere nennen
wir den Onkel  von .
Weil  rot ist, muss  schwarz sein (Invariante 2).
Der Onkel  kann ein nil-Knoten sein.

Wir betrachten hier nur die Situation, dass der Elternknoten  ein linkes
Kind des Großvaterknotens  ist.
Die Reparatur funktioniert analog wenn  ein rechtes Kind von  ist, indem
wir überall links und rechts vertauschen.
Nun unterscheidet der Reparaturschritt drei Fälle.

1. Fall: Der Onkel  von  ist rot.
Dieser Fall wird in Abbildung  beschrieben.



1. Fall bei der Reparatur nach dem Einfügen eines neuen Knotens: Der Onkel von  ist rot. Knoten  und sein Elternknoten  sind beide rot und Eigenschaft 4 ist verletzt.
Unabhängig davon, ob  ein linkes oder rechtes Kind ist färben wir den Elternknoten  sowie den Onkel  von  schwarz. Der Großelternknoten 
wird rot gefärbt.
Ist der Elternknoten von  schwarz, sind wir fertig. Anderenfalls liegt die Verletzung von Eigenschaft 4 nun bei  und dessen Elternknoten.
Wir setzen  und fahren mit der Reparatur fort.
Der Fall funktioniert genauso, wenn  ein rechtes Kind von  ist.

 Wir färben zunächst den Elternknoten  sowie den Onkelknoten  schwarz.
 Dann färben wir  rot.
 Schließlich führen wir einen weiteren Reparaturschritt auf dem Großelterknoten
  aus, d.h. wir starten die Reparatur erneut mit .
Teilschritt 1 repariert die verletzte Eigenschaft 4.
Allerdings fügt er einen zusätzlichen schwarzen Knoten auf allen Pfaden zu Blättern
ein, die im Teilbaum unter dem Großelterknoten  liegen.
Dies korrigieren wir mit Teilschritt 2 (wir erinnern uns, dass 
zuvor schwarz war; das Umfärben entfernt also tatsächlich einen schwarzen Knoten
auf allen Pfaden, die durch  laufen).
Damit erhält der Reparaturschritt Eigenschaft 5.
Möglicherweise besitzt  aber einen roten Elternknoten, so dass Eigenschaft 4
nun zwischen  und seinem Elternknoten verletzt ist (oder  ist die Wurzel
und nun entgegen Eigenschaft 2 rot gefärbt).
Das ist der Grund, warum wir in Teilschritt 3 den Reparaturschritt jetzt für den
Großelterknoten, d.h. mit  ausführen.
Wir halten fest, dass wir die Verletzung näher an die Wurzel geschoben haben.
Warum gelten unsere Invarianten weiterhin?

 Die Eigenschaften 1 und 3 werden von der Reparatur nicht berührt und wir
 haben oben argumentiert, dass Eigenschaft 5 erhalten bleibt.
 Damit erhalten wir Invariante 1 aufrecht.
 Wir haben dafür gesorgt, dass  und  nicht mehr beide rot sind.
 Stattdessen sind jetzt möglicherweise  und sein Elternknoten rot.
 Damit erhalten wir Invariante 2 aufrecht, wenn wir  setzen.
 Eigenschaft 2 kann nur verletzt werden, wenn  die Wurzel ist (ansonsten
 färben wir die Wurzel nicht rot). Da wir in diesem Fall Eigenschaft 4 überall
 repariert haben und  setzen, gilt Invariante 3.

2. Fall: Der Onkel  von  ist schwarz und  ist
ein rechtes Kind.

Diesen Fall betrachten wir in Abbildung .



2. Fall bei der Reparatur nach dem Einfügen eines neuen Knotens: Der Onkel von  ist schwarz und  ist ein rechtes Kind.
Wir machen eine Linksrotation um .
Nach der Rotation ist  der Elternknoten von , daher vertauschen wir die Zeiger  und  (aber nicht die Knoten).
Anschließend fahren wir mit Fall 3 fort.
Ist  ein rechtes Kind von  gelangen wir in Fall 2, wenn der Onkel von  schwarz ist und  ein linkes Kind ist. Wir führen dann eine Rechtsrotation um 
statt der Linksrotation um  aus.

Wir überführen den Baum in die Situation von Fall 3.

 Wir machen zunächst eine Linksrotation um 
 Da jetzt  ein Kind von  ist, vertauschen wir die Zeiger (nicht die Knoten)  und ,
 so dass  wieder ein Kind von  ist.
 Dann führen wir Fall 3 (s. unten) aus.
Nach dem Zeigertausch ist  weiterhin der Großvater von  und  der Onkel
von , so dass  auch weiterhin schwarz ist. Allerdings ist  jetzt ein
linkes Kind.

Warum erhalten wir damit die Invarianten aufrecht?

Eigenschaft 1 und 3 sind nicht berührt. Eigenschaft 5 bleibt erhalten,
weil  und  beide rot sind. Wir erhalten also Invariante 1.
Eigenschaft 4 ist weiterhin verletzt, weil  und  beide rot sind;
wir haben keine Knoten umgefärbt, so dass Eigenschaft 4 weiterhin überall
sonst gilt. Damit haben wir Invariante 2 erhalten.
Die Teilschritte haben keinen Einfluss auf Invariante 3.
Eigenschaft 4 ist weiterhin darin verletzt, dass  und 
beide rot sind.

3. Fall: Der Onkel  von  ist schwarz und  ist
ein linkes Kind.
In diesem Fall führen wir zwei Teilschritte aus (s. Abbildung ):



3. Fall bei der Reparatur nach dem Einfügen eines neuen Knotens: Der Onkel von  ist schwarz und  ist ein linkes Kind.
Wir färben  schwarz und  rot. Anschließend machen wir eine Rechtsrotation um .
Nach der Rotation sind die Eigenschaften 4 und 2 hergestellt und wir können die Reparatur stoppen.
Ist  ein rechtes Kind von  führen wir eine Linksrotation um  statt der Rechtsrotation aus.

 Wir färben den Elternknoten  schwarz und den Großelterknoten  rot.
 Anschließend führen wir eine Rechtsrotation um den Großelterknoten  aus.
Das Umfärben von  und  im ersten Teilschritt führt dazu, dass Pfade zu Blättern im Teilbaum
unter , die über  laufen, einen schwarzen Knoten verlieren.
Die Rotation in Teilschritt 2 behebt dieses Problem und stellt Eigenschaft 5
wieder her.
Tatsächlich haben wir am Ende von Teilschritt 2 auch Eigenschaft 4 überall
hergestellt und können die Reparatur stoppen.
Auch in dem Fall, dass  nach der Rotation die Wurzel ist verlezten wir
Eigenschaft 2 nicht, denn wir haben  schwarz gefärbt.

Die Laufzeit des initialen Aufrufs der Suchbaum-Einfügeoperation ist proportional
zur Höhe  des Suchbaumes, welche  ist, wenn der Baum  Schlüssel enthält.
Die Reparatur des Suchbaumes besteht potentiell aus mehreren Reparaturschritten.
Jeder der Schritte verursacht konstanten Aufwand.
Außerdem wird jeder Reparaturschritt entweder:

  die Reparatur beenden,
   eine Ebene näher an die Wurzel schieben
  direkt in einen Fall führen, der 1 oder 2 erfüllt.
Folglich können auf jeder Ebene des Baumes höchstens konstant viele Reparaturschritte
durchgeführt werden und der Gesamtaufwand der Einfügeoperation beträgt .

Betrachten wir abschließend ein Beispiel.



So sieht der Rot-Schwarz-Baum aus Abbildung  nach Einfügen von  aus.
Wir sehen, dass Eigenschaft  am neuen Knoten  verletzt ist.
Der Onkel  von  (mit Schlüssel ) ist rot. Es handelt sich also um Fall 1 (linke Abbildung).
Wir färben daher Elterknoten  und Großelterknoten  um und gelangen so zur rechten Abbildung.
Eigenschaft  ist nun an  wiederhergestellt, jedoch an  verletzt.
In Abbildung  fahren wir mit  fort. 


Fortsetzung von Abbildung  mit neuem Knoten .
Der Onkelknoten  ist schwarz und  ist das linke Kind seines Elterknoten ,
daher befinden wir uns in Fall 3. Wir führen eine Rechtsrotation um den Großelterknoten  aus
(siehe Abbildungen und  zur Rechts-Rotation, wir führen diese mit  aus)
und gelangen so zu Abbildung  (links). 



Fortsetzung von Abbildung . Nach der Rechts-Rotation färben wir
noch  schwarz und  rot und gelangen so zur rechten Abbildung. Der Fehler ist nun repariert. 

Löschen in Rot-Schwarz-Bäumen
 
    
  
  Löschen eines Knotens  aus einem Rot-Schwarz-Baum, initiale Situation.
  Hier dargestellt ist der Fall, dass  ein nil-Kind und ein Kind  besitzt, das ein innerer Knoten ist.
  In diesem Fall muss  rot sein und zwei nil-Kinder besitzen:
  Sonst würde der Pfad von  zu seinem linken Kind weniger schwarze Knoten enthalten als
  jeder Pfad von  zu einem Blatt im rechten Teilbaum unter .
  Weil  rot ist, muss  schwarz sein.
  Wir können den Schlüssel von  nach  verschieben und anschließend
   entfernen.
Zum Löschen eines Schlüssels  aus einem Rot-Schwarz-Baum gehen wir zunächst
analog zu der bewährten delete-Methode von binären Suchbäumen vor.
Wir müssen hier allerdings darauf achten, dass in unserem Rot-Schwarz-Baum nur
die inneren Knoten Schlüssel enthalten und die Blätter nil-Knoten sind.


Zunächst suchen wir also den Knoten  mit Schlüssel  im Baum und unterscheiden
wieder drei Fälle, abhängig davon, wieviele echte Kinder der Knoten 
besitzt (da  ein innerer Knoten ist, besitzt er auf jeden Fall zwei Kinder; aber
es kann sich um nil-Knoten oder um innere Knoten mit Schlüsseln handeln).


    Besitzt  zwei innere Knoten als Kinder vertauschen wir den Schlüssel in 
mit dem Schlüssel des Nachfolgers  von  (das ist hier der linkeste innere
Knoten im rechten Teilbaum unter ).
Das linke Kind von  muss ein nil-Knoten sein (sonst wäre  nicht
der linkeste innere Knoten). Wir setzen  (d.h., wir verfolgen das Ziel, 
zu löschen) und machen mit Fall 2 oder Fall 3 weiter.
  Besitzt  genau einen inneren Knoten  und einen nil-Knoten als
  Kind, so muss  schwarz sein während  rot sein muss (s. Abbildung ).
  Außerdem müssen die Kinder von  beide nil-Knoten sein.
  Wir kopieren den Schlüssel von  nach , löschen beide nil-Kinder
  von  und machen aus  einen nil-Knoten.
  Abbildung  zeigt, dass wir hier wieder einen gültigen Rot-Schwarz-Baum
  erhalten.
   Im dritten Fall besitzt  zwei nil-Knoten als Kinder.
  Wir löschen die beiden nil-Kinder aus dem Baum, färben 
  schwarz und machen aus  einen nil-Knoten.

  War  rot, erhalten wir einen gültigen Rot-Schwarz-Baum.
  Ebenso sind wir fertig, wenn  schwarz und die Wurzel war.
  Anderenfalls war  schwarz und nicht die Wurzel. Wir haben auf
  allen Pfaden, die vor dem Löschen in einem Blatt unter  geendet sind einen
  schwarzen Knoten entfernt, und daher ist nun Eigenschaft 5 verletzt.
  Wir lösen dieses Problem, indem wir es erlauben, dass  eine zusätzliche
  Schwarzfärbung trägt.
  Doppelt-schwarze und rot-schwarze Knoten zählen für Eigenschaft 5 und die
  Schwarzhöhenberechnung als zwei bzw. ein schwarzer Knoten (für Eigenschaften
  2,3 und 4 gehen wir aber davon aus, dass sie schwarz bzw. rot sind).
  Wir färben also  doppelt-schwarz und erfüllen damit Eigenschaft 5 wieder.
  Allerdings verletzen wir nun Eigenschaft 1, da es nun einen Knoten gibt, der
  weder rot noch (einfach) schwarz ist. Dieses Problem lösen wir im Folgenden.

Wir folgen einer analogen Strategie wie beim Einfügen und verwalten einen
Knotenzeiger , der immer auf den einen doppelt-gefärbten
(doppeltschwarz oder rot-schwarz) Knoten zeigt.
Zu Beginn zeigt  wie beschrieben auf ein Blatt.



Wieder führen wir Reparaturschritte an  aus, die entweder dafür sorgen, dass
 im Baum nach oben (Richtung Wurzel) wandert oder wir den Baum vollständig
reparieren.
Wir tragen dafür Sorge (Invariante), dass nach jedem Reparaturschritt alle
Rot-Schwarz-Baum-Eigenschaften bis auf Eigenschaft 1 erfüllt sind.
Außerdem ist Eigenschaft 1 ist höchstens bei  verletzt, und dann auch nur, weil
 doppelt gefärbt ist.
Wir können daher  einfach schwarz färben und die Reparatur stoppen,
sobald  die Wurzel ist oder  auf einen rot-schwarzen Knoten zeigt.









Unsere Invariante ist zu Beginn, d.h. vor dem ersten Reparaturschritt, erfüllt.
Wir unterscheiden nun verschiedene Fälle danach, welche Farbe der Geschwisterknoten  von 
und dessen linkes bzw. rechtes Kind  bzw.  besitzen.
Wir gehen davon aus, dass  ein linkes Kind seines Elternknotens  ist; anderenfalls
funktionieren die folgenden Fälle analog, wenn wir jeweils rechts und links
vertauschen.
Da wir stoppen können, wenn  schwarz-rot ist, können wir davon ausgehen,
dass  stets doppelt-schwarz ist (anderenfalls färben wir  einfach
schwarz und beenden die Reparatur).
Wir wissen daher auch, dass der Geschwisterknoten  von  kein Blatt ist;
anderenfalls wäre Eigenschaft 5 an  vor dem Löschen verletzt gewesen.
Insbesondere besitzt  zwei Kinder.

1. Fall:  ist rot.
Wir beobachten, dass der Elternknoten  schwarz sein muss, weil  ein
Kind von  ist und rot ist.
Weiterhin muss  zwei schwarze Kinder haben.

    fig-rbtrees-delete-case-1
  
  Fall 1 der Reparatur während der Löschoperation in einem Rot-Schwarz-Baum:
  Der Geschwisterknoten  von  ist rot.
  Wir möchten Knoten  reparieren, dieser ist doppelt-schwarz.
  Wir färben  rot und  schwarz. Anschließend rotieren wir links an .
  Nach der Rotation ist der neue Geschwisterknoten von  schwarz und wir
  fahren mit einem der Fälle 2, 3 oder 4 fort.

Wir möchten diesen Fall in die Fälle 2-4 überführen und sorgen daher dafür,
dass  ein schwarzes Geschwister bekommt.
Anschließend machen wir dann mit einem der Fälle 2-4 weiter.

  Färbe den Geschwisterknoten  von  schwarz.
  Färbe den Elternknoten  von  rot.
  Anschließend machen wir eine Linksrotation um .
  Mache einen weitere Reparaturschritt mit Fall 2,3 oder 4.
Auch nach der Rotation ist der Geschwisterknoten von  nun schwarz.
Dieser Reparaturschritt ist in Abbildung  dargestellt.

Der Reparaturschritt erhält unsere Invariante, denn:

er hat keinen Einfluss auf die Eigenschaften 2,3 und 4
nach dem Schritt zeigt  weiterhin auf den eindeutigen Knoten
mit doppelter Färbung
Er erhält außerdem Eigenschaft 5: Indem wir  rot und 
schwarz färben entfernen wir zunächst einen schwarzen Knoten auf allen Pfaden,
die  enthalten.
Für alle anderen Pfade ändert sich die Anzahl schwarzer Knoten nicht.
Dieses Problem beheben wir durch die Linksrotation um , die genau
diesen Pfaden einen schwarzen Knoten hinzufügt.

2. Fall:  ist schwarz, und  und  sind beide schwarz.
In diesem Fall können wir nicht auf die Farbe  des Elternknotens 
von  schließen, da beide Kinder  und  von  schwarz sind.


    fig-rbtrees-delete-case-2
  
  Fall 2 der Reparatur während der Löschoperation in einem Rot-Schwarz-Baum:
  Der Geschwisterknoten  ist schwarz und dessen Kindern sind ebenfalls beide
  schwarz.
  Der Elternknoten  kann rot oder schwarz sein.
  Wir färben  rot und verschieben die zusätzliche schwarze Färbung von 
  auf dessen Elternknoten .
  Ist der Elternknoten nun rot-schwarz können wir stoppen (z.B. wenn wir aus Fall 1 kamen),
  anderenfalls haben  wir  näher zur Wurzel bewegt und machen einen neuen
  Reparaturschritt mit .


  Färbe den Geschwisterknoten  von  rot und verschiebe
  eine der beiden schwarzen Färbungen von  nach .
  Ist  nun schwarz-rot, färbe  einfach schwarz und stoppe
  (das passiert inbesondere, wenn wir aus Fall 1 gekommen sind).
  Anderenfalls ist  doppelt-schwarz. Führe einen weiteren
  Reparaturschritt mit  aus.
Durch das Verschieben einer schwarzen Färbung von  nach  haben wir allen
Pfaden, die  und  enthalten einen schwarzen Knoten hinzugefügt.
Indem wir  rot färben entfernen wir auf genau diesen Pfaden einen schwarzen
Knoten, so dass nach dem Reparaturschritt Eigenschaft 5 gilt.
Eigenschaft 4 gilt trotz unserer rot-Färbung von , da beide Kinder von 
schwarz sind und  am Ende des Schrittes schwarz oder doppelt-schwarz ist.
Die Eigenschaften 2 und 3 werden nicht berührt ( kann kein Blatt sein).
Nach dem Reparaturschritt zeigt  auf den doppelt gefärbten Knoten.
Wir schließen, dass unsere Invariante weiterhin gilt.

3. Fall:  ist schwarz, linkes Kind  ist rot, rechtes Kind  ist schwarz.
Auch in diesem Fall kennen wir die Farbe von  nicht, wissen aber, dass 
schwarz sein muss.

    fig-rbtrees-delete-case-3
  
  Fall 3 der Reparatur während der Löschoperation in einem Rot-Schwarz-Baum:
  Der Geschwisterknoten  ist schwarz, dessen linkes Kind bzw. rechtes Kind ist rot bzw. schwarz.
  Der Elternknoten  kann rot oder schwarz sein.
  Wir färben das linke Kind von  schwarz und  selbst rot.
  Nun machen wir eine Rechtsrotation um  und setzen  auf den neuen Geschwisterknoten
  von  (das frühere linke Kind von ). Nun ist das rechte Kind von  rot und
  wir fahren mit Fall 4 fort. 

Da nun ein Kind von  rot ist, dürfen wir  nicht rot färben.
Wir überführen diesen Fall in Fall 4.

  Färbe linkes Kind  von  schwarz und  selbst rot.
  Anschließend führe eine Rechtsrotation um  aus.
  Fahre fort mit Fall 4.
Wir argumentieren, dass dieser Schritt die Invariante erhält.
Das Umfärben von  fügt einen schwarzen Knoten zu allen Pfaden hinzu,
die  und  enthalten. Indem wir nun  rot färben, entfernen wir aber gleichzeitig
einen schwarzen Knoten von allen Pfaden, die  enthalten.
Damit reparieren wir die Pfade, die  und  enthalten; Pfade die  und 
enthalten besitzen nun aber einen schwarzen Knoten zu wenig.
Das reparieren wir mit der Rechtsrotation um .
Die Eigenschaften 2 und 3 werden nicht berührt.
Unsere Umfärbung erhält Eigenschaft 4, da sie dafür sorgt, dass 
zwei schwarze Kinder besitzt und daher  rot gefärbt werden kann.

4. Fall:  ist schwarz, linkes Kind  beliebig, rechtes Kind  rot
Wir kennen die Farbe von  und  nicht, wissen aber, dass  schwarz
sein muss.

    fig-rbtrees-delete-case-4
  
  Fall 4 der Reparatur während der Löschoperation in einem Rot-Schwarz-Baum:
  Der Geschwisterknoten  ist schwarz und dessen rechtes Kind ist rot (linkes Kind beliebig).
  Die Farbe des Elternknotens  und die Farbe des linken Kindes von  sind beliebig.
  Wir färben  in der Farbe von  und färben das rechte Kind von  schwarz.
  Ebenso färben wir nun  schwarz.
  Abschließend führen wir eine Linksrotation um  durch und färben 
  einfach schwarz.
  Ist nun der vorherige Geschwisterknoten  die Wurzel des Rot-Schwarz-Baumes,
  färben wir  schwarz.
  Anderenfalls kann es passieren, dass  nun rot ist; dies kann aber nicht
  Eigenschaft 4 verletzen, weil  nur dann rot gefärbt wird, wenn zuvor 
  schon rot war; nach der Rotation muss der Elternknoten von  also schwarz sein.
  In jedem Fall ist der Baum nach Abschluss des Schrittes repariert.
  In rot: Die Schwarzhöhe der Knoten.


  Färbe  in der Farbe von 
  Färbe  schwarz und  schwarz.
  Führe eine Linksrotation um  aus. Falls der ehemalige Geschwisterknoten
   jetzt die Wurzel des Baumes ist, färbe  schwarz.
  Färbe  einfach schwarz und stoppe.
Der Reparaturschritt berührt nicht Eigenschaft 3 und stellt in Teilschritt 3
Eigenschaft 2 her, falls diese verletzt sein sollte.
Eigenschaft 1 wird in Teilschritt 4 hergestellt.
Wir färben lediglich  potentiell rot; allerdings nur, wenn  zuvor rot
war. Vor der Linksrotation muss der Elternknoten  (sofern vorhanden) von  dann
aber schwarz sein. Nach der Linksrotation ist  der Elternknoten von , so dass
nach der Linksrotation der Elternknoten von  schwarz ist, falls wir  rot färben.
Die Kinder von  sind nach der Rotation beide schwarz. Damit gilt Eigenschaft 4
nach dem Reparaturschritt.

Auch Eigenschaft 5 bleibt erhalten (s. Abbildung ):
Betrachten wir zunächst den Zustand zu Beginn
des Reparaturschritts (also vor den Umfärbungen und der Linksrotation).
Nennen wir die Schwarzhöhe von  zu diesem Zeitpunkt .
Zur gleichen Zeit besitzt  als eine Schwarzhöhe von  (da  doppelt
schwarz ist, also die Schwarzhöhe von  um zwei erhöht) und  besitzt eine
Schwarzhöhe von  (da  einfach schwarz ist).
Das rechte Kind  von  besitzt ebenfalls eine Schwarzhöhe von ,
denn  ist rot.
Weder die Rotation, noch die Umfärbungen ändern die Schwarzhöhe von  und
, so dass  am Ende des Reparaturschrittes eine Schwarzhöhe von 
besitzt.
Der Reparaturschritt ändert auch nicht die Schwarzhöhe von .
Wegen der Umfärbungen besitzt  nun zwei schwarze Kinder  und , die nach
unserer Argumentation beide Schwarzhöhe  besitzen.
Damit gilt Eigenschaft 5 für  und  und  besitzt nun Schwarzhöhe .
Insgesamt bleibt Eigenschaft 5 also erhalten.

Laufzeit der Löschoperation.
Was ist also die Laufzeit der Löschoperation?
Die Laufzeit des initialen Aufrufs der Suchbaum-Löschoperation ist proportional
zur Höhe des Suchbaumes, welche  ist.
Die Reparatur des Suchbaumes besteht potentiell aus mehreren Reparaturschritten.
Jeder der Schritte verursacht konstanten Aufwand.

 Führen wir Fall 1 aus, so gelangen wir danach in eine Situation, in der
 Fall 2 stoppt.
 Gelangen wir anderweitig nach Fall 2, stoppt Fall 2 entweder oder schiebt
  im Baum nach oben.
 Fall 3 führt immer zu Fall 4.
 Fall 4 beendet immer die Reparatur.
Wir sehen also, dass nur Fall 2 mehrfach ausgeführt werden kann.
Da aber Fall 2 stets den doppelt gefärbten Knoten um eine Ebene nach oben
schiebt, kann Fall 2 auf einem Baum mit Höhe  während eines Löschvorgangs
höchstens  mal ausgeführt werden.

Folglich können auf jeder Ebene des Baumes höchstens konstant viele Reparaturschritte
durchgeführt werden und der Gesamtaufwand der Löschoperation beträgt .




Hashing als Datenstruktur für ungeordnete Mengen
Auch in diesem Kapitel werden wir uns damit beschäftigen, wie wir eine dynamische
Menge verwalten können.
Uns ist es im vorherigen Kapitel gelungen, die Laufzeit der wichtigsten
Mengenoperationen add, delete und find einer Menge mit 
Schlüsseln in Zeit  zu implementieren.
In diesem Kapitel stellen wir uns die Frage, ob es noch besser geht.

Tatsächlich können wir alle drei Operationen in Zeit  implementieren,
wenn wir viel Speicherplatz investieren: Nehmen wir an, dass wir nur
nicht-negative Schlüssel speichern möchten und dass der größte
Schlüssel, den wir speichern wollen durch eine Zahl  beschränkt ist.
Wir legen wir ein Array  der Größe  an und markieren initial alle Einträge
von  als leer (etwa mit einer Platzhalter-Markierung oder einem zusätzlichen
boolean-Array).
Wird nun Schlüssel  eingefügt, speichern wir ihn in konstanter Zeit an
Position .
Ebenso können wir  in konstanter Zeit finden: Ist Position  leer, ist 
nicht in der Menge enthalten, sonst haben wir  gefunden.
Zum Löschen von  markieren wir in konstanter Zeit Position  in  als leer, s. Abbildung .

  fig-hashing-intro
  
  
      Liegen alle Schlüssel im Bereich  können wir in Speicherplatz 
  die Methoden add, delete und find realisieren, indem wir in einem
  Array der Größe  anlegen und Schlüssel  an Position  speichern.

Das Problem mit dieser Implementierung ist lediglich, dass wir Zeit 
benötigen, um das initiale Array anzulegen; ebenso beträgt der Speicherplatzbedarf
, selbst wenn wir nur einen einzigen Schlüssel  speichern und
 sehr viel kleiner als  ist (wir schreiben ).

Wir lösen dieses Problem mit einem geschickten Trick.
Anstelle des Arrays der Größe  legen wir ein Array mit Größe  an,
und zwar so, dass  von der Anzahl  tatsächlich eingefügter Schlüssel
abhängt (zm Beispiel können wir uns hier vorstellen, dass  in etwa  ist).
Nun können wir aber im Allgemeinen nicht mehr Schlüssel  an Position 
in  speichern; schließlich könnte  größer als  sein.
Wir bilden daher alle Schlüssel aus dem Universum
 der möglichen Schlüssel mit einer
Funktion  auf die tatsächlich verfügbaren Positionen
 ab und speichern also Schlüssel  an
Position  in .
Die Funktion  heißt Hashfunktion und wir nennen  die Hashtabelle.
Da wir davon ausgehen, dass es sehr viel mehr mögliche Schlüssel als Positionen
in  gibt, nehmen wir dabei in Kauf dass mehrere Schlüssel auf die gleiche
Position abgebildet werden; die Funktion  ist also nicht injektiv.

Sollen zwei Schlüssel  und  eingefügt werden, die an der gleichen Position
 stehen sollen sprechen wir von einer Kollision.
Wir werden im Folgenden zwei Strategien untersuchen, mit Kollisionen umzugehen:
Hashing mit Verkettung der Überläufer verwaltet an jeder Arrayposition
eine Liste, in der kollidierende Schlüssel abgelegt werden.
Hashing mit Sondierungen speichert alle Schlüssel direkt in der Hashtabelle,
indem bei einer Kollision an festgelegten alternativen Positionen versucht wird,
die kollidierenden Schlüssel einzufügen.
In beiden Fällen wird die Größe der Hashtabelle dynamisch an die Anzahl zu
speichernder Schlüssel angepasst, d.h. wir vergrößern , wenn der Füllstand 
(also die Anzahl der gespeicherten Schlüssel im Vergleich zur Größe der Hashtabelle)
zu groß wird und verkleinern , wenn der Füllstand sinkt.

Unter der Annahme, dass alle Schlüssel gleich häufig auftreten können wir
unsere add-, delete- und find-Operation im Mittel
in konstanter Zeit implementieren. Wir sehen zum Abschluss des Kapitels, wie
wir diese (sehr starke) Annahme entfernen können.
Außerdem werden wir uns damit beschäftigen, wie wir eine geeignete Hashfunktion
finden.

Hashing mit Verkettung der Überläufer

Hashing mit Verkettung der Überläufer verwaltet ein Array von (doppelt) verketteten
Listen und speichert alle Schlüssel, die an Position  gehasht wurden in der Liste,
die im Array an Position  steht.
Abbildung  zeigt ein Beispiel.

  
    
Eine Hashtabelle, in die Schlüssel mit Hilfe der Hashfunktion  eingefügt wurden.

[caption=Initialisierung einer Hashtabelle mit Verkettung der Überläufer, label=hashing:linkedhash:init]
class HashTable(m):
  n=0
  A = Array(m)
  for i=0,...,m-1:
    A[i] = DLinkedList()
Um einen Schlüssel  in der Hashtabelle aufzufinden müssen wir also zunächst
seinen Hashwert  berechnen und anschließend die Liste 
an Position  durchsuchen.
Besitzt  die Länge , so benötigen wir im schlimmsten Fall Zeit 
um  in  zu finden bzw. zu entscheiden, dass  nicht in der Liste enthalten ist.
Da im schlimmsten Fall alle Schlüssel der Hashtabelle kollidiert sind und in Liste 
liegen, beträgt die Worst-Case-Laufzeit der Methode .
Dazu kommt die Zeit, die wir benötigen um  zu berechnen.
Wir gehen aber hier und im Folgenden davon aus, dass  in konstanter Zeit
berechnet werden kann.
[caption=Methode find einer Hashtabelle mit Verkettung der Überläufer, label=hashing:linkedhash:find]
class HashTable(m):
  ...
  function find(x):
    i = h(x)
    return A[i].find(x)
Um einen Schlüssel  einzufügen berechnen wir seinen Hashwert
 und fügen  in die Liste  ein.
[caption=Methode add einer Hashtabelle mit Verkettung der Überläufer, label=hashing:linkedhash:add]
class HashTable(m):
  ...
  function add(x):
    i = h(x)
    A[i].append(x)
    n++
Da wir davon ausgehen, dass wir  in konstanter Zeit berechnen können
und sofern wir am Ende (oder am Anfang) der Liste einfügen, haben wir die
add-Operation in konstanter Zeit umgesetzt.
Dies gilt allerdings nur unter der Voraussetzung, dass wir nicht prüfen, ob der
Schlüssel  bereits in der Hashtabelle enthalten ist, denn ansonsten müssen wir
die gesamte Liste  nach  durchsuchen, bevor wir  einfügen können.
Wir hatten schon festgestellt, dass das Durchsuchen der Liste im schlimmsten Fall
Zeit  benötigt.
Um einen Schlüssel  aus der Hashtabelle zu löschen berechnen wir wieder zunächst
den Hashwert  und löschen  anschließend aus der Liste .
Auch diese Operation benötigt also im schlimmsten Fall Zeit .
[caption=Methode delete einer Hashtabelle mit Verkettung der Überläufer, label=hashing:linkedhash:delete]
class HashTable:
  ...
  function delete(x):
    i = h(x)
    e = A[i].find(x)
    A[i].delete(e)
    n-
In unserer Implementierung ist die Größe der Hashtabelle zunächst fest auf 
gesetzt.
Wir beobachten, dass wir trotzdem beliebig viele Schlüssel einfügen können und
definieren den Auslastungsfaktor  der Hashtabelle als .
Ist  haben wir weniger Schlüssel eingefügt, als es Positionen in
der Hashtabelle gibt; für  übersteigt die Anzahl der eingefügten
Schlüssel die Länge der Hashtabelle.

Mittlere Laufzeiten der Hashtabellen-Operationen
Wir möchten im Folgenden nachweisen, dass alle die Operationen find und delete
im Mittel in konstanter Zeit ablaufen, sofern die Hashtabelle nicht zu voll ist.
Dazu machen wir zunächst eine Annahme über die Häufigkeitsverteilung der Schlüssel:
Wir setzen voraus, dass unsere Hashfunktion jeden Schlüssel mit gleicher Wahrscheinlichkeit
auf eine der  Positionen verteilt, und dass diese Verteilung des Schlüssels
unabhängig von allen anderen Schlüsseln erfolgt.
Diese Voraussetzung nennen wir Gleichverteilungsannahme.
Sie sagt aus, dass auf jede Position  der Hashtabelle genau -tel der möglichen Schlüssel abgebildet werden.
Wir folgern, dass unter dieser Annahme die Länge  der Liste an Position  im Mittel gerade 
beträgt.

Aus unserer Beobachtung folgt direkt, dass wir eine erfolglose Suche (also eine Suche nach einem Schlüssel, der nicht in der Hashtabelle vorkommt)
im Mittel unter der Gleichverteilungsannahme in Zeit  durchführen können: Wir benötigen Zeit  um den Hashwert von 
zu bestimmen und Zeit  um die vollständige Liste  nach  zu durchsuchen.(Die Laufzeit beträgt hier nicht 
da wir auch dann noch konstante Zeit benötigen, wenn  ist.)

Eine erfolgreiche Suche nach  (also eine Suche nach einem Schlüssel, der in der Hashtabelle vorkommt) ist potentiell schneller, da wir nicht die gesamte
Liste an Position  durchsuchen müssen.
Gehen wir zur Analyse davon aus, dass jeder Schlüssel  in der Tabelle gleich häufig (also mit Wahrscheinlichkeit ) gesucht wird.
Dafür, dass wir den ersten Schlüssel der Liste  in einem Schritt finden können, müssen wir für den letzten Schlüssel in der Liste 
Schritte machen. Den zweiten Schlüssel finden wir in 2 Schritten, aber den Vorletzten in , usw.
Allgemein gibt es für einen Schlüsse, den wir in  Schritten finden auch einen, für den wir  Schritte benötigen.

Unsere Intuition sagt uns daher, dass wir im Mittel etwa die Hälfte der Liste durchsuchen müssen.
Formaler betrachten wir eine Einfügereihenfolge  der Schlüssel.
Wenn Schlüssel  eingefügt wird, wird an das Ende der Liste 
angehängt. Da sich zu diesem Zeitpunkt  Schlüssel in der Hashtabelle
befinden, hat diese Liste nach unseren bisherigen Überlegungen im Mittel
eine Länge von .
Wir finden den -ten Schlüssel also nach  Schritten in der er Liste .
Um die mittlere Laufzeit zu finden, bilden wir nun den Durchschnitt über alle Schlüssel
in der Hashtabelle als
*
Wir schlussfolgern, dass die erfolgreiche Suche im Mittel unter der
Gleichverteilungsannahme Zeit  benötigt.
Mit dem gleichen Argument sehen wir ein, dass auch die delete-Methode
im Mittel unter der Gleichverteilungsannahme Laufzeit 
besitzt.
Wir hatten schon eingesehen, dass die Methode add im Worst-Case
in Zeit  arbeitet, wenn wir doppelt eingefügte Schlüssel in Kauf nehmen.
Wir sehen nun, dass wir im Mittel Zeit  benötigen, wenn wir
beim Einfügen doppelte Schlüssel vermeiden wollen.
Gehen wir also davon aus, dass  ist, arbeiten find, add
mit Dopplungsvermeidung und delete im Mittel unter der Gleichverteilungsannahme
in konstanter Zeit.

Es bleibt nun also sicherzustellen, dass  ist.
Dies gelingt uns, indem wir das Verdopplungsschema verwenden, dass wir bereits
bei den Arraylisten kennengelernt haben.
Z.B. können wir die Größe der Hashtabelle verdoppeln, wenn der Füllstand einen
kritischen Wert (z.B. ) überschreitet.
Fällt der Füllstand unter einen kritischen Wert (z.B. ) halbieren
wir die Größe der Hashtabelle.
Auf diese Weise ist  und also .
In beiden Fällen müssen wir ggf. unsere Hashfunktion an die neue Hashtabellengröße
anpassen und alle Schlüssel neu hashen.
Wir hatten aber bereits bei den Arraylisten eingesehen, dass das Verdopplungsschema
in amortisiert konstanter Zeit durchgeführt werden kann.

Wahl der Hashfunktion

Idealerweise wünschen wir uns von unserer Hashfunktion, dass sie die
Gleichverteilungsannahme (zumindest näherungsweise) erfüllt und die zu hashenden
Schlüssel gleichmäßig über die Hashtabelle verteilt.
Ob das so ist, hängt aber davon ab, welche Schlüssel tatsächlich gehasht werden
müssen.
Zu Beginn des Kapitels haben wir etwa eine Hashfunktion der Form 
verwendet.
Mit ihr können z.B. fortlaufend erzeugte Auftragsnummern perfekt gleichverteilt
über  Fächer verteilen, da sie als Hashwert die drei niederwertigsten
Ziffern verwendet.
Sind die Auftragsnummern aber nicht fortlaufend vergeben sondern enthalten
z.B. in den niederwertigsten Stellen das Auftragsdatum ist unsere Hashfunktion
denkbar schlecht geeignet.

Divisionsmethode
Das Hashen mit einer Funktion  wird Divisionsmethode
genannt.
Wie das folgende Beispiel zeigt, empfielt es sich  nicht als Potenz einer Basis
 zu wählen (wir sollten sicherstellen, dass  nicht gleich  für ein
 und ein  ist), falls die Ziffern der Schlüssel Informationen (wie etwa ein Datum) codieren.

  
  Hashen wir basierend auf einzelnen Stellen der Eingabe kommt es leicht zu Kollisionen:
  In diesem Beispiel codieren die hinteren Stellen der Schlüssel einen Auftragsmonat, so dass alle Aufträge des gleichen Monats auf das
  gleiche Fach der Hashtabelle gehasht werden.

Abbildung  zeigt ein weiteres ungünstiges Beispiel; hier sind alle Schlüssel
durch drei teilbar und wir haben  gewählt.

  
  Verwenden wir  als Hashfunktion erhalten wir keine
  gleichmäßige Verteilung, wenn die Schlüssel gemeinsame Teiler mit  besitzen.
  Hier ist  und alle Schlüssel sind durch 3 teilbar.
Da drei ein Teiler von zwölf ist werden die Schlüssel nun ausschließlich an
die Positionen 0, 3, 6 und 9 verteilt.
Das Risiko von derartigen Mustern können wir verringern, indem wir  als eine
Primzahl wählen.


Ein zusätzliches Problem dieser Methode ist daher, dass sie die Größe der
Hashtabelle auf Primzahlen einschränkt.
Das ist unpraktisch für die Implementierung des Verdopplungsschemas, das wir
verwenden um den Füllstand der Hashtabelle zu kontrollieren.
Die Hashfunktion  mit einer Primzahl  erlaubt
es, eine Hashtabelle beliebiger Größe zu verwenden, indem sie zunächst die Schlüssel
gleichmäßig auf den Bereich  verteilt.

Multiplikationsmethode
Die Multiplikationsmethode multipliziert einen Schlüssel  zunächst mit einer
(ir)rationalen Zahl  mit .
Das Ergebnis  verwandeln wir nun erneut in eine Zahl zwischen  und 
indem wir nur den Nachkomma-Anteil  betrachten.
Jetzt stellen wir uns vor, dass die Hashtabelle das Interval 
repräsentiert.
Jede Position von  stellt also ein Intervall der Länge  dar.
Nun berechnen wir, in welches dieser Intervalle die Zahl 
fällt, indem wir  berechnen und abrunden.

Formal wählen wir .
Es hat sich als gut herausgestellt,  als den Kehrwert 
des goldenen Schnitts zu wählen.

Hashen von Strings
Wir sind bisher davon ausgegangen, dass wir ausschließlich ganze Zahlen hashen.
Eine Möglichkeit, andere Datentypen zu hashen ist, sie zunächst in ganze
Zahlen zu überführen.
Zum Beispiel können wir einen Java-String hashen, indem wir ihn als Zahl
zur Basis  interpretieren (jedes Zeichen des Strings ist eine 16-bit Zahl).
[caption=Beispielhafte Hashfunktion für einen String, label=hashing:linkedhash:stringhash]
function hash_string(s):
  hash = 0
  R = 2^16
  for i=0,...,s.laenge-1:
    hash = R*hash + as_number(s[i])

  return hash
Hier konvertiert der Aufruf asnumber(s[i]) das -te Zeichen des Strings
in eine 16bit-Zahl.
Ist hash ein 32-bit integer ohne Vorzeichen rechnet die
Methode automatisch modulo  und hält dadurch den Hashwert von  in
einem sinnvollen Zahlenbereich.

Hashing mit Sondierungen

Hashing mit Sondierungen - auch Hashing mit offener Adressierung genannt -
speichert alle Schlüssel direkt in der Hashtabelle.
Finden wir beim Einfügen eines Schlüssels  an Position  einen anderen
Schlüssel, so testen wir nach einem festen Schema - der Sondierungsreihenfolge
- Ausweichpositionen, bis wir einen freien Platz für  finden.
Im Gegensatz zu Hashing mit Verkettung der Überläufer kann das Verfahren also
insbesondere höchstens  Schlüssel in einer Hashtabelle mit  Einträgen
speichern.

Weil alle Schlüssel direkt in der Hashtabelle verwaltet werden, sparen wir
den Speicherplatz, der ansonsten für die Verkettung benötigt würde; außerdem
ist ein fortlaufendes Array vorteilhaft für die Speicherhierarchie moderner
Computerarchitekturen.

Wir untersuchen im Folgenden, welche Sondierungsreihenfolgen sich besonders
gut eignen und welchen Einfluss der Aulastungsfaktor  hat.
Dazu definieren wir für  die -te Hashposition von
 als
*
Dabei wird  von der Sondierungsreihenfolge vorgegeben.
Die implizite Annahme ist hier, dass alle Hashpositionen
voneinander verschieden sind und wir also nach  Versuchen jede Position in
 ausprobiert haben.
Nicht jede Sondierungsreihenfolge erfüllt diese Annahme; dennoch begnügen wir uns
damit,  Versuche zur Verfügung zu haben.

Unsere Hashtabelle besteht nun aus einem einfachen Array, dessen Positionen
wir initial auf den Platzhalter nil setzen um anzuzeigen, dass sie frei
sind.
[caption=Initialisierung einer HashTabelle für Hashing mit Sondierungen, label=hashing:probing:table-init]
class ProbingHashTable(m):
  n = 0        # Anzahl Schluessel in Tabelle
  A = Array(m) # Hashtabelle
  for i=0,...,m:
    A[m] = NIL # markiere Positionen als leer

Einfügen und suchen mit Sondierungen
Im Folgenden überlegen wir uns zunächst, wie wir die add- und die
find-Operation realisieren können.
Dabei gehen wir zunächst davon aus, dass keine Schlüssel aus der Hashtabelle
gelöscht werden.

Fügen wir einen Schlüssel  ein, so testen wir zunächst Position .
Ist Position  belegt, testen wir , usw. bis wir eine
freie Position für  gefunden haben (oder alle  Versuche verbraucht sind;
dann gehen wir davon aus, dass die Hashtabelle voll ist).
Abbildung  zeigt ein Beispiel mit einer
einfachen Sondierungsmethode.   [caption=Methode add einer HashTabelle für Hashing mit Sondierungen, label=hashing:probing:table-add]
class ProbingHashTable(m):
  ...
  function add(x):
    i=0, j=0
    while j < m:
      i = (h(x) + p(x,j)) mod m
      if A[i] == NIL or A[i] == DEL:
        break

      j++

    if j == m:
      error "No space left in hash table."

    A[i] = x
    n++

Das Suchen eines Schlüssel  in der Hashtabelle funktioniert analog.
 Wir vergleichen zunächst  mit dem Schlüssel an Position  in .
 Haben wir  gefunden, sind wir fertig; anderenfalls vergleichen wir mit
 Position  in  usw.
 Wieder unternehmen wir maximal  Versuche,  an den  Hashpositionen zu
 finden.
 Dabei können wir vorzeitig stoppen, wenn wir  gefunden haben.
 Finden wir eine freie Position , können wir ebenfalls vorzeitig stoppen.
 Denn: Befindet sich  in der Hashtabelle, so muss  schon beim Einfügen
 von  frei gewesen sein, da wir davon ausgehen, dass wir keine Schlüssel
 löschen. Der Schlüssel  wäre also bei  eingefügt worden, ein
 Widerspruch.
[caption=Methode find einer HashTabelle für Hashing mit Sondierungen, label=hashing:probing:table-find]
class ProbingHashTable(m):
  ...
  function find(x):
    i=0, j=0
    while j < m:
      i = (h(x) + p(x,j)) mod m
      if A[i] == NIL:
        return NIL

      if A[i] == x:
        return A[i]

      j++

    return NIL

 
fig-hashing-linear-probing

Hashing mit Sondierungen. Wir hashen mit  und
linearem Sondieren: Ist die gewünschte Position belegt, testen wir
die Position rechts daneben. Formal testen wir in Versuch  die
Position , mit ..


fig-hashing-linear-probing-delete


Fortsetzung von Abbildung . Nach dem Löschen des Schlüssels 22 markieren wir seine Position mit einem
nil-Zeiger, da ansonsten etwa die Sondierung von find(22) hier
vorzeitig stoppen würde. Der nil-Platzhalter zeigt dennoch an, dass
an dieser Position neue Elemente eingefügt werden können.


 Löschen in offener Adressierung

Löschen wir einen Schlüssel  aus der Hashtabelle erzeugen wir eine frei Position,
die dazu führen kann, dass wir beim Suchen eines (anderen) Schlüssels
fälschlicherweise vorzeitig stoppen.
Um dieses Problem zu vermeiden löschen wir nicht nur  aus der Hashtabelle,
sondern markieren seine Position zusätzlich mit dem Platzhalter del.
Wir modifizieren die Methode find zusätzlich so, dass sie nicht an
markierten freien Position vorzeitig stoppt.
Das gleiche Problem tritt auf, wenn wir verhindern möchten, dass Schlüssel doppelt
in die Hashtabelle eingefügt werden; auch hier müssen wir die Sondierungsreihenfolge
bis zum Ende oder zu einer unmarkierten freien Position verfolgen, bevor wir sicher sind,
dass der einzufügende Schlüssel noch nicht in der Tabelle enthalten ist.
Abbildung  zeigt ein Beispiel.




[caption=Methode delete einer HashTabelle für Hashing mit Sondierungen, label=hashing:probing:table-delete]
class ProbingHashTable(m):
  ...
  def delete(x):
    i=0, j=0
    while j < m:
      i = (h(x) + p(x,j)) mod m
      if A[i] == NIL:
        return

      if A[i] == x:
        free A[i]
        A[i] = DEL
        n-
        return

      j++
 



Lineares Sondieren

Wir haben in Abbildung  bereits lineares Sondieren gesehen.
Diese Sondierungsmethode testet bei einer Kollision der Reihe nach benachbarte
Positionen im Array bis eine freie Position gefunden wird.
Genauer setzen wir  für alle  und alle Schlüssel  und
erhalten
*
Die Sondierungsreihenfolge sucht also von  aus nach rechts.
Wir das Ende von  erreicht, suchen wir von  aus weiter.

Das lineare Sondieren illustriert sehr anschaulich die Funktionsweise von
Hashing mit Sondierungen.
Es hat in der Praxis aber den Nachteil, dass sich die Schlüssel in der Hashtabelle
im Allgemeinen selbst bei einer guten Wahl der Hashfunktion schlecht verteilen,
s. Abbildung :
Wir betrachten einen zusammenhängenden Bereich  von 
Schlüsseln(Zusammenhängend bedeutet hier, dass sich zwischen
den Schlüsseln keine freien Positionen befinden.) in  und wie vorher gehen wir
davon aus, dass die Hashfunktion die Schlüssel auf alle Positionen der Hashtabelle
gleichmäßig verteilt.
Wird nun ein neuer Schlüssel  eingefügt, dann trifft die Hashfunktion
den Bereich  mit Wahrscheinlichkeit , d.h. in  von 
Fällen.
Da wir linear Sondieren wird  dann am Rand des zusammenhängenden Bereiches
(also an Position  bzw. ) eingefügt, so dass der zusammenhängende
Bereich wächst und bei der nächsten Einfügeoperation eine noch größere Wahrscheinlichkeit
besteht, dass der Bereich getroffen wird.
Dieser Effekt wird primäre Häufung genannt.
Da  die Funktion  eindeutig festlegt gibt es nur  mögliche
Sondierungsreihenfolgen.
Falls also  ist, dann sind auch 
für alle .

fig-hashing-linear-probing-primary

Ein Beispiel mit primärer Häufung.
Zu Beginn des Beispiels enthält die Hashtabelle bereits drei Schlüssel, die in einem
zusammenhängenden Bereich der Hashtabelle stehen. Hier führen bereits 5 von 10 möglichen
Hashwerten dazu, dass der zusammenhängede Bereich vergrößert wird ().
Drei dieser Möglichkeiten führen zu mindestens einer Kollision.
Je mehr eingefügte Elemente mit dem zusammenhängenden Bereich kollidieren, je größer
wird auch die Wahrscheinlichkeit, dass zukünftige Einfügeoperationen Kollisionen
in dem Bereich erzeugen. Nach dem Einfügen von Element 12 führen bereits die Hälfte
der möglichen Hashwerte zu einer Kollision im zusammenhängenden Bereich, 7 mögliche
Hashwerte führen dazu, dass er größer wird.
Beim Einfügen des Schlüssels 83 erzeugen wir 5 Kollisionen, obwohl 83 das erste
Element mit Hashwert 3 ist.

Quadratisches Sondieren

  fig-hashing-quadratic
  
  
  Beispiel für quadratisches Sondieren mit  und .
  Da 
  ist, können wir die Schrittweiten  vorberechnen (s. Tabelle).
  Das quadratische Sondieren kann große Teile der Hashtabelle mit wenigen Kollision überspringen.
  Wir sehen allerdings auch, dass nicht jede Wahl von  und  dazu führt, dass die
  Sondierungsreihenfolge die gesamte Hashtabelle abdeckt.
Quadratisches Sondieren versucht primäre Häufung zu vermeiden. Hier lassen
wir den Abstand zur ursprünglichen Einfügeposition  quadratisch in der
Anzahl der Versuche wachsen.
Dies erreichen wir, indem wir für zwei passend gewählte Konstanten
 setzen und also
*
haben.
Abbildung  zeigt ein Beispiel.

Quadratisches Sondieren vermeidet bei bei geeigneter Wahl von  und 
primäre Häufungen, allerdings legt auch hier  die gesamte
Sondierungsreihenfolge fest und auch hier gibt es also nur  verschiedene
Sondierungsreihenfolgen.
Auch beim quadratischen Hashing verwenden alle Schlüssel die gleichen
Schrittweiten.


Doppeltes Hashing



Ein Beispiel für doppeltes Hashing mit  und
. Da die Sondierungsreihenfolgen hier vom Schlüssel abhängen,
können wir nicht wie beim quadratischen Hashing eine Sondierungsreihenfolge für
alle Schlüssel vorberechnen. Stattdessen berechnen wir für jeden Schlüssel seine
Schrittweite, s. Tabelle.
Doppeltes Hashing verwendet eine zweite Hashfunktion , um Schlüsseln unterschiedliche
Schrittweiten zuzuweisen und die Anzahl der Sondierungsreihenfolgen zu erhöhen.
Wir setzen  und erhalten
*
Abbildung  zeigt ein Beispiel.


Laufzeitanalyse

Die Laufzeitanalyse für Hashing mit Sondierungen erfordert Wahrscheinlichkeitstheorie,
so dass wir hier auf Beweise verzichten und lediglich das Ergebnis der Analyse
sehen werden.
Interessierte Leser:innen finden die Analyse aber in Kapitel 11 von .
Die Analyse hängt nicht von einer festen Sondierungsmethode ab.
Stattdessen basiert sie wieder auf einer Gleichverteilungsannahme und gilt
für alle Sondierungsmethoden, die diese Annahme erfüllen.
Die Sondierungsmethoden die wir kennengelernt haben - lineares Sondieren,
quadratisches Sondieren und doppeltes Hashing - versuchen diese Gleichverteilungsannahme
so gut wie möglich zu erfüllen.
Wir nehmen also im Folgenden an, dass unser Sondierungsschema derart ist, dass
jede Sondierungssequenz, d.h. jede der mögliche  Permutation der Zahlen
, die gleiche Wahrscheinlichkeit hat als Sondierungssequenz für einen
Schlüssel  gewählt zu werden.
Natürlich wählen wir die Sequenz beim Einfügen, Löschen oder Suchen von  nicht
zufällig. Aber: Wenn wir einen zufälligen Schlüssel wählen, dann soll die Wahrscheinlichkeit,
dass seine Sondierungssequenz eine feste Sequenz  ist, gerade 
betragen.
Anders ausgedrückt: Wenn wir die Sondierungssequenzen aller möglichen
Schlüssel berechnen, finden wir jede mögliche Sequenz gleich oft.
Unter dieser Annahme ist die Anzahl der Sondierungen bei einer erfolglosen Suche
im Mittel höchstens  sofern der Auslastungsfaktor 
ist(Da wir höchstens  Elemente speichern können, kann der Fall 
nicht eintreten.).
Als direkte Folge erhalten wir, dass unter unserer Annahme auch die Methode add
im Mittel höchstens  Sondierungen durchführt, wenn  ist.
Die Anzahl der Sondierungen bei einer erfolgreichen Suche (und damit auch in der
Methode delete) beträgt im Mittel höchstens 
mit .
Hierbei setzen wir allerdings voraus, dass alle Schlüssel mit der gleichen
Wahrscheinlichkeit (d.h. gleich oft) gesucht werden.
Solange wir also den Auslastungsfaktor durch eine Konstante beschränken können
(etwa mit Hilfe unseres Verdopplungsschemas) bleibt auch die Anzahl der
Sondierungen konstant.


Universelles Hashing

Wir hatten in Abschnitt  das Verhalten von
Hashing mit Verkettungen untersucht und herausgefunden, dass es unter einer
Gleichverteilungsannahme über die Schlüssel eine gute durchschnittliche
Laufzeit aufweist.
In praktischen Anwendungen sind Verteilungsannahmen über die Schlüssel aber
schwierig zu überprüfen und nicht notwendigerweise gerechtfertigt.
Erinnern wir uns, dass wir ein ähnliches Problem bei der Analyse von QuickSort
gesehen hatten: Dort hatten wir zunächst die durchschnittliche Laufzeit unter
einer Gleichverteilungsannahme über das Eingabearray analysiert.
Anschließend hatten wir die Annahme über die Gleichverteilung der Eingabe
(die wir nicht kontrollieren können) in eine Annahme über die gleichverteilte
verteilte Wahl des Pivotelements (die wir kontrollieren können) umgewandelt.
Wir möchten hier ein ähnliches Prinzip anwenden und wählen unsere Hashfunktion
zufällig. Damit verschieben die Verteilungsannahme in die kontrollierbare
Wahl der Hashfunktion.

Dazu betrachten wir im Folgenden Hashfunktionen der Form
*
mit einer Primzahl , die so groß ist, dass jeder mögliche Schlüssel einen Wert
aus  annimmt.
Außerdem sei  und .
Die Menge aller dieser Hashfunktionen schreiben wir als
*
Die Menge  enthält für ein festes  und ein festes  genau 
Hashfunktionen und bildet eine universelle Hashklasse im folgenden Sinn:
  
  Eine Menge
  *
  von Hashfunktionen, die Schlüssel aus einem Universum
   auf Positionen  abbilden heißt
  universelle Hashklasse, wenn Folgendes gilt:
  Seien  zwei verschiedene Schlüssel () aus .
  Dann gibt es in  höchstens 
  Hashfunktionen , die  und  auf den gleichen Wert hashen (d.h., für die
   gilt).
Es ist bekannt, dass  eine universelle Hashklasse ist (ohne Beweis):
  Sei  eine Primzahl und . Seien  zwei verschiedene Schlüssel () aus
  .
  Dann gibt es in  höchstens 
  Hashfunktionen , die  und  auf den gleichen Wert hashen (d.h., für die
   gilt).





Betrachten wir also zwei feste Schlüssel  und wählen wir irgendeine
Hashfunktion  aus , so führen höchstens 
der  möglichen Wahlen zu einer Kollision von  und .
Der Anteil der Hashfunktionen in , die zu einer Kollision von
 und  führen liegt also bei höchstens .
Das bedeutet: Wählen wir  zufällig
gleichverteilt(Das heißt, dass wir jede mögliche Hashfunktion mit Wahrscheinlichkeit
 wählen),
so beträgt die Wahrscheinlichkeit für eine Kollision von  und  höchstens .
Unsere zufällige Wahl erfüllt die Gleichverteilungsannahme ohne etwas darüber
vorauszusetzen, welche Schlüssel gehasht werden oder welcher Verteilung die
Schlüssel folgen.

Fazit und Ausblick

Vor- und Nachteile von Hashing mit Sondierungen

Wie bereits angedeutet liegt der Vorteil von Hashing mit Sondierungen darin,
dass alle Schlüssel direkt in der Hashtabelle abgelegt werden.
Auf diese Weise sparen wir den Speicherplatz, der ansonsten für die Verkettung
benötigt wird und sind besser an moderne Hardwarearchitekturen angepasst.
Der große Nachteil des Verfahrens ist allerdings, dass die Laufzeit der erfolgreichen
und der erfolglosen Suche nicht nur von gespeicherten, sondern auch von bereits
gelöschten Schlüsseln abhängen.
Dieser Nachteil kann mit zusätzlichem Aufwand beim Löschen abgemildert werden
(es ist möglich, nach einer Löschoperation Schlüssel geignet so zu verschieben,
dass keine Lücken in der Sondierungssequenz auftreten).
Dennoch eignet sich verkettetes Hashing besser, wenn wir mit vielen Löschoperationen
rechnen.
Im Gegensatz zu Hashing mit Sondierungen erlaubt verkettetes Hashing das Einfügen
in konstanter Worst-Case-Laufzeit, sofern wir es erlauben, dass Schlüssel
mehrfach eingefügt werden.
Für das verkettete Hashing genügt es, eine gute Hashfunktion zu wählen;
möchten wir Sondieren, laufen wir Gefahr, dass zusätzlich eine ungünstige
Sondierungssequenz dazu führt, dass wir die Kapazität der Hashtabelle nicht
ausnutzen können.

Vorteile der verschiedenen Implementierungen der dynamischen Mengen

Wir haben in den vergangenen Kapiteln mehrere Arten von dynamischen Mengen
gesehen.
Doppelt verkettete Listen erlauben schnelle Einfügen und Löschen, benötigen
aber im Worst-Case lineare Zeit für die Suche nach Schlüsseln.

Rot-Schwarz-Bäume garantieren eine logarithmische Worst-Case-Laufzeit für
Suchoperationen, erkaufen das aber mit langsameren Einfüge- und Löschoperationen.
Mit Hilfe eines Rot-Schwarz-Baumes können wir außerdem in logarithmischer
Worst-Case-Laufzeit den kleinsten und den größten Schlüssel der Menge
bestimmen. Ebenso finden wir in logarithmischer Worst-Case-Laufzeit den
Vorgänger und Nachfolger eines Schlüssels (also den nächst kleineren bzw.
nächst größeren Schlüssel, der in der Menge enthalten ist) und
können in Linearzeit die Menge in sortierter Reihenfolge ausgeben.

Hashtabellen besitzen schlechte Worst-Case-Garantieren, sind aber in der
Praxis sehr schnell.
Ihr Nachteil liegt darin, dass sie keine feste Reihenfolge der Schlüssel
garantieren: Geben wir wiederholt die Schlüsselmenge einer Hashtabelle aus,
kann es passieren, dass wir die Schlüssel in unterschiedlichen Reihenfolgen
sehen, weil etwa in der Zwischenzeit eine neue Hashfunktion gewählt wurde (z.B.
weil der Auslastungsfaktor zu groß wurde oder Schlüssel schlecht verteilt waren).
Tatsächlich benötigen wir im Worst-Case Zeit  um alle  Schlüssel
einer Hashtabelle der Größe  auszugeben.

Anwendung: Dictionary

In vielen Anwendungen soll nicht nur ein Schlüssel, sondern auch mit dem Schlüssel
assozierte Daten gespeichert werden.
Eine Datenstruktur, die solche Paare aus (eindeutigen) Schlüsseln und Daten (Wert)
speichert heißt Dictionary.

  [add(x,v)] Speichert einen Schlüssel  mit Wert  im Dictionary.
  [delete(x)] Entfernt den Schlüssel  sowie den assoziierten Wert 
  aus dem Dictionary.
  [get(x)] Gibt den Wert  zurück, der mit Schlüssel  gespeichert
  wurde.
Wir können ein Dictionary implementieren, indem wir ein Schlüssel-Wert-Paar in
einer dynamischen Menge unserer Wahl (Liste, Binärbaum, Hashtabelle) speichern.
[caption=Klasse für Paare aus einem Schlüssel und einem Wert]
class KVPair(k,v):
    key = k
    value = v




Viele gängige Programmiersprachen besitzen Implementierungen von
dynamischen Mengen und Dictionaries.

 [Java (OpenJDK)] Mit Rot-Schwarz-Bäumen: Die Klasse TreeMap implementiert ein
 Dictionary, die Klasse TreeSet eine dynamische Menge.
 Die Klassen HashMap bzw. HashSet benutzen Hashing mit
 Verkettung um ein Dictionary bzw. eine dynamische Menge zu implementieren.
  [Python] Ein Dictionary findet sich im eingebauten Typ dict(),
  eine dynamische Menge als set().
  Beide Datenstrukturen sind als Hashing mit Sondierungen implementiert.
  [C++ (gcc)] Die Klassen map und set implementieren ein
  Dictionary bzw. eine dynamische Menge als Rot-Schwarz-Baum.
  Die Klassen unorderedmap und unorderedset
  verwenden stattdessen Hashing mit Verkettung.



Prioritätswarteschlangen

Betrachten wir als einführendes Beispiel in diesem Kapitel ein Betriebssystem,
das Prozesse auf einem Prozessor ausführen soll. Der Benutzer - und auch das
Betriebsystem selber - kann zu beliebigen Zeitpunkten neue Prozesse starten,
aber der Prozessor kann stets nur einen (oder wenige) Prozesse gleichzeitig ausführen.
Es kann also passieren, dass der Prozessor gerade beschäftigt ist, wenn der Benutzer
einen neuen Prozess startet.
In diesem Fall muss der Prozess in einer Warteschlange warten.
Dazu können wir eine Queue aus Kapitel  verwenden.
Oft ist es aber sinnvoll, die Prozesse mit Prioritäten zu versehen; so wird ein Benutzer erwarten,
dass die von ihm manuell gestarteten Programme auch zeitnah ausgeführt werden,
während die regelmäßige automatische Indizierung des Dateisystems warten kann.
Das Betriebssystem sollte also zu jedem Zeitpunkt den Prozess aus der Queue
wählen, der gerade die höchste Priorität besitzt.
Für diesen Zweck gibt es PriorityQueues (Prioritätswarteschlangen).

Prioritätswarteschlangen
Eine Prioritätswarteschlange verwaltet Schlüssel, die mit einer Priorität (und
ggf. weiteren Daten) versehen sind.
Der Einfachheit halber gehen wir hier davon aus, dass die Schlüssel ganze Zahlen
sind und dass die Priorität als Schlüssel verwendet wird, so dass wir sie nicht
gesondert speichern.
Außerdem setzen wir für die Beschreibung voraus, dass die Schlüssel eindeutig
sind, d.h. dass jeder Schlüssel höchstens einmal in der Warteschlange vorkommt.

Eine (Max-)Prioritätswarteschlange  unterstützt folgende Operationen.

  [insert(x)] Fügt ein Element mit Schlüssel  (mit Priorität ) zur Warteschlange
  hinzu.
  [extract-max()] Liefert das Element mit größtem Schlüssel (und daher mit größter Priorität)
  aus  zurück. Entfernt das Element aus der Datenstruktur.
  [increase-key(e, x)] Erhöht den Schlüssel/die Priorität von  auf .
  Das Element  muss in der Warteschlange enthalten sein, und der neue Schlüssel 
  muss mindestens so groß sein wie der bisherige Schlüssel von .

Analog können wir eine Min-Prioritätswarteschlange definieren, die mittels einer Methode
extract-min() einen kleinsten Schlüssel zurückliefert und über eine
Methode decrease-key(e,x) statt increase-key(e,x) verfügt.
Haben wir nur eine Max-Prioritätswarteschlange zur Verfügung, können wir eine
Min-Prioritätswarteschlange nachahmen, indem wir stets  statt  einfügen.

Binäre Heaps: Eine Implementierung von Prioritätswarteschlangen

 
        fig-heap-1
    
    Ein Heap. Heaps sind Binärbäume. Sie erfüllen nicht die Suchbaumeigenschaft. Stattdessen erfüllen sie die Eigenschaft, dass die Kinder eines Knotens kleinere Schlüssel enthalten als der Knoten selbst. Diese Eigenschaft verrät uns nichts darüber, wie sich linke und rechte Kinder zueinander verhalten. Im Bild sieht man zum Beispiel, dass der im linken Kind von  gespeicherte Schlüssel  größer ist als der im rechten Kind gespeicherte Schlüssel  - bei den Kindern von  hingegen ist es andersherum.
 
 
        fig-heap-2
    
    Arraydarstellung des Heaps aus Abbildung . In einem vollständigen Binärbaum der Höhe  können  Schlüssel gespeichert werden. Der abgebildete Heap enthält nur  Schlüssel. Diese werden ebenenweise  im Array abgelegt. Da alle Ebenen bis auf die unterste voll besetzt sind, finden sich die Kinder des in  gespeicherten Schlüssels immer an den Positionen  und . 
 

Ein binärer Heap(Der dynamische Speicherbereich eines Programms wird
ebenfalls häufig als Heap bezeichnet; diese Namensgleichheit ist aber ein Zufall.
Die hier betrachteten Heaps haben keinen (den Autoren bekannten) Zusammenhang
zum dynamischen Speicherbereich.)
implementiert die Schnittstelle einer Prioritätswarteschlange.
Der Heap verwendet dazu einen (fast) vollständigen Binärbaum: Jede Ebene außer der
untersten ist vollständig gefüllt.
Die unterste Ebene wird von links nach rechts befüllt, s. Abbildung .
Wie wir sehen ist der abgebildete Heap zwar ein Binärbaum, aber kein binärer
Suchbaum.


 Da der Baum bis auf die unterste Ebene vollständig ist und von links befüllt wird,
können wir ihn effizient in einem Array  verwalten und benötigen keine Zeigerstruktur.
Dazu speichern wir die Wurzel des Baumes in .
Die Kinder der Wurzel  speichern wir in  und .
Allgemein finden wir das linke Kind von  an der Position .
Das rechte Kind von  speichern wir in .
Wir können daher den Elternknoten von  an Position  in 
finden.
Abbildung  illustriert dieses Prinzip.
Ein binärer Heap mit Höhe  muss immer mindestens  vollbesetzte Ebene haben
und wird daher mindestens  Knoten enthalten.
Wir folgern, dass ein binärer Heap mit  Schlüsseln eine Höhe von höchstens
 besitzt.

Im Folgenden bezeichnen wir der Einfachheit halber den Index des linken Kindes
von  mit ), den Index des rechten Kindes von  mit 
und den Index des Eltern von  mit .
Listing  zeigt Pseudocode für die Initialisierung
eines solchen Heaps.
In unserer Implementierung ist der Heap auf  Elemente beschränkt; wir können
die Implementierung aber mittels eines Verdopplungsschema einfach so erweitern,
dass beliebig viele Elemente gespeichert werden können.
[caption=Initialer binärer Heap für maximal  Elemente, label=lst:prioqueues:binheap-init]
class BinaryHeap(m):
  A = Array(m) # speichert den Binaerbaum
  n = 0        # Anzahl gespeicherter Schluessel

  function left(i):
    return 2*i + 1

  function right(i):
    return 2*i + 2

  function parent(i):
    return (i-1) div 2

  function size():
    return n

  function peek-max():
    if n == 0:
      Error "Heap is empty."

    return A[0]

Damit wir den größten Schlüssel effizient auffinden können sorgen wir nun dafür,
dass stets die folgende (Max-)Heapeigenschaft erfüllt ist.

Für jedes Element  des Heaps mit  Elementen gilt:
*
Jedes Element im Heap muss also erfüllen, dass sein Schlüssel mindestens so groß
ist wie die Schlüssel seiner Kinder, sofern diese existieren.
Das Element mit dem größten Schlüssel befindet sich also an der Wurzel.
Wir können analog eine Min-Heapeigenschaft formulieren.


Wiederherstellung der Heap-Eigenschaft

Wenn die Heapeigenschaft nach einer Schlüsseländerung verletzt ist, kann das zwei
Gründe haben: Erstens, wir haben den Schlüssel eines Elementes erhöht.
Dann kann es passieren, dass der Schlüssel des neuen Elementes größer als der seines Elternknotens ist.
Dieses Problem lösen wir, indem wir die Verletzung mit der Methode heapify-up
nach oben Richtung Wurzel schieben.
Zweitens, haben wir den Schlüssel eines Elementes verringert, kann es passieren, dass er nun
kleiner als der Schlüssel von einem Kind oder von beiden Kindern ist.
Dieses Problem lösen wir, indem wir die Verletzung mit der Methode heapify-down
nach unten schieben.
Schauen wir uns die Situationen genauer an.

Die Verletzung nach oben schieben

Nehmen wir an, dass wir von einem intakten Heap ausgehen und den Schlüssel 
von  auf  erhöhen.


fig-heap-3
 
 Im Heap aus Abbildung  wird der Schlüssel  durch den Wert  ersetzt. Die Heapeigenschaft ist nun verletzt (Abbildung oben rechts), da  größer als der Schlüssel des Elterknoten ist. Die Knoten mit den Schlüsseln  und  werden vertauscht. Nun ist  größer als der Schlüssel des neuen Elterknotens , was eine weitere Vertauschung auslöst. Nach dem zweiten Tausch ist die Heapeigenschaft wiederhergestellt.

Durch die Erhöhung von  auf  kann es wie in Abbildung (obere Zeile) dargestellt passieren, dass
das veränderte Element nun einen größeren Schlüssel als sein Elternelement
besitzt, d.h. dass nun  ist.
Ist das tatsächlich der Fall, vertauschen wir  und .
Das stellt sicher, dass nun wieder  ist.
Allerdings könnte nun  wiederum einen größeren Schlüssel als sein
Elternelement enthalten. Wir haben die Verletzung also eine Ebene nach oben geschoben und fahren mit der Reparatur bei  fort. Die zweite Zeile von Abbildung  illustriert diesen Prozess.

[caption=Methode heapify-up, label=lst:prioqueues:binheap-heapify-up]
class BinaryHeap(m):
  ...
  function heapify-up(i):
    while i != 0 and A[i] > A[parent(i)]:
      swap(A[i], A[parent(i)])
      i = parent(i)


Wir argumentieren in Abbildung , dass die Heapeigenschaft an allen am Tausch nicht beteiligten Knotpaaren erfüllt bleibt. 
Damit sind wir von der Korrektheit unserer Methode überzeugt. Listing  zeigt den Pseudocode von heapify-up.

  fig-heap-4
 
 Die Abbildung zeigt einen Ausschnitt aus einem Heap. Dabei muss  nicht die Wurzel sein. Außerdem ist willkürlich, dass  rechtes Kind von  ist und  linkes Kind von .
  Der Schlüssel des mit  bezeichneten Knotens wird auf einen Wert  erhöht, so dass die Heapeigenschaft verletzt ist. Vor der Erhöhung war die Heapeigenschaft an allen Knoten erfüllt.
  Wir vertauschen nun  und  und überlegen uns, welchen Effekt dies auf den Baum hat.
  Für alle Knoten, die oberhalb von  oder in einem der mit  dargestellten Bereiche liegen, ändert dies nichts. Die Heapeigenschaft bleibt dort erfüllt, weil sich kein Schlüssel verändert hat. Für die Schlüssel  und  gilt aufgrund der vorher erfüllten Heapeigenschaft  und ; außerdem gilt , so dass folgt, dass  und  kleiner als  sind. Ähnlich argumentieren wir für . Es gilt  und wir ersetzen  durch den größeren Schlüssel , so dass  gilt und die Heapeigenschaft erfüllt ist. Einzig für die Beziehung zwischen  und  wissen wir nicht, ob die Heapeigenschaft erfüllt ist und müssen dies nun weiter prüfen.
  

Da wir in jeder Schleifeniteration entweder die Verletzung eine Ebene nach
oben schieben oder stoppen, besitzt die Methode eine Worst-Case-Laufzeit von
.

Die Verletzung nach unten schieben
Im anderen Fall nehmen wir an, dass wir einen intakten Heap hatten und dann den
Schlüssel  von  auf  verringert haben.


fig-heap-5
 
 Im oben links abgebildeten Heap wird der Schlüssel  auf  verringert. Dadurch wird die Heapeigenschaft verletzt:  ist sowohl kleiner als der Schlüssel des linken Kindes als auch kleiner als der Schlüssel des rechten Kindes. Der Knoten mit Schlüssel  wird daher mit dem Knoten mit Schlüssel  vertauscht, da  größer als  ist. Nun ist die Heapeigenschaft bei den beiden Knoten mit Schlüsseln  und  verletzt. Nach dem zweiten Tausch ist die Heapeigenschaft überall wiederhergestellt. 

Wie in Abbildung  dargestellt könnte nun  die Heapeigenschaft mit
einem seiner Kinder verletzen (d.h., möglicherweise ist  oder 
oder beides).
Ist das der Fall, schieben wir die Verletzung nach unten, indem wir  mit
dem größeren der beiden Kinder vertauschen (existiert nur ein Kind, vertauschen
wir natürlich mit diesem).
Nennen wir dieses Kind .
Da wir das Größere der beiden Kinder nach  getauscht haben, ist nun
der Schlüssel in  größer als der beider seiner Kinder (sofern diese überhaupt
existieren).
Allerdings verursacht  an seiner neuen Position  nun möglicherweise eine
Verletzung der Heap-Eigenschaft mit seinen Kindern.
Wir fahren daher bei  mit der Reparatur fort.
Dabei zeigt die zweite Zeile von Abbildung , wie wir in einem
Beispieldurchlauf die Heapeigenschaft durch wiederholtes Austauschen wiederherstellen.
Dieser Vorgang wird auch versickern genannt.


[caption=Methode heapify-down, label=lst:prioqueues:binheap-heapify-down]
class BinaryHeap(m):
  ...
  function heapify-down(i):
    # solange A[i] ein linkes Kind hat
    while left(i) < n:
      j = left(i)

      # j wird Index des groesseren Kindes
      if right(i) < n and A[right(i)] > A[j]:
        j = right(i)

      # stoppe, falls groesseres Kind kleiner als A[i]
      if A[j] < A[i]:
        break

      # ansonsten tausche A[i] mit groesserem Kind
      swap(A[i], A[j])

      # weiter mit vertauschten Kind
      i = j

Wir überzeugen uns in Abbildung , dass nach dem Vertauschen zweier Knoten überall außer an dem ausgetauschten Knoten und seinen neuen Kindern die Heapeigenschaft gilt und wir daher die Verletzung erfolgreich eine Ebene nach unten geschoben haben.

fig-heap-6
 
 Die Abbildung zeigt einen Ausschnitt aus einem Heap. Dabei muss  nicht die Wurzel sein. Außerdem ist willkürlich, dass  rechtes Kind von  ist und  linkes Kind von .
  Der Schlüssel des mit  bezeichneten Knotens wird auf einen Wert erhöht, der größer als , größer als  oder größer als  und  ist. Dadurch wird die Heapeigenschaft verletzt. Wir tauschen  mit dem größeren der beiden Schlüssel der Kinderknoten. Nehmen wir an, dass dieses der Knoten  ist. Nach dem Austausch von  und  gilt die Heapeigenschaft für das Paar  und , da  das größere Kind war. Außerdem gilt sie für das Paar  und , weil aufgrund der vorher geltenden Heapeigenschaft  gelten muss. Für  und die Kinder von  kann die Heapeigenschaft verletzt sein.
  

Da wir in jeder Schleifeniteration entweder die Verletzung eine Ebene nach
unten schieben oder stoppen, besitzt die Methode eine Worst-Case-Laufzeit von
.





















Einfügen in binäre Heaps

Um ein neues Element mit Schlüssel  in einen binären Heap einzufügen schreiben
wir  zunächst an Position , also hinter den letzten Schlüssel, und
erhöhen  um eins.
In der Binärbaumdarstellung bedeutet das, dass wir ein neues Blatt soweit links
wie möglich auf der untersten Ebene einfügen (ist die unterste Ebene voll, so
eröffnen wir dafür eine neue Ebene).
Das neue Blatt  verletzt potentiell die Heap-Eigenschaft mit seinem Elternknoten
(d.h., es könnte  gelten), so dass wir nun heapify-up(n-1)
aufrufen (wenn wir ganz präzise sein möchten, können wir uns vorstellen, dass wir zunächst
ein Blatt mit Schlüssel  einfügen. Auf diese Weise ist die Heapeigenschaft
nach dem Einfügen weiterhin erfüllt. Dann verwenden wir increase-key, um
den Schlüssel des neuen Blattes auf  zu erhöhen).
Listing  zeigt den Pseudocode dieser Methode.

[caption=Methode insert, label=lst:prioqueues:binheap-insert]
class BinaryHeap(m):
  ...
  function insert(x):
    if n == A.laenge:
      error "Heap is full."

    A[n] = x
    n += 1
    heapify-up(n-1)

Die Methode besitzt eine Worst-Case-Laufzeit von .

Entfernen des Maximums


fig-heap-7
 
 In diesem Beispiel wird die Wurzel des Heaps ausgegeben und entfernt. Das letzte Element des Arrays enthält den Schlüssel , welcher in die Wurzel getauscht wird. Anschließend wird die Heapeigenschaft durch Vertauschungen wiederhergestellt.
  
Der größte Schlüssel des Heaps steht in , so dass wir ihn einfach auslesen
und zurückgeben können. Die Schnittstelle schreibt für extract-max() jedoch
auch vor, dass der größte Schlüssel entfernt werden soll.
Die Wurzel auf eines der Kinder von  zu verschieben würde aber die
Binärbaumstruktur zerstören, so dass wir stattdessen  zunächst
mit dem rechtesten Blatt  tauschen. Anschließend können wir 
entfernen und  um eins reduzieren.
Es kann allerdings passieren, dass wir nach dem Tausch die Heapeigenschaft zwischen
der Wurzel und ihren Kindern verletzen, wenn sich ihr Schlüssel durch den
Tausch verringert hat. Wir rufen daher heapify-down(0) auf.
Listing  zeigt den Pseudocode der Methode
und Abbidlung  einen Beispieldurchlauf.
Die Methode besitzt eine Worst-Case-Laufzeit von .
[caption=Methode extract-max(), float, label=lst:prioqueues:binheap-extract-max]
class BinaryHeap(m):
  ...
  function extract-max():
    max_key = A[0]

    n -= 1
    swap(A[0], A[n])

    heapify-down(0)
    return max_key
Erhöhen eines Schlüssels

Erhöhen wir den Schlüssel in  führt das potentiell dazu, dass 
ist.
Wir rufen also heapify-up() auf.
Listing  zeigt den Pseudocode der Methode.
[caption=Methode increase-key(), label=lst:prioqueues:binheap-increase-key]
class BinaryHeap(m):
  ...
  function increase-key(i, x):
    if x <= A[i]:
      Error "Cannot increase to smaller key."

    A[i] = x
    heapify-up(i)
Die Methode besitzt eine Worst-Case-Laufzeit von .

Herstellung der Heapeigenschaft in Linearzeit

Wir können aus einer beliebige Menge von  Schlüsseln einen Heap erzeugen,
indem wir die Schlüssel nacheinander in einen anfänglich leeren Heap einfügen.
Da jeder Einfügeoperation im Worst-Case eine Laufzeit von  besitzt,
erzeugen wir also den Heap in Zeit .

Es stellt sich aber heraus, dass wir einen Heap über  Schlüsseln auch in Zeit 
erzeugen können.
Dazu schreiben wir zunächst die  Schlüssel in beliebiger Reihenfolge in ein
Array und interpretieren das Array als binären Baum.
Dann laufen wir von rechts nach links (also für ) durch das
Array und rufen für jeden Eintrag  die Methode heapify-down(i) auf.
Auf den ersten Blick hat diese Methode auch eine Laufzeit von , wir
zeigen aber gleich, dass eine genauere Betrachtung eine Laufzeit von  liefert.
Das liegt daran, dass die Laufzeit heapify-down(i) proportional zur Höhe von 
ist und die meisten Schlüssel des Heaps auf den unteren Ebenen liegen.
Listing  gibt den Pseudocode der resultierenden Methode an
und Abbildungen  und  zeigen einen Beispieldurchlauf.
Die Methode makeheap ist so implementiert, dass wir ein  übergeben können
und sie aus den ersten  Elementen von , d.h. aus , einen Heap macht.

[caption=Methode makeheap(), label=lst:prioqueues:make-heap]
function make_heap(A, n):
  b = BinaryHeap(A.laenge)
  b.A = A
  b.n = n

  for i=(b.n div 2)-1,...,0:
    b.heapify-down(i)

  return b


fig-heap-8
 
 Ein Beispieldurchlauf der Methode makeheap(A, 11). In diesem Beispiel benötigen wir nur fünf Vertauschungen, um die Heapeigenschaft herzustellen. Abbildung  zeigt denselben Durchlauf in der Baumdarstellung. 


fig-heap-9
 
 Ein Beispieldurchlauf der Methode makeheap().
 Die erste Verletzung der Heapeigenschaft tritt direkt beim Knoten mit Schlüssel , dem
 letzten inneren Knoten, auf.
 Dies ist  der erste, den wir kontrollieren; er ist in  gespeichert.
 Bei  ist die Heapeigenschaft bereits erfüllt. Wir laufen nun im Array nach links und überprüfen und reparieren die Heapeigenschaft.
 Bei heapify-down() werden zwei Vertauschungen ausgeführt.
 Abbildung  zeigt denselben Durchlauf in der Arraydarstellung. 


Überzeugen wir uns, dass diese Methode tatsächlich überall die Heapeigenschaft
herstellt.
Wir zeigen per vollständiger Induktion folgende Invariante:

Wenn wir heapify-down(i) aufrufen, ist jeder Knoten mit Index 
die Wurzel eines (Max-)Heaps.
Die Induktion läuft rückwärts, weil die Knoten mit den höheren Indizes zu
Teilbäumen mit geringerer Höhe gehören.

Induktionsanfang. Für  gilt die
Induktionsvoraussetzung, denn die Indizes  gehören zu den
Blättern des binären Heaps: Ein Knoten ist hier ein Blatt, wenn er kein linkes Kind
besitzt und für  ist .
Jedes Blatt für sich betrachtet ist aber ein ein-elementiger Heap.

Induktionsschritt. Betrachten wir jetzt ein .
Die Kinder von  besitzen Indizes, die echt größer als  sind, so dass
wir die Induktionsvoraussetzung anwenden können.
Wir wissen also, dass der linke und der rechte Teilbaum unter  bereits
Max-Heaps sind. Falls der Teilbaum unter  kein Max-Heap ist, muss
das also daran liegen, dass  kleiner als eines seiner Kinder ist.
Wir hatten schon gesehen, dass wir dieses Problem beheben können, indem wir
heapify-down(i) aufrufen. Nach dem Aufruf ist  also die Wurzel
eines Max-Heaps. Dass die Knoten  Wurzeln eines Max-Heaps sind
erhalten wir aus der Induktionsvoraussetzung.
Damit haben wir die Induktionsvoraussetzung für  gezeigt.

Abschließend erhalten wir aus der Invarianten, dass der letzte Aufruf
heapify-down(0) die Heapeigenschaft für den gesamten Heap herstellt,
weil die Kinder  und  gemäß der Invarianten Wurzeln eines Teilheaps
sind.

In der tatsächlichen Implementierung können wir den heapify-down(i)-Aufruf
für die Blätter, d.h. die Indizes  sparen,
s. Listing .

Für die Laufzeitanalyse von makeheap betrachten wir einen Heap mit
Höhe .
Auf Ebene  enthält der Heap höchstens  Knoten.
Wird heapify-down(i) auf Ebene  aufgerufen, ist die Laufzeit
von heapify-down(i) proportional zu .
Bei einem Aufruf auf Ebene 1 ist die Laufzeit proportional zu , usw.
Allgemein: Wird heapify-down(i) für einen Knoten  auf Ebene  aufgerufen,
so ist die Laufzeit von heapify-down(i) proportional zu .
Wir erhalten die Gesamtlaufzeit, wenn wir den Aufwand über die Ebenen des Heaps
summieren.
Da wir heapify-down nicht für die unterste Ebene des Heaps aufrufen
erhalten wir also eine Gesamtlaufzeit die proportional ist zu
*
Aus Lemma  folgt mit , dass 
gilt, und da  ist, folgt dass die Worst-Case-Laufzeit
von make-heap in  liegt.

Heapsort

Da wir mit einem Heap schnell den größten Schlüssel in einem Array finden
können ist es eine natürlich Idee, einen Heap zu verwenden um ein Array  zu
sortieren. Dazu könnten wir zunächst alle Schlüssel aus  in einen Heap
einfügen. Dann entfernen wir mit extract-max() den größten Schlüssel
aus dem Heap und schreiben ihn an Position .
Wenn wir jetzt nochmal extract-max() aufrufen, erhalten wir den
zweitgrößten Schlüssel der ursprünglichen Eingabe.
Diesen schreiben wir also nach , usw. Das Verfahren wiederholen wir, bis
der Heap leer ist.
Der Heap verwaltet also immer die Schlüssel, die bisher noch nicht eingefügt wurden
während wir nach dem -ten extract-max() Aufruf in 
die  größten Schlüsesl der Eingabe in sortierter Reihenfolge gespeichert
haben.
Wir haben also eine Variante von SelectionSort gefunden.
Die Worst-Case-Laufzeit des Verfahrens beträgt , wenn in 
gerade  Schlüssel gespeichert sind:
Wir können in Zeit  den initialen Heap aus  konstruieren, indem wir eine
Kopie von  machen und makeheap aufrufen.
Anschließend rufen wir  mal extract-max() auf; und da die Worst-Case-Laufzeit
von extract-max()  beträgt, dauert das im Worst-Case
Zeit .
Das Ablegen eines Schlüssels in  benötigt konstante Zeit.

Bei genauerer Betrachtung stellt sich heraus, dass wir dieses Verfahren mit
konstantem zusätzlichen Speicherplatz implementieren können, wenn wir den Heap
direkt in  verwalten.
Dazu erzeugen wir zunächst mit makeheap(A) einen Heap aus .
Rufen wir jetzt extact-max() auf, werden  und 
vertauscht,  dekrementiert und  mit heapify-down(0) versickert,
so dass die Heapeigenschaft auf  wieder hergestellt wird.
Wir haben nun also den größten Schlüssel aus  nach 
getauscht. Außerdem führt das Versickern von  dazu, dass nun der
größte Schlüssel in  an Position  steht!
Wir rufen nun solange extract-max() auf, bis  ist und haben
dann  sortiert.
Dieses Verfahren heißt HeapSort und ist in Listing  dargestellt.
Abbildung  zeigt ein Beispiel.


fig-heapsort-iteration

Eine Iteration von Heapsort. Oben links: Schlüssel 94 wurde bereits einsortiert und
die Wurzel enthält den größten der verbleibenden Schlüssel, die 91.
Oben rechts: Schlüssel 91 wird mit dem letzten Schlüssel des Heaps getauscht, das ist Schlüssel 18.
Nun ist 18 die Wurzel und die Heapeigenschaft ist verletzt.
Unten: Schlüssel 18 wird entlang des markierten Pfades versickert. Wir tauschen Schlüssel 18 jeweils mit dem Kind, das den
größeren Schlüssel enthält und stoppen, sobald beide Kinder von 18 kleiner sind als 18; dann ist die Heapeigenschaft wiederhergestellt.


fig-heapsort-example

Ein Beispieldurchlauf von Heapsort: In jedem Schritt steht das größte Element in  und
wird daher mit dem letzten Element des Heaps vertauscht. Nach dem Tausch wird das neue 
versickert. Die Abbildung zeigt jeweils das Ergebnis der Versickerung.
Abbildung  zeigt eine Iteration im Detail.


[caption=Heapsort mit unabhängiger Implementierung von heapify-down, label=lst:prioqueues:heap-sort]
function heapify-down(A, n, i):
    while left(i) < n:
      j = left(i)

      if right(i) < n and A[right(i)] > A[j]:
        j = right(i)

      if A[j] < A[i]: 
        break         

      swap(A[i], A[j])
      i = j

function heapsort(A):
    n = A.laenge
    # make_heap(A)
    for i=(n div 2)-1,...,0:
        heapify-down(A, n, i)

    for i=n-1,...,1: 
        # extract-max():
        swap(A[0], A[i])
        heapify-down(A, i, 0)



Ein beispielhafter Korrektheitsbeweis

Wir nutzen HeapSort um noch einmal einen sehr ausführlichen Korrektheitsbeweis
zu führen.
Dieser soll als Beispiel im Detail illustrieren, wie Korrektheitsbeweise
funktionieren und würde normalerweise kürzer formuliert werden.

Unser Argument für die Korrektheit des Algorithmus war, dass wir im vorderen
Teil von  einen Heap verwalten und aus diesem Heap immer den größten
Schlüssel heraustauschen.
Da wir den hinteren Teil von  von rechts füllen, speichern wir dort immer
den bereits sortierten Teil der Eingabe.
Wir formalisieren dieses Argument. Implizit enthält unsere Argumentation,
dass die Struktur vorne ein Heap, hinten sortiert immer gilt.
Um das immerauszudrücken, sagen wir: Bei jedem Durchlauf der for-Schleife
in Zeile  gilt für das aktuelle .

  im vorderen Teil von  verwalten wir einen Heap wird
  präziser zu:  ist ein Heap.  im hinteren Teil von  speichern wir den bereits sortierten
  Teil der Eingabe wird präziser zu:
   enthält die  größten Schlüssel der Eingabe
  in sortierter Reihenfolge.
Dabei können wir uns überzeugen, dass die Indizes richtig gewählt sind, indem
wir z.B.  einsetzen.

Jetzt müssen wir zeigen, dass unser Argument - das bisher nur eine Behauptung ist -
auch stimmt.
Dazu könnten wir ein beliebiges  festhalten und für dieses  argumentieren,
dass die Behauptung stimmt.
Wir stellen aber fest, dass wir beim Programmieren eine weitere implizite Annahme
getroffen haben: Wenn wir eine Iteration der Schleife starten, dann gehen wir davon
aus, dass  die behauptete Struktur vorne ein Heap, hinten sortiert
hat. Schließlich gehen wir z.B. davon aus, dass wir in  den größten Schlüssel
des vorderen Bereichs finden!
Tatsächlich soll eine Iteration der Schleife den vorderen Teil von  um ein Element
verkleinern und den hinteren Teil von  um ein Element vergrößern (in unseren
beiden Invarianten sehen wir das, wenn wir uns überlegen, dass  mit jeder
Iteration der Schleife um 1 sinkt).
Diese implizite Annahme müssen wir in unseren Beweis mit einbauen, denn die
  Schleife tut nicht das richtige, wenn z.B. der vordere Teil kein Heap ist; in
diesem Fall kann es also auch keinen Korrektheitsbeweis geben.
Wir machen daher einen Induktionsbeweis - das erlaubt uns eine beliebige Iteration
der Schleife (also eine beliebige Wahl von ) anzuschauen und vorauszusetzen, dass
alle vorherigen Iterationen wie gewünscht funktioniert haben.
Damit der Induktionsbeweis funktioniert, müssen wir zwei Aussagen beweisen.
Erstens, dass unsere behaupteten Invarianten gelten, wenn wir die for-Schleife
zum ersten Mal erreichen (Induktionsanfang).
Zweitens, dass jede Iteration der for-Schleife die Invarianten erhält (Induktionsschritt).

Für den Induktionsanfang müssen wir nachweisen, dass die Behauptung mit  gilt, denn
das ist die Wahl von  zu Beginn der ersten Iteration der Schleife.
Setzen wir also  in die Invarianten ein und überprüfen, ob wir eine wahre
Aussage erhalten.

  Behauptung:  ist ein Heap.
  Die Aussage stimmt, denn wir führen vor der for-Schleife makeheap
  auf dem gesamten Array  aus.
  Behauptung:  enthält die 
  größten Schlüssel der Eingabe in sortierter Reihenfolge.
  Die Aussage ist sicher wahr, denn  ist ein leeres Array
  und enthält 0 Schlüssel.
Wir wissen also, dass die Invarianten gelten, wenn wir Zeile zum ersten mal erreichen.
Jetzt könnten wir argumentieren, dass die erste Iteration die Invarianten erhält.
Dann wüssten wir, dass die Invarianten auch vor der zweiten Iteration gelten.
Dann könnten wir argumentieren, dass auch die zweite Iteration die Invariante
erhält, usw.
Diese Art der Argumentation ist nicht nur umständlich, sie führt auch nicht zum
Ziel, da die for-Schleife  Iterationen macht und 
beliebig groß sein kann. Wir dürften also niemals mit unserer Argumentationskette
aufhören.
Wir verallgemeiner daher unser Argument und zeigen, dass jede Iteration
die Invarianten erhält.
Betrachten wir also irgendeine Iteration und das zugehörige  und gehen davon
aus, dass die Invarianten vor der Iteration gelten.
Was passiert jetzt in einer Iteration der for-Schleife?
Zuerst tauschen wir die Wurzel  mit , dem letzten Element des Heaps
(nach Invariante 1 ist  ein Heap, wenn die Iteration beginnt).
Nun ist  kein Heap mehr.
Da wir aber heapify-down(A,i,0) aufrufen, wird die Heap-Eigenschaft
auf  wiederhergestellt.
Und da  vor der nächsten Iteration um eins verringert wird, gilt Invariante 1
daher vor der nächsten Iteration.

Invariante 1 sagt uns ebenfalls, dass (vor dem Tausch ) der größte Schlüssel
im vorderen Teil  von  ist.
Wegen Invariante 2 muss  der -t größte Schlüssel aus der Eingabe sein.
Da wir  gerade an Position  tauschen, haben wir nun in 
die  größten Schlüssel der Eingabe.
Vor der nächsten Iteration sinkt  um 1, so dass die Invariante 2 auch vor der
nächsten Iteration gelten wird.
Nun können wir aus den Invarianten die Korrektheit von HeapSort folgern:
Wenn der Kopf der for-Schleife in Zeile das letzte mal durchlaufen wird ist  und die Schleife bricht ab.
Unsere Invariante 2 sagt uns, dass dann  die  größten
Schlüssel in sortierter Reihenfolge enthält.
Der -t größte (d.h. der kleinste) Schlüssel muss demnach in  stehen
und  ist sortiert.

Bottom-Up Heapsort

Wenn wir  mit  tauschen, wird ein Blatt zur neuen Wurzel
des Heaps.
Die Wahrscheinlichkeit, dass  wieder bis auf die Blattebene versickert
werden muss ist also groß.
Die Idee von Bottom-Up-Heapsort ist nun, in der Methode heapify-down
auf den vorzeitigen Stopp in den Zeilen und  zu verzichten.
Wir testen also nicht mehr, ob  kleiner ist als die Schlüssel seiner Kinder,
sondern tauschen  in jedem Fall mit dem Kind, das den größeren Schlüssel besitzt.
Damit wird  in jedem Fall - ggf. unter Verletzung der Heapeigenschaft
- bis auf die Blattebene versickert und muss anschließend im Heap wieder nach
oben geschoben werden. Wir sparen aber in der Methode heapify-down
einen Vergleich.
Um  von der Blattebene wieder nach oben zu schieben tauschen wir 
mit seinem Elter, bis der Elter nicht mehr größer als  ist.
Wenn der Heap viele Ebenen hat und  tatsächlich auf den unteren Ebene
einsortiert wird, sparen wir dadurch insgesamt Vergleiche ein.
Listing  zeigt eine Implementierung dieses Verfahrens.

[caption=Bottom-Up-Heapsort, float, label=lst:prioqueues:bottom-up-heap-sort]
function heapify-up(A, n, i):
    while i != 0 and A[i] > A[parent(i)]: 
      swap(A[i], A[parent(i)])
      i = parent(i)

function heapify-to-bottom(A, n, i):
    while left(i) < n:
      j = left(i)

      if right(i) < n and A[right(i)] > A[j]:
        j = right(i)

      swap(A[i], A[j])
      i = j

    heapify-up(A, n, i)

function heapify-down(A,n,i):
    # s. Listing 
    ...

function heapsort-bottom-up(A):
    # make_heap(A)
    for i=(A.laenge div 2)-1,...,0:
        heapify-down(A, A.laenge, i)

    for i=A.laenge-1,...,1:
        # extract-max():
        swap(A[0], A[i])
        heapify-to-bottom(A, i, 0)

Wir können in der Methode heapify-up() einen weiteren Vergleich mit
einem Programiertrick einsparen.
Dazu speichern wir in  einen Wächter (auf Englisch: Sentinel),
d.h. einen Schlüssel, der sicher größer ist als alle Schlüssel, die im Heap
gespeichert werden.
Den eigentlichen Heap speichern wir in .
Nun müssen wir im Kopf der while-Schleife in Zeile nicht mehr testen, ob  ist: Die Bedingung A[i] > A[parent(i)] spätestens
dann nicht mehr erfüllt, wenn  ist, denn dann ist parent(i) der Wächter.
Da wir nun den Heap ab 1 indizieren, müssen wir die Implementierungen von
left, right und parent anpassen.
Wir finden nun das linke Kind von  an Position , das rechte Kind an
Position  und den Elternknoten von  an Position .
Die Implementierungen von left und parent benötigen in der
Indizierung ab 1 also ebenfalls weniger arithmetische Operationen.


fig-bottom-up-heapsort-example

Bottom-up Heapsort. Oben links: Der Beispielheap aus Abbildung .
Schlüssel 94 ist bereits einsortiert worden.
Oben rechts: Die Wurzel wird ans Ende des Heaps getauscht. Schlüssel 18
wird die neue Wurzel und muss versickert werden, um die Heapeigenschaft wiederherzustellen.
Unten links: Bottom-up Heapsort versickert den Schlüssel 18 bis in die Blattebene, indem
es 18 stets mit dem größeren Kind vertauscht.
Dabei kontrollieren wir nicht, ob die Vertauschung die Heapeigenschaft repariert, sondern tauschen
bis Schlüssel 18 ein Blatt ist.
Tatsächlich ist nun die Heapeigenschaft zwischen 16 und 18 verletzt.
Unten rechts: Um die Heapeigenschaft wiederherzustellen wird 18 nun solange nach oben
geschoben, bis der Schlüssel des Elternknotens von 18 größer ist 18. Nach dem Tausch von 18 und 16
ist der Heap repariert.
Abbildung  zeigt die gleiche Iteration mit der herkömmlichen Heapsort-Implementierung.


Union-Find: Eine Datenstruktur für Partitionierungen


In diesem Kapitel beschäftigen wir uns damit, wie wir Zerlegungen einer endlichen Menge effizient im Speicher verwalten können.


Wir nennen zwei Mengen  und  disjunkt, wenn sie keine gemeinsamen Elemente besitzen, also  gilt.
Eine Partitionierung einer Menge  ist eine Zerlegung  in  paarweise disjunkte Teilmengen, die wir Partitionen nennen.
Hier bedeutet paarweise, dass für alle  gilt, dass  und  disjunkt sind sofern  ist.


Da eine Partitionierung die Menge  in disjunkte Mengen zerlegt und die Vereinigung aller Partitionen  genau die Menge  ergibt, existiert für jedes  genau eine Partition aus , die  enthält.
Wir können diese Partition in der Folge also die (eindeutige) Partition von  nennen.

Mit dieser Definition können wir nun präzisieren, was wir möchten: Wir suchen eine Datenstruktur, die eine Partitionierung der endlichen Menge  mit  in  Partitionen  verwalten kann.
Dabei soll die Partitionierung dynamisch sein, sich also im Laufe der Zeit verändern können.
Genauer bedeutet das, dass die Datenstruktur in der Lage sein soll, vorhandene Partitionen zu vereinigen.
Das Aufteilen einmal vereinigter Partitionen lassen wir hingegen nicht zu.
Außerdem soll die Datenstruktur für jede Partition  ein beliebiges Element  als Repräsentanten von  vorhalten.
Unsere Datenstruktur nennen wir Union-Find-Datenstruktur.
Sie soll folgende Operationen unterstützen:



Zur Verdeutlichung der Anforderungen an die Datenstruktur betrachten wir ein Beispiel.

  Wir verwalten Partitionen der Menge  und beobachten in Abbildung , wie sich das Mengensystem unter den Datenstrukturoperationen verändert.
  Wir beobachten, dass wir der union-Operation auch Elemente übergeben können, die keine Repräsentanten sind.
  Außerdem ist immer find(x)  find(y), wenn  und  in der gleichen Partition liegen.


      
  
  Die Tabelle zeigt eine Folge von Operationen auf einer Union-Find-Datenstruktur über der Menge .
  Die mittlere Spalte gibt den Zustand der Datenstruktur an, nachdem die Operation in der linken Spalte ausgeführt wurde. Partitionen repräsentieren wir hier durch abgerundete Rechtecke, die jeweils den Repräsentanten der Partition (vor dem Doppelpunkt) und die Elemente der Partition (nach dem Doppelpunkt) zeigen.
  Die Ausgabe der Datenstruktur ist in der letzten Spalte aufgeführt.

Eine Implementierung von Union-Find mit schnellen Find-Operationen

Wir überlegen uns nun, wie wir eine Datenstruktur realisieren können, die den obigen Anforderungen genügt.
Zunächst müssen wir festlegen, wie wir die Partitionen im Speicher repräsentieren.
Wir stellen fest, dass wir eine Partitionierung von  in einem Array  mit  Einträgen speichern können, indem wir an Position  des Arrays abspeichern, in welcher Partition das Element  liegt - genauer speichern wir in  den Repräsentanten der Partition von .
Diese Eigenschaft soll von allen Operationen der Datenstruktur als Invariante 1 erhalten werden.

Zum Durchführen der union-Operation müssen wir daher jedes Element in  darauf testen, ob es zu einer der beiden vereinigten Mengen gehört und ggf. seinen Eintrag in  anpassen.
Dies würde Zeit  benötigen.
Wir möchten aber, dass die Operation nur diejenigen Elemente betrachten muss, die auch zu einer der beiden vereinigten Mengen gehören.
Dazu verwenden wir ein Array , das ebenfalls  Einträge besitzt.
An Position  in  legen wir eine doppelt verkettete Liste an, die alle Elemente derjenigen Partition enthält, die von  repräsentiert wird.
Repräsentiert  keine Partition, so speichern wir in  einen null-Zeiger, im Folgenden dargestellt durch .
Diese Invariante 2 ist einfach aufrecht zu erhalten: Bei der Vereinigung zweier Partitionen hängen wir die zugehörigen Listen aneinander.
Dazu benötigen wir die splice-Operation einer doppelt verketteten Liste, die zwei Listen in konstanter
Zeit aneinander hängen kann.
Zudem können wir nun den Repräsentanten einer Partition ändern, indem wir über die zugehörige Liste iterieren und müssen nicht mehr zwingend alle Einträge in  untersuchen.







Beispielsweise sieht die Datenstruktur am Ende des Beispiel aus Abbildung  also folgendermaßen aus:



Zu Beginn bildet jedes  seine eigene Partition.
Damit die Initialiserung unsere Invarianten herstellt, müssen wir also für alle  gerade  setzen und in  eine Liste anlegen, die nur das Element  enthält.
[caption=Initialisierung der UnionFind-Datenstruktur]
class ArrayBasedUnionFind(n):
  R = Array(n)
  L = Array(n)
  for each x in 0,...,n-1:
    R[x] = x
    L[x] = DLinkedList()
    L[x].append(x)

Die Operation find(x) soll den Repräsentanten der Partition zurückliefern, die  enthält.
Diese Information speichern wir aber gerade in unserem Array .
Wir müssen also nur  zurückgeben.
Die Operation ändert die Datenstruktur nicht, so dass die Invarianten erhalten bleiben.
[caption=Die Operation find der UnionFind-Datenstruktur]
class ArrayBasedUnionFind(n):
    ...
    function find(x):
        return R[x]
Um die Partitionen  und  von  bzw.  zu vereinigen wählen wir einen Repräsentanten für die neue vereinigte Partition .
Anschließend weisen wir allen Elementen in  diesen neuen Repräsentanten zu.
Dies stellt Invariante 1 wieder her.
Die Elemente in  und  können wir zwar dank der Listen  und  einfach finden, das Aktualisieren erfordert aber konstanten Aufwand für jedes Element in , da wir für jedes  den Eintrag  ändern müssen.
Geschickter ist es, etwa den Repräsentanten von  als Repräsentanten von  zu wählen, so dass wir nur für  die Einträge in  aktualisieren müssen.
Noch besser fahren wir, wenn wir den Repräsentanten der größeren der beiden Partitionen als neuen Repräsentanten wählen (sind beide Mengen gleich groß, spielt es keine Rolle, ob wir den Repräsentanten von  oder von  verwenden).
Anschließend müssen wir die Listen von  und  aneinander hängen, damit weiterhin Invariante 2 gilt.
[caption=Die Operation union der UnionFind-Datenstruktur]
class ArrayBasedUnionFind(n):
    ...
    function union(x,y):
        # x und y sind nicht notwendigerweise die Repraesentanten
        # ihrer jeweiligen Partition
        x = find(x)
        y = find(y)

        # liegen x und y bereits in der gleichen Partition,
        # bleibt nichts zu tun
        if x == y:
            return

        # o.B.d.A. ist die Partition von y nicht die Kleinere,
        # sonst vertauschen wir x und y
        if L[x].size() > L[y].size():
            swap(x,y)

        # setze Repraesentaten aller Elemente in Partition
        # von x auf y (wir haben anfangs sichergestellt, dass y
        # der Repraesentant der Partition von y ist)
        for each z in L[x]:
            R[z] = y

        # haenge die Liste von x an das Ende der Liste von y;
        # x ist nun kein Repraesentant mehr
        L[y].splice(L[x])
        free L[x]

Laufzeitanalyse der Implementierung

Nachdem wir unsere Datenstruktur spezifiziert und implementiert haben, möchten wir nun die Laufzeit der Operationen init, union und find analysieren.

Die for-Schleife in der Methode init macht  Iterationen.
Jede Iteration verursacht konstanten Aufwand, so dass die Methode insgesamt in Zeit  arbeitet.
Die Operation find besitzt konstante Laufzeit, da wir in konstanter Zeit auf ein Element eines Arrays zugreifen können.

Analysieren wir nun die Methode union(x,y).
Die beiden Aufrufe von find in den Zeilen 4 und 5 benötigen nach unseren Überlegungen jeweils Zeit .
Auch den Vergleich in Zeile 9 und 10 können wir in konstanter Zeit ausführen.
Für den Größenvergleich in Zeile 14 gehen wir davon aus, dass wir die Länge der Listen  und  in konstanter Zeit bestimmen können.
Erfüllt unsere Listendatenstruktur diese Anforderung nicht, können wir die Länge der Listen in einem zusätzlichen Array mitführen.
Damit sind auch die Zeilen 14 und 15 in konstanter Zeit durchführbar.
Die for-Schleife macht  viele Iterationen (wir bezeichnen hier mit  die Länge einer Liste ). Im schlimmsten Fall ist , so dass die Schleife  viele Iterationen mit jeweils konstantem Aufwand macht.
Das abschließende Verketten der Listen in Zeile 25 ist in konstanter Zeit möglich.
Auch Zeile 26 erfordert lediglich konstanten Aufwand.
Insgesamt besitzt die Methode also im schlimmsten Fall eine Laufzeit von .

Eine weitere amortisierte Analyse
Da jede (nicht-triviale) union-Operation die Anzahl der Partitionen um eins verringert und wir mit  Partitionen starten, können wir  nicht-triviale union-Operationen durchführen bevor die Datenstruktur nur noch eine Partition enthält.
Nach unserer vorherigen Analyse haben diese  Operationen gemeinsam eine Worst-Case-Laufzeit von .
Wir stellen allerdings fest, dass nicht alle union-Operationen teuer sein können:
Schließlich müssen wir in den kritischen Zeilen 20 und 21 nur dann  Repräsentanten ändern, wenn zuvor schon mindestens  union-Operationen ausgeführt worden sind.
Wir vertiefen diese Idee und analysieren die worst-case Laufzeit der gesamten Sequenz von  union-Operationen präziser.
Dazu benötigen wir erneut eine amortisierte Analyse.

  Jede beliebige Folge von  union- und  find-Operationen kann in Zeit  ausgeführt werden.

  Da wir die  find-Operationen in Zeit  durchführen können, bleibt nur die Laufzeit der  union-Operationen zu analysieren.
  Seien dazu  und  die Partitionen, die von der -ten union-Operation vereinigt werden.
  O.B.d.A. können wir davon ausgehen, dass  jeweils die kleine Partition ist, also  für alle  gilt.
  Die Gesamtlaufzeit der  union-Operationen beträgt dann nach unseren vorherigen Überlegungen
  
  Wir betrachten nun ein beliebiges, aber festes  und behaupten, dass  in höchstens  vielen union-Aufrufen in der kleineren Partition liegt, d.h. dass  gilt:
  Wenn  das erste Mal in einem union-Aufruf in der kleineren Partition  liegt, so ist .
  Da  die kleinere Partition ist, muss die Vereinigung mindestens zwei Elemente enthalten.
  Liegt  also das nächste mal in der kleineren Partition eines union-Aufrufs, besitzt seine Partition mindestens zwei Elemente, und damit die Vereinigung mindestens vier und so weiter.
  Nachdem also das Element  in  union-Aufrufen in der kleineren Partition enthalten war, muss es in einer Partition mit mindestens  Elementen liegen.
  Insgesamt gibt es aber nur  Elemente, so dass  und damit auch  sein muss.
  Also ist  für alle  und aus eq:uf-amortised-bookkeeping folgt, dass die Gesamtlaufzeit aller  union-Operationen
  *
  beträgt.
Wir bemerken abschließend, dass die obige Analyse eine Worst-Case-Analyse für die Gesamtlaufzeit der Sequenz von Operationen ist.
Wir definieren die amortisierte Laufzeit einer union-Operation als deren durchschnittliche Laufzeit  in der Sequenz.
Damit ergibt sich folgende Laufzeitübersicht für die Operationen der UnionFind-Datenstruktur.




Eine Implementierung von Union-Find mit schnellen Union-Operationen












Nachdem wir eine Implementierung der Union-Find-Schnittstelle mit einer schnellen find-Methode gesehen haben
möchten wir nun die union-Methode beschleunigen.
Dafür wird nun find nicht mehr in konstanter Worst-Case-Laufzeit arbeiten.

Anstatt die Partionen in einer Kombination von Arrays und Listen zu speichern legen wir nun
jede Partition in einem Baum ab.
Dieser Baum ist im allgemeinen weder binär, noch ein Suchbaum.
Genauer legen wir initial für jede Partition ,  einen Baum an, der nur aus einer Wurzel
besteht und dort gerade Element  speichert.
Für union(x,y) hängen wir den Baum von  direkt unter die Wurzel des Baumes von  ein
und wählen als Repräsentanten von  die Wurzel des Baumes von :

 Sei  der Baum von  mit Wurzel/Repräsentant 
 Sei  der Baum von  mit Wurzel/Repräsentant 
 Dann setze den Elter von  auf 
Um find(x) zu implementieren steigen wir von  aus im Baum auf, bis wir die Wurzel gefunden haben
und geben das dort gespeicherte Element zurück:

 Solange  nicht die Wurzel ist:
 Setze  auf den Elter von .
 Gib  zurück.
Abbildung  zeigt ein Beispiel.

  fig-uf-trees
 
 Einige Beispieloperationen auf einer Union-Find-Datenstruktur mit Baumdatenstruktur und Strategie Vereinigung nach Größe.


In dieser Implementierung erhalten wir eine Worst-Case-Laufzeit von 
für die Initialisierung.
Die Laufzeit von find(x) ist proportional zur Höhe des Baumes von .
Da wir höchstens  Elemente im Baum von  speichern ist seine Höhe höchstens
.
Abbildung  zeigt, dass diese Höhe im schlimmsten Fall auch  sein kann,
so dass find(x) eine Worst-Case-Laufzeit von  besitzt.

 
  Beispiel: Ohne die Strategie Vereinigung nach Größe
 kann es passieren, dass die Union-Find-Bäume zu Listen degenerieren.
 Das Beispiel in der Abbildung lässt sich verallgemeinern, indem wir  Elemente
 betrachten und in Schritt  die Operation  ausführen.
 Wird stets der größere Baum in den kleineren Baum eingehangen, erhalten wir am Ende
 wie im Beispiel einen Baum mit Tiefe .
Mit einem einfachen Trick können wir die Laufzeit von find aber auf 
reduzieren:
Immer wenn wir in union(x,y) zwei Bäume ineinander hängen, hängen wir den
kleineren Baum an die Wurzel des größeren.
Formaler: Wenn  mindestens so viele Knoten enthält wie , hängen wir  an
die Wurzel von . Anderenfalls hängen wir  an die Wurzel von .
Diese Strategie heißt Vereinigung nach Größe. Sei  ein beliebiger Baum in unserer Union-Find-Datenstruktur und sei  die
 Höhe von .


 Dann gilt zu jedem Zeitpunkt (also nach einer beliebigen Sequenz von union
 und find-Operationen), dass
 
 .
 


 Wir zeigen zunächst als Hilfsbehauptung, dass zu jedem Zeitpunkt
 
  
 
 gilt, wobei  die Anzahl der Knoten in  bezeichne. Die find-Operation
 ändert weder  noch .
 Das gleiche gilt für triviale(Wir nennen eine Operation union(x,y) trivial,
 wenn  und  in der gleichen Partition liegen.) union-Operationen.
 Wir müssen daher nur nicht-triviale union-Operationen betrachten und zeigen
 die Hilfsbehauptung mittels Induktion über die Anzahl  der durchgeführten
 nicht-trivialen union-Operationen.

 Induktionsanfang. Für den Induktionsanfang betrachten wir  und
 einen beliebigen Baum .
 Nach  nicht-trivialen union-Operationen ist  noch in dem Zustand in dem
  war, nachdem init(n) ausgeführt wurde.
Daher enthält  genau einen Knoten und besitzt Höhe .
Wir haben ; die Behauptung stimmt für .

Induktionsschritt.
Wähle ein beliebiges .
Wir setzen voraus, dass die Behauptung nach  nicht-trivialen union-Operationen
galt und betrachten nun die -te nicht-triviale union-Operation union(x,y).
Diese Operation vereinigt zwei Bäume, die wir  und  nennen.
Wir wählen die Bezeichnung der Bäume so, dass  ist, d.h. so, dass 
der größere Baum ist und also  an die Wurzel von  eingehangen wird.
Wir nennen den resultierenden Baum .
Die Bäume  und  sind die einzigen Bäume, die durch die union-Operation
berührt werden, so dass wir unsere Betrachtungen auf diese beiden Bäume einschränken
können.
Wir unterscheiden nun zwei Fälle.

1. Fall. Ist  so ist die Höhe von  genau
, s. Abbildung :

*
und die Behauptung ist gezeigt.

2. Fall. Anderenfalls ist .
Damit ist die Höhe von  weiterhin genau  (s. Abbildung ),
so dass wir die Höhe des Baumes nicht verändert haben, aber neue Knoten dazu bekommen.
In Formeln haben wir
*
Damit haben wir die Hilfsbehauptung gezeigt.

Jeder Baum  enthält aber höchstens  Knoten.
Es gilt also für jeden Baum :
*

[T]0.45
    fig-union-find-proof-1
    
Besitzt  mindestens die gleiche Höhe wie  so dominiert  nach dem
Einhängen unter der Wurzel von  die Höhe des neuen Baumes .
Weil  erst auf Ebene 1 beginnt, ist die Höhe von  nun .
[T]0.45
    fig-union-find-proof-2
    
Besitzt  eine echt geringere Höhe als  besitzt  die gleiche Höhe wie .
Beweis von Lemma : Die Bäume  und  werden zu einem neuen
Baum  vereinigt, indem  unter der Wurzel von  eingehangen wird.
Die Höhe von  ist die Länge eines längsten Pfades von der Wurzel  von 
(das ist auch gleichzeitig die Wurzel von ) zu einem Blatt.
Im linken Bild verlaufen die längsten Wurzel-Blatt-Pfade von  zu einem Blatt von  und haben daher
Länge . Im rechten Bild gibt es einen längsten Wurzel-Blatt-Pfad von  zu einem Blatt in 
und dieser Pfad hat Länge .















Mit der Strategie Vereinigung nach Größe können wir also find und union
mit einer Worst-Case-Laufzeit von  implementieren.
Die gleiche Garantie erhalten wir, wenn wir die Strategie Vereinigung nach Höhe
verwenden. Hier wird der Baum mit der geringeren Höhe an den Baum mit der größeren
Höhe eingehängt.

Für die Implementierung verwenden wir eine einfache Knotenklasse, die nur einen
Schlüssel und einen Zeiger auf den Elternknoten speichert (Zeiger auf die Kinder brauchen
wir nicht, da wir niemals abwärts im Baum gehen).
[caption=Knotenklasse für UnionFind mit Bäumen]
class UFNode(key):
    key = key
    parent = null
Wir speichern nun für jedes Element  einen Knoten in einem
Array.
Dieses Array dient dazu, dass wir den Knoten eines Elements in konstanter Zeit
in der Datenstruktur finden können.
Zusätzlich speichern wir in einem zweiten Array size an Position 
die Größe der Partition, die von  repräsentiert wird.
Repräsentiert  keine Partition kann size[r] eine beliebige Zahl sein.
[caption=Knotenklasse für UnionFind mit Bäumen]
class TreeBasedUnionFind(n):
    nodes = Array(n)
    size = Array(n)
    for i=0,...,n-1:
        nodes[i] = UFNode(i)
        size[i] = 1
Mit Hilfe dieser beiden Arrays erhalten wir die Implementierungen für
union und find in Listing .
[caption=UnionFind mit Bäumen und der Strategie Vereinigung nach Größe, float, label=lst:unionfind:find-and-union]
class TreeBasedUnionFind(n):
    ...
    function find(x):
        v = nodes[x]
        # solange v nicht die Wurzel ist
        while v.parent != null:
            v = v.parent

        return v.key

    function union(x,y):
        # Repraesentanten für x und y
        x = find(x)
        y = find(y)

        # Stopp falls x und y in gleicher Partition
        if x == y:
            return

        # O.b.d.A. ist die Partition von y nicht die
        # kleinere Partition
        if size[y] < size[x]:
            swap(x, y)

        # haenge Baum von x unter Baum von y ein
        nodes[x].parent = nodes[y]
        size[y] += size[x]

        return y



Ausblick: Union-Find mit Pfadverkürzung

Es stellt sich (mittels einer komplizierten amortisierten Analyse) heraus, dass
wir die Laufzeit von union und find verbessern können, indem wir
regelmäßig eine Aufräumoperation durchführen, die Pfadverkürzung genannt
wird.
Dabei verfolgen wir folgende Idee:
Die Laufzeit von find(x) wird dadurch dominiert, dass die Methode von 
bis zur Wurzel des Baumes  von  aufsteigen muss.
Besitzt  eine große Höhe, ist find(x) also langsam.
Wir möchten nun dafür sorgen, dass zukünftige Aufrufe von find(x) nicht
noch einmal den langen Pfad aufsteigen müssen.
Daher modifizieren wir die find-Operation so, dass sie zum Absschluss alle
Knoten, die sie auf dem Pfad von  zur Wurzel findet, direkt an die Wurzel hängt.
Abbildung  zeigt ein Beispiel.
Von dieser Verbesserung profitieren auch zukünftige union-Operationen, da
union(x,y) ebenfalls find(x) aufruft.

Man kann zeigen, dass beliebige  union und find Operationen
mit Pfadverkürzung in Zeit  durchgeführt werden können, wenn wir union
mit einer geeigneten Vereinigungsstrategie versehen.
Dabei ist  die sog. inverse Ackermannfunktion, die nahezu konstant ist
(für alle praktischen Zwecke ist ).
Die geeignete Vereinigungsstrategie ist hier Vereinigung nach Rang, die
der Vereinigung nach Höhe ähnelt, aber einfacher durchzuführen ist.

Wir haben damit folgende Laufzeiten für Union-Find:




  
Effekt der Pfadverkürzung nach einem Aufruf von find(9): Die Knoten auf dem Pfad von 9 zur Wurzel
sind nun direkte Kinder der Wurzel.

Amortisierte Analyse

Nachdem wir nun einige beispielhafte amortisierte Analysen gesehen haben fassen wir in diesem
Abschnitt unsere Vorgehensweise etwas abstrakter zusammen.

Zunächst betrachten wir aber erneut ein Beispiel:
Ein Binärzähler speichert eine Binärzahl mit  Stellen in einem (Bit-)Array der Länge .
Dabei speichern wir an Position  die Stelle, die zu  gehört;
in  speichern wir das also am wenigsten signifikante Bit und die vom Zähler gespeicherte
Zahl ist .
Der Binärzähler wird mit der Zahl 0, d.h. , , initialisiert und verfügt über eine
Methode increment(), die die gespeicherte Binärzahl um 1 erhöht:


 
 Dazu wird zunächst das Bit an Position 0 geflippt: Ist es 0, setzen wir es auf 1; ist es 1, setzen wir es auf 0. Flippen wir von 0 auf 1 ist nichts weiter zu tun. Wenn wir von 1 auf 0 flippen haben wir einen Übertrag generiert und müssen anschließend das Bit auf Position 1 flippen, usw.
Allgemein suchen wir also das rechteste Bit , das auf 0 gesetzt ist und setzen es auf 1.
Alle Bits , die sich rechts davon von  befinden stehen auf 1 und werden auf 0 gesetzt.
Wird increment aufgerufen und alle Bits stehen auf 1 generieren wir einen Überlauf (d.h., alle Bits werden auf 0 gesetzt).
Listing  zeigt eine Implementierung.

Mit welcher Laufzeit müssen wir rechnen, wenn wir den Binärzähler verwenden um von 0 bis  zählen, d.h.  mal increment aufrufen?
Sicherlich beträgt die Laufzeit der Methode increment() , da im schlimmsten Fall alle  Bits geflippt werden müssen (für  passiert das z.B. beim Übergang von 01111 auf 10000).
Als erste Schätzung rechnen wir daher mit einer Laufzeit von .
Wir möchten nun nachweisen, dass die tatsächliche Laufzeit sehr viel geringer ist.
Dazu betrachten wir verschiedene Sichtweisen auf die (grundsätzlich) gleiche Analyse.

[caption=Binärzähler, label=lst:unionfind:amortizedana:bincounter]
class BinaryCounter(k):
    A = Array(k)
    for l=0,...,k-1:
        A[l] = 0

    function increment():
        l=0
        # Finde rechtestes 0-bit,
        # flippe alle 1en auf dem Weg
        while l < k and A[l] == 1:
            A[l] = 0
            l++

        # Falls kein Überlauf, setze
        # rechtestes 0-bit auf 1
        if l < k:
            A[l] = 1

Die Aggregatmethode
Die Aggregatmethode haben wir bereits bei der Analyse von Arraylist.append() und bei Laufzeitanalyse von union der baumbasiertem UnionFind-Datenstruktur gesehen.
Hier ist die Idee, dass wir anstelle einer einzelnen Operation eine Sequenz von Operationen betrachten
und die Worst-Case-Laufzeit der Sequenz analysieren.
Anschließend definieren wir die amortisierte Laufzeit einer einzelnen Operation, indem wir die Worst-Case-Laufzeit der Sequenz durch die Anzahl in ihr enthaltener Operationen teilen.

Im Beispiel des Binärzählers sollten wir also die Worst-Case-Laufzeit von  aufeinanderfolgenden increment-Aufrufen bestimmen.
Dazu betrachten wir zunächst einen Beispieldurchlauf mit .


 
Wir erkennen als Muster, dass das 0-te Bit 15 mal geflippt wird, nämlich in jedem Aufruf von increment.
Das 1-te Bit wird insgesamt 7 mal geflippt, und zwar in jedem zweiten increment-Aufruf.
Das 2-te Bit wird insgesamt 3 mal geflippt, und zwar in jedem vierten increment-Aufruf.
Das 3-te Bit wird lediglich 1 mal - in jedem achten increment-Aufruf - geflippt.
Allgemein wird das -te Bit  mal geflippt, wenn wir von  bis  zählen; nämlich in jedem -ten Aufruf von increment.
Insgesamt gibt es also
*
Bit-Flips und die Worst-Case-Laufzeit von  aufeinanderfolgenden increment-Aufrufen beträgt .
Damit ist die amortisierte Laufzeit eines einzelnen increment()-Aufrufes .

Die Buchhalter-/Accountingmethode

Die Buchhaltermethode nimmt eine ökonomische Sichtweise ein: Hier müssen Operationen dafür bezahlen,
vom Prozessor ausgeführt zu werden.
Wieviel eine Operation bezahlt ist dabei proportional zur Anzahl der in ihr enthaltenen Elementaroperationen.
Dabei stellen wir uns vor, dass wir ein gewisses Budget haben, das wir verwenden dürfen, um Elementaroperationen
zu bezahlen.
Denken wir etwa an die Methode append unserer Arrayliste zurück.
Im günstigen Fall besitzt unser Array noch Kapazität und es muss beim Aufruf von append kein Verdopplungsschritt mit enlarge ausgeführt werden.
Dann besitzt append eine konstante Laufzeit und muss daher auch nur eine konstante Menge an Geld für seine Ausführung bezahlen.
Sagen wir, append bezahlt in diesem Fall einen Euro.
Anderenfalls muss das Array verdoppelt werden.
Dann führt die append-Operation  Elementaroperationen durch und muss daher auch  Euro an den Prozessor bezahlen.
Sagen wir der Einfachheit halber, dass append in diesem Fall  Euro für seine Ausführung bezahlt (schließlich
müssen wir ein Array der Größe  erzeugen und  Elemente kopieren).

Die Idee der Buchhaltermethode ist nun, dass die günstigen append-Operationen die Kosten für den
teure Verdopplungsschritt - als eine Investition in die Zukunft - auf einem Konto ansparen können.
Der teure Verdopplungsschritt kann dann seine Kosten mit dem angesparten Geld decken anstatt
mehr Geld aus dem Budget zu entnehmen.
Dazu bezahlt nun jede günstige append-Operation nicht mehr einen Euro, sondern sieben Euro; diese
sieben Euro werden aus dem Budget entnommen.
Ein Euro davon geht an den Prozessor, sechs Euro werden auf einem Konto angespart.

Was passiert nun, wenn ein teurer Verdopplungsschritt ausgeführt wird, um die Kapazität des Arrays
von  auf  zu erhöhen?
Die grobe Idee ist hier, dass seit dem letzten Verdopplungsschritt (oder der Initialisierung der Liste)
 Elemente eingefügt worden sein müssen.
Es wurde also mindestens  mal append ohne Verdopplungsschritt ausgeführt.
Damit haben wir mindestens  Euro auf dem Konto und können damit den Verdopplungsschritt
(bzw. dessen Elementaroperationen) bezahlen.
Da jede append-Operation (egal ob mit oder ohne Verdopplung) höchstens 7 Euro aus dem Budget entnimmt,
können wir mit einem Budget von  Euro immer  append-Operationen ausführen und wissen,
dass die Gesamtlaufzeit von  append-Operationen in  sein muss.

Allgemein kann sich jede Operation kann also entscheiden:

ihre Ausführung aus einem globalen Budget zu bezahlen,
Geld aus dem globalen Budget auf ein Konto einzuzahlen oder
Geld von einem (initial leeren) Konto abzuheben. Im Allgemeinen können wir dabei mehrere verschiedene Konten haben.
Solange die Konten nicht ins Minus rutschen, wissen wir also, dass das globale Budget ausreichend ist, um alle Elementaroperationen zu bezahlen, die vom Algorithmus durchgeführt werden.
Die Laufzeit des Algorithmus ist daher proportional zum globalen Budget.
Als amortisierte Laufzeit einer Operation definieren wir hier den Betrag, den sie aus dem globalen Budget entnimmt.

Analysieren wir zum Abschluss den Binärzähler mit der Buchhaltermethode.
Zunächst eröffnen wir  Konten - eines für jedes Bit.
Die Konten sind initial leer.
Wird nun das -te Bit von 0 auf 1 gesetzt, entnehmen wir 2 Euro aus dem Budget.
Davon verwenden wir einen Euro, um das -te Bit von 0 auf 1 zu flippen.
Den anderen Euro zahlen wir auf das Konto von Bit  ein.

Wenn das -te Bit von 1 auf 0 gesetzt wird, muss es zuvor von 0 auf 1 gesetzt worden sein, denn initial sind alle Bits 0.
Folglich enthält das Konto von Bit  mindestens einen Euro und wir können den Wechsel von 1 auf 0 bezahlen.
Nun müssen wir nur noch beobachten, dass jeder Aufruf von increment() stoppt, sobald ein Bit von 0 auf 1 gesetzt wird.
Jeder Aufruf entnimmt also höchstens 2 Euro aus dem Budget.
Da wir insgesamt  Aufrufe von increment() machen genügt also ein Budget von  Euro und die
Gesamtlaufzeit von  increment-Aufrufen beträgt .

Graphalgorithmen








Einleitung: Was sind Graphen?

Im Alltag begegnen uns immer wieder Informationen in Form von binären Relationen (paarweisen Beziehungen): Kennt Arne Emilia? Sind Antonia und Mattis befreundet? Gibt es zwischen Köln und Gerolstein eine direkte Zugverbindung? Kann man von Hamburg aus Berlin in weniger als drei Stunden erreichen? Das mathematische Konzept, das solche Relationen modelliert heißt
Graph.(Unglücklicherweise wird der Begriff Graph auch für Funktionsgraphen verwendet. Hier sind kombinatorische Graphen gemeint.)
Ein Graph besteht aus Knoten und Kanten.
Knoten stellen die Menschen oder Objekte dar, deren Beziehungen wir modellieren möchten.
Kanten bestehen immer aus zwei Knoten.
Enthält ein Graph eine Kante  aus zwei Knoten  und  (wir sagen auch zwischen zwei Knoten  und ), dann drückt das aus, dass  und  miteinander in Beziehung stehen.
Graphen können symmetrische Beziehungen ( und  arbeiten im gleichen Büro) und potentiell asymmetrische Beziehungen ( findet  symphatisch) ausdrücken.
Im ersten Fall verwenden wir eine ungerichtete Kante .
Sie drückt aus, dass sowohl  mit  als auch  mit  in Beziehung stehen. Abbildung  zeigt ein Beispiel für einen ungerichteten Graphen.

fig-graph-1
 
 Ein Beispiel für einen ungerichteten Graphen. In diesem Beispiel stellen wir die Beziehung Arbeiten im gleichen Bürofür ein Beispiel mit elf Personen dar. In der Darstellung als Graph sind die Personen durch den ersten Buchstaben ihres Vornamens abgekürzt. Wir sehen zwei Darstellungen desselben Sachverhalts und bemerken, dass derselbe Graph auf verschiedene Weise gezeichnet werden kann und doch dieselbe Information ausdrückt. Nicht immer geben wir den Knoten explizit Namen; ein Beispiel für einen (gerichteten) Graphen mit unbenannten Knoten seheh wir in Abbildung .

Eine gerichtete Kante  von  nach  modelliert hingegen, dass  mit  in Beziehung steht;  aber nicht notwendigerweise mit .
Wir nennen  den Start und  das Ziel von  und sagen, dass  eine ausgehende Kante von  und eine eingehende Kante von  ist.
Wir können eine ungerichtete Kante  durch zwei gerichtete Kanten  und 
darstellen.

 fig-graph-2
 
 Ein Beispiel für einen gerichteten Graphen. In einer Zeichnung benötigen wir nicht unbedingt Knotennamen, wenn wir die Knoten nicht referenzieren wollen. Wenn wir einen Graphen durch Angabe der Kanten definieren, benötigen wir natürlich Knotennamen oder eine Nummerierung der Knoten.
 



Graphen: Grundbegriffe
Nachdem wir bereits einige Grundbegriffe informell eingeführt haben legen wir jetzt eine einheitliche Notation fest.

 Ein ungerichteter Graph  besteht aus einer Knotenmenge  und einer Kantenmenge .
 Ein gerichteter Graph  besteht aus einer Knotenmenge  und einer Kantenmenge .
Wir gehen hier davon aus, dass  und  endlich sind, und dass zwischen  und  höchstens eine Kante existieren darf (in gerichteten Graphen darf es aber gleichzeitig eine Kante  und eine Kante  geben).
Wir bezeichnen die Anzahl  der Knoten in  mit  und die Anzahl  der Kanten in  mit .
Enthält  eine Kante , so sagen wir, dass  und  adjazent oder benachbart sind.
Außerdem nennen wir dann  und  bzw.  inzident.
Eine Kante, die  mit sich selbst verbindet nennen wir Schleife und schreiben (in nicht ganz exakter Notation)  für diese Kante.
In gerichteten Graphen verwenden wir analoge Benennungen.

In einem ungerichteten Graphen  bezeichnet  die Menge aller Kanten, die zu  inzident sind. Wir nennen diese Menge den Stern von .
In einem gerichteten Graphen  unterscheiden wir die Menge  aller ausgehenden Kanten von  und die Menge  aller eingehenden Kanten von .
Hier setzen wir .
Wir nennen die Anzahl  der zu  adjazenten Kanten den Grad von .
Entsprechend heißt die Anzahl eingehender bzw. ausgehender Kanten  bzw.  Eingangs- bzw. Ausgangsgrad von .

Wir stellen fest, dass die Definition eines Graphen keine Aussage darüber macht, wie der Graph dazustellen ist; bereits Abbildung  zwei unterschiedliche Zeichnungen des gleichen Graphen.

Starten wir an einem beliebigen Knoten  im Graphen und bewegen uns entlang der Kanten zu einem Knoten 
so erhalten wir einen --Weg, d.h. eine Abfolge  von Knoten und Kanten die derart ist, dass  ist.
Wir beobachten, dass ein Weg mit  Knoten genau  Kanten enthält und nennen  die Länge von .
Außerdem sagen wir, dass  die Knoten  besucht.
Besucht  seine Knoten nur je einmal - sind also alle Knoten auf dem Weg paarweise verschieden - so nennen wir  einen einfachen Weg oder einen Pfad.
Besitzt  unter allen --Wegen eine minimale Anzahl  an Kanten (gibt es also keinen --Weg mit weniger als  Kanten) so nennen wir  einen kürzesten Weg und sagen, dass die (Kürzeste-Wege-)Distanz  von  und  in  gerade  beträgt.
Ist  von  nicht erreichbar, so setzen wir .

Ein Kreis ist ein Weg, der zu seinem Ausgangspunkt zurückkehrt und außer dem Ausgangspunkt keinen Knoten zweimal enthält.
Für ungerichtete Graphen verwenden wir eine analoge Definition.

Jeder gerichtete Graph induziert einen ungerichteten Graphen, den wir erhalten, indem wir die Richtungen der Kanten ignorieren und ggf. doppelte Kanten entfernen.
Genauer erhalten wir aus dem gerichteten Graphen  einen ungerichteten Graphen  mit
.
Wir sagen, dass ein ungerichteter Graph  zusammenhängend ist, wenn zwischen je zwei Knoten 
ein --Weg in  existiert.
Ein gerichteter Graph heißt zusammenhängend, wenn der von ihm induzierte ungerichtete Graph zusammenhängend ist.
Ein zusammenhängender Graph, der keinen Kreis enthält, heißt Baum. Wir haben Bäume schon früher kennengelernt und können uns nun überzeugen, dass ein Suchbaum tatsächlich ein zusammenhängender kreisfreier Graph ist und so auch tatsächlich die Definition eines Baums erfüllt.

Datenstrukturen für Graphen

Überlegen wir uns nun, wie eine geeignete Datenstruktur für einen Graphen aussehen könnte.

Darstellung als Adjanzenzmatrix

Eine Adjazenzmatrix ist ein zweidimensionales Array , das  Spalten und  Zeilen enthält - für jeden Knoten im Graphen je eine.
Dabei ist es zweckmäßig, davon auszugehen, dass  ist, d.h., dass die Knoten fortlaufend von  bis  numeriert sind.
Um nun einen gerichteten Graphen  in einer Adjazenzmatrix  zu speichern, setzen wir , wenn  die Kante  enthält und  andernfalls.
Die zu  gehörige Zeile von  enthält also eine 1 für jede ausgehende Kante von .
Ist  ungerichtet, setzen wir , wenn  die Kante  enthält.
In diesem Fall erhalten wir also eine symmetrische Adjazenzmatrix.
Abbildung  zeigt ein Beispiel für einen gerichteten Graphen und seine Adjazenzmatrix, Abbildung  zeigt die Adjazenzmatrix zu einer durchnummerierten Version des Graphen in Abbildung .

In einem Graphen  mit  Knoten benötigen wir mit der Adjazenzmatrixdarstellung immer   Speicherplatz, unabhängig davon, wieviele Kanten  tatsächlich enthält.
Außerdem brauchen wir Zeit , um alle Nachbarn eines Knotens  zu finden, da wir die Zeile von 
in  komplett durchlaufen müssen.
Das ist auch so, wenn  nur konstant viele Nachbarn besitzt.
Die Adjazenzmatrix erlaubt es uns aber, in konstanter Zeit zu entscheiden, ob  eine Kante zwischen zwei Knoten  und  enhält, indem wir den Eintrag  kontrollieren.


fig-graph-3

Ein Beispiel für einen gerichteten Graphen und die zugehörige Adjazenzmatrix. 

fig-graph-4

Diese Abbildung zeigt die Adjazenzmatrix zu dem in Abbildung  abgebildeten Graphen, wobei die Knoten in der linken Abbildung von  aus im Uhrzeigersinn durchnummeriert wurden ( ist Knoten ,  ist Knoten , ,  ist Knoten ). 

Darstellung mit Adjazenzlisten

Bei der Darstellung mit Adjazenzlisten verwalten wir ein (eindimensionales) Array der Größe , das für jeden Knoten  eine Liste enthält.
Diese Liste heißt Adjazenzliste von ; in ihr speichern wir den Stern von .
Genauer:

Ist  gerichtet, so speichern wir für alle Knoten  in der Liste  alle ausgehenden Kanten von . Jede Kante  wird also in genau einer Adjazenzliste - der von  - gespeichert.
In der Praxis ist es oft nützlich, in einem zweiten Array die eingehenden Kanten zu verwalten.
Ist  ungerichtet, so speichern wir für alle Knoten  in der Liste  alle zu  inzidenten Kanten. Jede Kante  wird also in genau zwei Adjazenzlisten - der von  und der von  - gespeichert.
Abbildung  illustriert die Adjanzenlistendarstellung und Listing  zeigt eine Beispielimplementierung eines gerichteten
Graphen.

[caption=Ein gerichteter Graph mit Adjanzenlistendarstellung, label=lst:graphalgos:digraph]
class Edge(v,w):
    src = v
    tgt = w

class Digraph(nNodes,nEdges, S, T):
    n = nNodes
    m = nEdges
    edges = Array(m)
    adj = Array(n)

    for v=0,...,n-1:
        adj[v] = DLinkedList()

    for i=0,...,m-1:
        edges[i] = Edge(S[i], T[i])
        adj[ S[i] ].append(edges[i])

Der benötigte Speicherplatz für die Adjazenzlistendarstellung ist , also proportional zur Anzahl der Kanten im Graphen.
Außerdem können wir nun in Zeit  die Nachbarn eines Knotens  ausgeben; die benötigte Zeit
ist also proportional zur tatsächlich vorhandenen Anzahl an Nachbarn.
Der Nachteil der Adjazenzlistendarstellung ist, dass wir im Allgemeinen nicht mehr in konstanter Zeit entscheiden
können, ob eine Kante  im Graphen enthalten ist, weil wir für diese Entscheidung die Adjazenzliste
von  bzw.  durchsuchen müssen.



fig-graph-5

Beispiele für die Adjazenzlistendarstellung. Auf der linken Seite ist die Adjanzenlistendarstellung des Graphen aus Abbildung  zu sehen. Auf der rechten Seite sehen wir die Adjanzenlistendarstellung des ungerichteten Graphen, der von dem Graphen in Abbildung   induziert wird. 



Algorithmen zum Durchsuchen von Graphen

Im Folgenden betrachten wir immer einen Graphen .
In diesem Abschnitt lernen wir zwei Algorithmen kennen, mit denen wir Graphen durchsuchen können.
Die Durchsuchung dient selten dazu, einen bestimmten Knoten oder damit verbundene Informationen zu finden.
Vielmehr interessieren wir uns für Verbindungen zwischen Knoten:

 Können wir von einem Knoten  zu einem Knoten  gelangen, indem wir nur Kanten des Graphen
 benutzen (existiert also ein --Pfad in )?
 Wieviele Kanten müssen wir entlanglaufen, um von  nach  zu gelangen? Was ist die kürzeste Verbindungen zwischen  und , und wie lang ist sie?
 Ist  zusammenhängend?
Die Antwort auf alle diese Fragen liefert der Algorithmus Breitensuche aus dem nächsten Abschnitt.
Der Algorithmus ist außerdem ein wichtiger Baustein für viele fortgeschrittene Graphenalgorithmen.

BreitensucheWir betrachten hier die Breitensuche (englisch: Breadth-First-Search, BFS) für gerichtete Graphen.
Sie funktioniert aber genauso, wenn der Eingabegraph ungerichtet ist.
Die Breitensuche startet bei einem Startknoten .
Von dort besucht sie zunächst alle Nachbarn von , dann die Nachbarn der Nachbarn usw.
Genauer: Die Breitensuche besucht von  aus zunächst alle Knoten  mit Distanz  (das ist nur ), bevor sie den ersten Knoten  mit Distanz  besucht.
Erst nachdem alle Knoten mit Distanz 1 besucht wurden, besucht die Breitensuche den
ersten Knoten  mit Distanz .
Allgemein wird die Breitensuche von  aus erst alle Knoten  mit Distanz   besuchen, bevor der erste Knoten mit Distanz  besucht wird.
Dabei berechnet die Breitensuche auch für jeden Knoten  die Kürzeste-Wege-Distanz  von  nach , und einen BFS-Baum  mit Wurzel , aus dem wir einen kürzesten --Weg ablesen können:
Für jeden Knoten  ist der eindeutige --Weg in  ein kürzester --Weg in .

Die Breitensuche unterteilt die Knotenmenge  zu jedem Zeitpunkt in drei Teilmengen.
Die schwarzen Knoten sind diejeningen, die bereits fertig bearbeitet sind: Die Breitensuche hat sie bereits besucht.
Graue Knoten sind Nachbarn von einem schwarzen Knoten, aber wurden selbst noch nicht besucht; die
Breitensuche hat sie bisher nur entdeckt und führt nun Buch darüber, dass diese Knoten später noch besucht
werden müssen.
Alle übrigen Knoten sind weiß: Sie sind bisher noch nicht von der Breitensuche entdeckt worden, weil
noch keiner ihrer Nachbarn besucht wurde.

Die wichtige Idee der Breitensuche ist nun, eine Queue für die Verwaltung der grauen Knoten zu verwenden.
Wie wir sehen werden, stellt das sicher, dass die Knoten in der gewünschten Reihenfolge abgearbeitet
(besucht) werden.
Damit haben wir folgenden Algorithmus:

 Färbe alle Knoten weiß.
 Lege den Startknoten  in eine Queue  und färbe ihn grau.
 Solange  nicht leer ist:
 Nimm den vordersten Knoten  aus 
 Besuche 
 Färbe alle weißen Nachbarn von  grau und hänge sie an das Ende von  an
 Färbe  schwarz

Kürzeste Wege mit einer Breitensuche finden

Wir können nun zusätzlich für jeden Knoten  die Länge eines kürzesten --Weges - d.h. die Kürzeste-Wege-Distanz  - in einem Array  berechnen.
Dazu setzen wir initial  und  für alle anderen Knoten .
Besuchen wir nun einen Knoten , so setzen wir die Distanzen aller seiner weißen Nachbarn auf .
Die Idee ist hier, dass wir einen weißen Nachbarn  von  erreichen, indem wir zunächst über  Kanten von  nach  gehen und dann dort die Kante  entlang laufen.
Dieser Weg besitzt  Kanten.
Da  weiß ist, wissen wir, dass  zuvor noch nicht entdeckt wurde und wir daher nicht schon einen kürzeren
Weg zu  gefunden haben können (denn  ist der erste Nachbar von , der besucht wird).
Wir zeigen später, dass diese Vorgehensweise korrekt ist.
Außerdem verwalten wir für jeden Knoten  einen Vorgängerkante  im BFS-Baum; das ist die
eindeutige eingehende Kante von  im BFS-Baum (oder null, wenn  ist oder
 von  aus nicht erreichbar ist).
Intuitiv ist also bei Terminierung für alle erreichbaren Knoten  die Kante  die letzte Kante auf
dem kürzesten --Weg, den die BFS berechnet hat.
Wir berechnen  analog zum Distanzwert :
Dazu initialisieren wir alle Einträge von  mit null.
Wird nun ein Nachbar  von  besucht und ist  weiß, so speichern wir in  die Kante .
Schließlich ist nach unserer Intuition  der Vorgänger von  auf einem kürzesten Weg von  nach .
Wir erhalten damit den Algorithmus in Listing .

Wir sehen, dass die Farbe der Knoten im Laufe des Algorithmus von weiß über grau nach schwarz
wechselt:

 Zunächst ist jeder Knoten  weiß.
  Nur wenn  weiß ist, kann  grau gefärbt und in die Queue  eingefügt werden ( wird entdeckt): Wir fügen nur in Zeile  und in Zeile  einen Knoten  in  ein.
  Beide Zeilen werden nur erreicht, wenn der eingefügte Knoten weiß war und direkt vor dem Einfügen grau umgefärbt wird.
  Da  nun grau ist, kann  nie wieder in  eingefügt werden.
  Wird  aus der  entnommen ( wird besucht), so wird  schwarz gefärbt.
Wir sehen außerdem, dass  stets nur graue Knoten enthält:
Es werden nur grau gefärbte Knoten in  eingefügt und ein grauer Knoten  kann nur dann seine Farbe wechseln, wenn er aus  entnommen und schwarz gefärbt wird (und nach unserer Beobachtung kann  auch nicht
noch ein zweites mal in  enthalten sein).

[caption=Methode Breitensuche, label=lst:graphalgos:bfs]
function bfs(g, s):
    color = Array(g.n, white)
    d = Array(g.n, infinity)
    p = Array(g.n, null)

    q = Queue()

    d[s] = 0
    color[s] = gray
    q.push(s) 

    while not q.empty(): 
        v = q.pop()      
        for e in g.adj[v]:
            if color[e.tgt] == white:
                d[e.tgt] = d[v]+1 
                p[e.tgt] = e

                color[e.tgt] = gray
                q.push(e.tgt) 

        color[v] = black

    return d, p
[caption=Berechnung eines kürzesten --Weges mittels Breitensuche]
 function shortest-path(g, s, t, p):
    # berechne Distanzen und Vorgaenger mit BFS
    d, p = bfs(g,s)

    # enthaelt am Ende Kanten eines kuerzesten
    # s-t-Weges in richtiger Reihenfolge
    # (von s nach t)
    sp = DLinkedList()

    # Folge Vorgaengern bis zu s.
    e = p[t]
    while e != null:
        sp.insert(sp.head, e)
        e = p[e.src]

    return sp



fig-graph-6

Beispielausführung BFS mit Startknoten 0.

fig-graph-6b

Beispielausführung BFS mit Startknoten 0, Fortsetzung.

fig-graph-6c

Beispielausführung BFS mit Startknoten 0, Fortsetzung.



Laufzeit der Breitensuche

Damit können wir uns nun überlegen, dass die Worst-Case-Laufzeit der Breitensuche  ist.
Dabei gehen wir davon aus, dass der Eingabegraph in Adjazenzlistendarstellung gegeben ist.

Da jeder Knoten  also höchstens einmal in  eingefügt und höchstens einmal aus  entnommen werden kann
ist der Aufwand, der für die Verwaltung von  anfällt höchstens .
Betrachten wir nun eine beliebige Iteration der while-Schleife.
In dieser Iteration wird ein Knoten  aus  entnommen und die for-Schleife durchläuft die
Adjazenzliste von .
Der Aufwand für jedes Element der Adjazenzliste von  (d.h. jede ausgehende Kante von ) ist konstant.
Damit ist der Aufwand der betrachteten Iteration der while-Schleife proportional zur Länge
der Adjazenzliste von .
Jede Kante von  befindet sich in genau einer Adjazenzliste.
Daher summieren sich die Längen der Adjazenzlisten gerade zu .(Wäre  ungerichtet würde sich jede Kante in zwei Adjazenzlisten befinden, so dass die Länge der Adjazenzlisten sich zu  summieren würde. Das macht für die Worst-Case-Laufzeit in -Notation keinen Unterschied.)
Es folgt, dass der Gesamtaufwand der while-Schleife abgesehen von der Verwaltung von  gerade  beträgt.
Insgesamt haben wir also eine Worst-Case-Laufzeit von .
Das Ergebnis ist überraschend: Eine oberflächliche Betrachtung des Pseudocodes ergibt, dass die while-Schleife  Iterationen macht und der Aufwand pro Iteration im Worst-Case  beträgt, da
alle Kanten in einer Adjanzenliste enthalten sein können. Daher würden wir eine Laufzeit von  erwarten.
Unsere genauere Analyse trägt der Tatsache Rechnung, dass zwar alle Kanten in einer Adjazenzliste liegen können, dann aber alle anderen Adjazenzlisten leer sind. Wir können unsere Analyse daher als Anwendung der Aggregatmethode sehen: Anstatt eine Worst-Case-Analyse für eine einzelne Iteration der while-Schleife zu machen, analysieren wir alle Iterationen gemeinsam.

 Die Methode Breitensuche benötigt auf einem Graphen  im Worst-Case eine Laufzeit von .
Korrektheit der Breitensuche
Wir möchten noch nachweisen, dass die Breitensuche tatsächlich kürzeste Wege findet.
Dazu überlegen wir uns zunächst, dass die Kürzeste-Wege-Distanz  eine nützliche
Eigenschaft hat:
Ist nämlich  ein Nachbar von , so kann die Kürzeste-Wege-Distanz  von  höchstens  sein.
Das liegt daran, dass wir einen --Weg  mit  Kanten erhalten, wenn wir einen kürzesten Weg von  nach  um die Kante  verlängern.
Auf diese Weise erhalten wir zwar nicht notwendigerweise einen kürzesten --Weg, aber zumindest kann
ein tatsächlicher kürzester --Weg nicht mehr Kanten enthalten als .
Wir notieren unsere Beobachtung in einem Lemma.
 Sei  ein fester Startknoten und sei  die Kürzeste-Wege-Distanz von  nach .
 Dann gilt für jede Kante :
 *
 Liegt  auf einem kürzesten Weg von  nach , so gilt sogar .

 Wir haben bereits argumentiert, dass der erste Teil des Lemmas richtig ist.
 Um nun auch den zweiten Teil zu beweisen betrachten wir einen kürzesten --Weg , der die Kante  enthält und zeigen, dass dann  gilt.
Der Teilweg  von  ohne die Kante  muss ein kürzester --Weg sein, denn:
Gäbe es einen kürzeren --Weg , dann wäre  ein --Weg, der weniger
Kanten enthält als .
Also muss  genau  viele Kanten enthalten, und damit hat  genau Länge .

Als nächstes überlegen wir uns, dass die von der Breitensuche berechnete Distanz  zumindest eine obere
Schranke für die echt Kürzeste-Wege-Distanz  sein muss.
 Wir betrachten eine Breitensuche, die bei einem festen Startknoten  startet.
 Nachdem diese Breitensuche terminiert hat, gilt:
 

Wir zeigen per Induktion, dass eq:graphalgos:bfs-sp-dist tatsächlich immer gilt, wenn
Zeile  durchlaufen wird.

Induktionsanfang. Wenn Zeile  zum ersten Mal erreicht wird,
ist , so dass eq:graphalgos:bfs-sp-dist zumindest für  gilt.
Für alle  ist .

Induktionsschritt. Betrachten wir irgendeine Iteration der while-Schleife und
setzen wir voraus, dass eq:graphalgos:bfs-sp-dist gilt, bevor die Iteration startet.
Ist  leer, so stoppt die Breitensuche und die Behauptung gilt.
Anderenfalls wird ein Knoten  aus  entnommen.
Für  gilt nach Induktionsvoraussetzung .
Nun wird  für alle weißen Nachbarn von  aktualisiert (für alle
anderen Knoten ändert sich  nicht).
Betrachten wir also einen weißen Nachbarn  von .
Wir setzen
*
Die Behauptung gilt also weiterhin nachdem wir eine Iteration der while-Schleife
durchgeführt haben.
Als letzte Eigenschaft weisen wir nach, dass  stets nach den Distanzen  sortiert ist und
sich die Distanzen der Knoten in  um höchstens 1 unterscheiden.
 Zu Beginn einer beliebigen Iteration der while-Schleife
 in Zeile  enthalte  die Knoten  in dieser
 Reihenfolge ( ist dabei der zuerst eingefügte Knoten).
 Dann gilt:
 *

Wir zeigen die Behauptung per Induktion über die Anzahl der Iterationen der while-Schleife.

Induktionsanfang.
Bevor die while-Schleife das erste mal erreicht wird enthält  nur den Knoten 
und die Behauptung ist eine leere Bedingung.

Induktionsschritt Betrachten wir also eine beliebige Iteration der while-Schleife.
Hier enthalte  die Knoten  in dieser Reihenfolge.
Per Induktion können wir voraussetzen, dass
 
 gilt.
 Wir müssen nun zeigen, dass eine Iteration der while-Schleife diese Eigenschaften nicht zerstört.
Wir untersuchen systematisch die Zeilen, die  verändern.

 Zeile  entnimmt  aus . Dadurch kann  nicht
 verletzt werden.
 Allerdings ist nun  der neue erste Knoten in , so dass wir  überprüfen müssen.
 Es ist aber  nach Induktionsvoraussetzung.
 Zeile  fügt Nachbarn von  zu  hinzu.
 Nennen wir diese Nachbarn .
 Wegen Zeile  gilt  so dass eq:graphalgos:bfs:inv-1 gilt.
 Außerdem ist  und eq:graphalgos:bfs:inv-2
 gilt weiterhin.

Jetzt können wir zeigen, dass die Breitensuche die kürzesten Wege korrekt berechnet.
 Wenn die Breitensuche terminiert gilt für alle Knoten : .

 Beweis per Widerspruch.
 Nehmen wir an, es gäbe einen Knoten  mit .
 Wegen Lemma  muss  gelten.
 Betrachten wir einen kürzesten Weg  von  nach  und gehen wir davon aus, dass
 für alle anderen Knoten auf dem Weg die Behauptung erfüllt ist.
 Ist das nicht der Fall, wählen wir  als den ersten Knoten auf , der die
 Behauptung verletzt.
 Sei jetzt  der Knoten, der auf  direkt vor  liegt (d.h.,  endet mit der Kante ).

 Aufgrund unserer Wahl von  ist , und das galt auch schon, als  aus 
 entfernt wurde (denn dann wurde  schwarz gefärbt und damit  fixiert).
 Wir behaupten, dass  zu diesem Zeitpunkt weiß war, denn: Nehmen wir an,  war grau
 als  aus der Queue entfernt wurde.
 Dann gab es einen Nachbarn  von , der vor  entdeckt wurde.
 Lemma  sagt uns, dass dann  sein muss.
 Es folgt
 *
 Das ist aber ein Widerspruch zu unserer Annahme und  muss weiß gewesen sein, als  aus 
 entfernt wurde.
 Dann haben wir aber  gesetzt.

Die Breitensuche berechnet Kürzeste-Wege-Distanzen von  zu allen anderen Knoten. In einem ungerichteten Graphen  können
wir mit Hilfe einer BFS auch testen, ob  zusammenhängend ist:
Ist nach Abschluss der Breitensuche für alle  der Wert , so können alle Knoten von  aus erreicht werden und  ist zusammenhängend.(Ein ungerichteter Graph  ist zusammenhängend, wenn für alle Knotenpaare  ein --Weg in  existiert. Es genügt aber zu testen, ob für ein festes  und alle  ein --Weg existiert, denn:
Existiert für alle  ein --Weg, dann erhalten wir für ein beliebiges  einen --Weg, indem wir zunächst von  nach  und dann von  nach  laufen (dabei können wir ggf. doppelte Kanten entfernen). Wir beobachten, dass dieses Argument nur in ungerichteten Graphen funktioniert.
Umgekehrt existiert insbesondere für alle  ein --Weg, wenn  zusammenhängend ist.
Die Bedingungen sind in ungerichteten Graphen also äquivalent.)
Anderenfalls gibt es einen Knoten, der von  aus nicht erreicht werden kann und  kann nicht zusammenhängend sein.
Möchten wir testen, ob ein gerichteter Graph zusammenhängend ist, testen wir gemäß der Definition von zusammenhängend den induzierten ungerichteten Graphen.

Tiefensuche

Die Breitensuche sucht in die Breite des Graphen: Sie schiebt eine Front von grauen Knoten vor sich her, und diese grauen Knoten haben alle (fast) den gleichen Abstand zum Startknoten .
Die Tiefensuche (englisch: Depth First Search, DFS) versucht sich hingegen so schnell wie möglich vom Startknoten  zu entfernen und sucht also in die Tiefe des Graphen:
Sobald sie einen neuen Knoten entdeckt, folgt sie von dort einer Kante zum nächsten unbekannten Knoten und setzt sich dort rekursiv fort.
Gibt es keine solche Kante kehrt die Rekursion und damit auch die Tiefensuche zum zuvor besuchten Knoten zurück.

Ein weiterer Unterschied zur Breitensuche besteht im Aufruf des Verfahrens:
Wir hatten die Breitensuche von einem Startknoten  gestartet und es hingenommen, dass sie ggf. nicht alle Knoten des Graphens erreicht (etwa, weil der Graph nicht zusammenhänged ist oder keinen gerichteten Pfad von  zu einem Knoten  enthält).
Der Grund für diese Herangehensweise ist, dass die die Breitensuche in der Regel dafür verwendet wird, kürzeste Wege Distanzen von  aus zu berechnen, so dass wir nicht erreichbare Knoten mit Distanz  markieren möchten.
In Anwendungen für die Tiefensuche ist es jedoch oft erwünscht, dass der gesamte Graph durchsucht wird.
Erreicht daher die Tiefensuche einen Knoten  nicht, starten wir sie von dort erneut.
Natürlich könnten wir bei der Breitensuche analog vorgehen.

Wie die Breitensuche markiert die Tiefensuche nun den Suchfortschritt an jedem Knoten durch eine Farbe: Weiße Knoten wurden noch nicht entdeckt, graue Knoten sind aktuell in Bearbeitung und bei schwarzen Knoten ist die Bearbeitung abgeschlossen.
Ein Besuch der Tiefensuche bei einem weißen Knoten  beginnt nun damit, dass sie  grau markiert um anzuzeigen, dass die
Bearbeitung von  begonnen hat.
Außerdem notiert sie den Zeitpunkt des Beginns der Bearbeitung (diesen Zeitpunkt nennen wir discovery und schreiben ). Der Zeitpunkt wird in DFS-Zeit gemessen.
Diese ist nicht die aktuelle Systemzeit: Sie beginnt bei 0 und schreitet immer um eins voran, wenn ein Besuch bei einem Knoten begonnen oder abgeschlossen wird.
Dementsprechend lässt die Tiefensuche nun die DFS-Zeit um eins voranschreiten.
Anschließend besucht sie rekursiv einen weißen Nachbarn  und setzt einen Vorgänger-/Elternzeiger von  auf .
Gibt es keine weißen Nachbarn von , so wird  schwarz markiert, um das Ende der Bearbeitung anzuzeigen.
Die Tiefensuche notiert den Zeitpunkt (finish, wir schreiben ) der Bearbeitung von  und lässt die DFS-Zeit um eins voran schreiten.
Nun kehrt die Rekursion zurück.



    fig-dfs-directed-example
    
  Ein Beispieldurchlauf der Tiefensuche (DFS) auf einem gerichteten Graphen.
 Wir klassifizieren die Kanten als , ,  und .
 Die Tiefensuche startet zunächst am Knoten . Nachdem sie alle Knoten  besucht hat, die von  aus erreicht werden können, bleiben die Knoten  und  unentdeckt.
 Die Tiefensuche wird nun an Knoten  fortgesetzt. Wir untersuchen die Kanten in lexikografischer Reihenfolge.

Die beschrieben Tiefensuche funktioniert sowohl für gerichtete als auch für ungerichtete Graphen.
Wir betrachten in Listing  eine Implementierung für gerichtete Graphen
und halten fest, dass die Tiefensuche mit Worst-Case-Laufzeit  funktioniert, da
wie bei der Breitensuche jede Kante bei gerichteten Graphen höchstens einmal und bei ungerichteten Graphen
höchstens zweimal betrachtet wird.

Der Algorithmus Tiefensuche benötigt auf einem Graphen  im Worst-Case eine Laufzeit von .
[caption=Tiefensuche, float, label=lst:graphalgos:dfs]
class DFSData(n):
    color = Array(n)
    p = Array(n)
    discovery = Array(n)
    finish = Array(n)
    time = 0

function dfs(g):
    d = DFSData(g.n)
    for v=0,...,g.n-1:
        d.color[v] = white
        d.p[v] = null
        d.discovery[v] = -1
        d.finish[v] = -1

    d.time = 0

    for v = 0,...,g.n-1:
        if d.color[v] == white:
            dfs(g, v, d)

function dfs(g, v, d):
    d.discovery[v] = d.time
    d.time++
    d.color[v] = gray
    for e in g.adj[v]: 
        if d.color[e.tgt] == white:
            p[e.tgt] = v
            dfs(g, e.tgt, d) 

    d.color[v] = black
    d.finish[v] = d.time
    d.time++


Abbildung  zeigt einen Beispieldurchlauf der Tiefensuche mit discovery- und finish-Zeiten, sowie einer
Kantenklassifizierung, die wir im Folgenden definieren werden.

Zunächst untersuchen wir einige Eigenschaften der Tiefensuche genauer und müssen dafür unsere Definition eines Weges und eines Kreises für gerichtete Graphen präzisieren.


Ein gerichteter --Weg in  ist eine Folge 
von  Knoten und  Kanten so, dass  für alle  gilt.
Ein gerichteter --Weg ist also ein Weg, der Kanten  in  nur in der Richtung von  nach 
benutzt.
Analog ist ein gerichteter Kreis in  ein gerichteter --Weg, der nur den Knoten  doppelt
enthält.

Da wir die Tiefensuche von jedem Knoten aus starten, berechnet sie potentiell nicht nur einen DFS-Baum, sondern mehrere.
Jeder Knoten ist in genau einem der DFS-Bäume enthalten.
Wir sagen, dass  ein DFS-Nachfolger von  ist, wenn im DFS-Baum ein gerichteter Pfad von  nach 
existiert, d.h. wenn es Knoten  gibt so, dass  ist, für .
Anschaulich bedeutet das, dass der DFS-Aufruf bei  (ggf. über weitere rekursive DFS-Aufrufe) den Knoten  entdeckt hat.
Wir halten diese Intuition in einem Lemma fest.
 Ein Knoten  ist genau dann ein DFS-Nachfolger eines Knotens , wenn  zum Discovery-Zeitpunkt von  grau ist.
[Beweisidee]
 Wir untersuchen zunächst genauer, was passiert wenn ein neuer Knoten entdeckt wird und fixieren dazu einen
 beliebigen Zeitpunkt.
 Wir betrachten diejenigen Knoten , die zu diesem Zeitpunkt grau sind und
 wählen die Indizes  so, dass die Knoten nach ihrem Discovery-Zeitpunkt sortiert sind
 (also so, dass  gilt.
 Man kann nun per Induktion zeigen, dass wir einen einfachen Weg erhalten, wenn wir
  in dieser Reihenfolge besuchen und dass  genau die Knoten 
 als DFS-Vorgänger hat, für .

Wir verwenden nun das Ergebnis dieses Induktionsbeweises für den Zeitpunkt, zu dem  entdeckt wird.
Aufgrund der Funktionsweise der Tiefensuche kann  nur von  aus (dem grauen Knoten mit dem größten Discovery-Wert) entdeckt werden. Also besitzt  genau alle grauen Knoten  als DFS-Vorgänger.

Für einen Knoten  nennen wir den Zeitabschnitt 
das Bearbeitungsintervall von .
Das Bearbeitungsintervall enthält genau die Zeitpunkte, zu denen  grau ist.
Es ist daher kaum überraschend, dass wir an den Bearbeitungsintervallen ablesen können, ob ein Knoten  ein DFS-Nachfolger eines Knotens  sein kann.

 Für jeden Graphen  (gerichtet oder ungerichtet) und für jedes Paar von verschiedenen Knoten   gilt genau eine der drei folgenden Aussagen.
  
  Die Bearbeitungsintervalle  und  von  bzw.  sind disjunkt, d.h. es ist  oder .
  Weder ist  ein DFS-Nachfahre von , noch andersherum.
  Das Bearbeitungsintervall  von  ist vollständig im Bearbeitungsintervall  von 
  enthalten, d.h. es gilt  und .
  Es ist  ein DFS-Nachfolger von .
  Das Bearbeitungsintervall  von  ist vollständig im Bearbeitungsintervall  von 
  enthalten, d.h. es gilt  und .
  Es ist  ein DFS-Nachfolger von .
 [Beweis. Nicht besprochen/nicht klausurelevant]
Wir gehen o.B.d.A. davon aus, dass  vor  entdeckt wird, d.h. dass  gilt.
Ansonsten vertauschen wir die Benennung von  und  und stellen fest, dass 
nicht eintreten kann, weil .
Da also nun  vor  entdeckt wird ist  kein DFS-Nachfolger von .

Wir machen eine Fallunterscheidung nach .
Ist  sind die Bearbeitungsintervalle von  und  disjunkt.
Da  zum Zeitpunkt  schwarz gefärbt wird ist  schwarz, wenn  entdeckt wird.
Lemma  sagt uns, dass  kein DFS-Nachfolger von  sein kann.
Es gilt Aussage (1.), aber weder (2.) noch (3.).

Ist anderenfalls  wird  entdeckt bevor die Bearbeitung von 
abgeschlossen ist. Also ist  grau, wenn  entdeckt wird.
In diesem Fall erhalten wir aus Lemma , dass  ein DFS-Nachfolger von 
ist.
Da  erst schwarz gefärbt werden kann nachdem der rekursive DFS-Aufruf bei  zurückgegekehrt ist gilt auch .
Zusammen mit unserer anfänglichen Voraussetzung, dass  gilt folgt, dass  vollständig in  enthalten ist und es gilt Aussage (3.), aber weder (1.) noch (2.).

Wir klassifizieren nun Kanten anhand der discovery- und finish-Zeitpunkte.
Wenn die DFS eine Kante  in den Zeilen - untersucht, unterscheiden wir:(Ist  ungerichtet wird  zweimal untersucht. Wir wählen für die Klassifizierung den Zeitpunkt der ersten Untersuchung)

 Ist  weiß so wurde  noch nicht besucht und die DFS wird auf  rekursiv aufgerufen.
 Wir nennen  eine Tree-Kante.
 
 Jede Tree-Kante entspricht genau einer Kante eines DFS-Baumes:
 Nur wenn  weiß ist hatten wir  gesetzt (die Kanten in den DFS-Bäumen sind aber genau umgekehrt gerichtet).
 Ist  grau, so wurde  von der Tiefensuche erreicht, seine Bearbeitung ist aber noch nicht abgeschlossen.
 Die Tiefensuche muss also von  aus zu  zurückgekehrt sein (sonst könnte sie jetzt nicht die Kante  betrachten).
 Wir nennen  eine Back-Kante.
 Back-Kanten führen zu einem Vorgängerknoten von .
 Ist  schwarz, so wurde  bereits von der Tiefensuche erreicht und ist vollständig bearbeitet.
 Ist  wurde  nach  entdeckt, aber die Bearbeitung von  ist noch nicht abgeschlossen, wenn die Kante  betrachtet wird.
 Also war  zum discovery-Zeitpunkt von  grau und  ist ein DFS-Nachfolger von .
 Wir nennen daher  eine Forward-Kante.
 Forward-Kanten führen zu einem Nachfolgerknoten von , d.h. im DFS-Teilbaum von  nach vorne.
 Ist  schwarz, so wurde  bereits von der Tiefensuche erreicht und ist vollständig bearbeitet.
 Ist , so wurde  vor  entdeckt.
 Da  schwarz ist, muss die DFS von  zurückgekehrt sein, bevor sie  entdeckt hat (es ist also auch .
 Daher liegt  in einem anderen Teilbaum des DFS-Baums, der  enthält, oder in einem anderen DFS-Baum.
 Wir nennen  eine Cross-Kante.


fig-dfs-arc-types

Eine Momentaufnahme eines DFS-Durchlaufs zum Zeitpunkt  mit weißen (), grauen () und schwarzen () Knoten.
Die DFS hat die Knoten in der Reihenfolge  besucht; links an den Knoten ist die discovery-Zeit, rechts an den Knoten die finish-Zeit notiert. Es ist gut zu erkennen, dass die discovery-Zeit den Zeitpunkt angibt, an dem der rekursive DFS-Aufruf an einem Knoten startet während die finish-Zeit angibt, wann die Rekursion zurückkehrt.
Da weiße Knoten noch nicht entdeckt wurden, haben sie auch keinen discovery-Wert; ebenso haben graue Knoten keinen finish-Zeit
In diesem Beispiel hat die DFS zunächst einen DFS-Baum mit den Knoten  gefunden. Anschließend wurde
eine weitere DFS von Knoten  gestartet, die zunächst den Ast  abgearbeitet hat.
Zum Zeitpunkt 17 befindet sich die DFS nun an Knoten 10 und wir erkennen, dass es einen einfachen Weg aus grauen Knoten vom Startknoten 3 zum aktuellen Knoten 10 gibt.
An diesen Knoten gibt es noch aktive rekursive DFS-Aufrufe.
Entdeckt die DFS eine Kante, die zu einem weißen Knoten führt, wird diese zu einer Tree-Kante (). Kanten zu grauen Knoten führen zu einem DFS-Vorgänger und werden Back-Kanten ().
Kanten zu schwarzen Knoten können im ersten Fall zu einem anderen DFS-Teilbaum führen (der auch in einem anderen DFS-Baum liegen kann); sie werden zu Cross-Kanten (). Das erkennen wir daran, dass die finish-Zeit ihres Endknotens vor der discovery-Zeit des aktuellen Knotens liegt.
Im zweiten Fall führt eine Kante zu einem schwarzen Knoten im gleichen DFS-Teilbaum; das passiert, wenn die finish-Zeit des schwarzen Knotens nach der discovery-Zeit des aktuellen Knotens liegt.
Diese Kanten werden zu Forward-Kanten ().
Gestrichelte Kanten wurden von der DFS noch nicht untersucht.

In einem ungerichteten Graphen vereinfacht sich die Klassifizierung.

In einem ungerichteten Graphen erzeugt die Tiefensuche nur Tree- und Back-Kanten.
[Beweis. Nicht besprochen/nicht klausurelevant]
 Betrachten wir eine beliebige Kante  und wählen wir o.B.d.A. die Benenennung
 von  und  so, dass  ist.

 Wird  von  aus zum ersten mal untersucht, so ist  zu diesem Zeitpunkt noch
 weiß (denn  wurde nach Voraussetzung vor  entdeckt. Ist  nicht weiß, hat die DFS  schon besucht
 und  wäre von  aus zuerst untersucht worden).
 Also wird  eine Tree-Kante.

 Anderenfalls wird  von  aus das erste Mal untersucht.
 Zu diesem Zeitpunkt ist aber nach Voraussetzung  schon entdeckt worden.
 Also ist  nicht weiß.
 Da aber an  die Kante  noch nicht untersucht wurde, kann  auch nicht schwarz sein, also ist  grau. Daher wird  eine Back-Kante.


Wir halten abschließend noch zwei wichtige Beobachtungen fest.
Ein ungerichteter Graph heißt kreisfrei, wenn er keinen Kreis enthält.
Ein gerichteter Graph heißt kreisfrei, wenn er keinen gerichteten Kreis enthält.
 Ein ungerichteter oder gerichteter Graph ist genau dann kreisfrei, wenn die Tiefensuche keine Back-Kanten erzeugt.
Beweisidee
  Wir führen den Beweis nur für einen gerichteten Graphen ; er funktioniert analog für ungerichtete Graphen.

  Für die Rückrichtung müssen wir zeigen, dass  einen Kreis enthält, wenn die DFS eine Back-Kante  erzeugt.
  Dann muss  noch grau sein, wenn  erreicht wird und die DFS die Kante  untersucht.
  Aus Lemma  folgt, dass  ein DFS-Vorgänger von  ist.
  Es gibt also einen --Pfad aus Tree-Kanten, der durch  zu einem (gerichteten) Kreis vervollständigt wird.

  Für die Hinrichtung müssen wir zeigen, dass die DFS eine Back-Kante erzeugt, wenn  einen (gerichteten) Kreis enthält.
  Sei  also ein Kreis in ; wir wählen die Bezeichungen so, dass  der erste Knoten auf dem Kreis ist, der von der DFS entdeckt wird.
  Demnach sind zum Zeitpunkt  die anderen Knoten des Kreises noch weiß, aber  ist grau.
  Man kann zeigen, dass  dann immer noch grau ist, wenn die Tiefensuche zum ersten mal  erreicht (die Idee ist hier, dass  erst schwarz werden kann, wenn alle Kanten aller DFS-Nachfolger von  untersucht wurden, und dass  ein DFS-Nachfolger von  sein muss, da es in  einen --Weg gibt. Also wird  untersucht, bevor  schwarz wird).
  Die Kante  wird daher zur Back-Kante.



Zum Abschluss der Diskussion der Tiefensuche halten wir fest, dass die Tiefensuche auf einem ungerichteten Graphen genau einen Tiefensuchbaum für jede Zusammenhangskomponente des Graphen berechnet.
Dabei verstehen wir unter einer Zusammenhangskomponente Folgendes.

Ein Subgraph eines Graphen  ist ein Graph  mit  und .
Eine Teilmenge  der Knoten induziert einen Subgraphen  auf folgende Weise:
Die Kantenmenge  enthält alle Kanten von , deren Knoten beide in  liegen; d.h.,
.
Eine Zusammenhangskomponente eines Graphen  ist nun eine maximale Knotenteilmenge  so, dass der von  induzierte Teilgraph  zusammenhängend ist.
Maximal bedeutet hier, dass für alle  der Graph  nicht zusammenhängend ist.
Anschaulich sind also die Zusammenhangskomponenten eines Graphen die Teilgraphen, zwischen denen keine Kanten verlaufen.



Eine Anwendung der Tiefensuche: Topologische Sortierung

In diesem Abschnitt betrachten wir mit topologischen Sortierungen eine Anwendung für die Tiefensuche.
Topologische Sortierungen werden dann benötigt, wenn Aufgaben erledigt werden sollen, die untereinander Abhängigkeiten besitzen.
Betrachten wir dazu ein Beispiel, das an  angelehnt ist:
Herr Trospot möchte sich anziehen: Dazu ist es erforderlich, dass er gewisse Kleidungsstücke in der
richtigen Reihenfolge anzieht (etwa sein T-Shirt vor seinem Hemd, seine Socken vor seinen Schuhen).
Diese Situation nennen wir eine Abhängigkeit.
Andere Kleidungsstücke sind dagegen voneinander unabhängig: So spielt es keine Rolle, ob Herr Trospot
zuerst seine Brille oder seine Uhr anzieht.
Um uns nun einen Überblick darüber zu verschaffen, in welcher Reihenfolge Herr Trospot seine Kleidungsstücke
anlegen sollte, modellieren wir die Problemstellung als gerichteten Graphen, in dem eine Kante von  nach  zeigt, wenn  eine Abhängigkeit/Vorbedingung von  ist.
Diesen Graphen nennen wir Abhängigkeitsgraphen.




































Herr Trospot könnte seine Kleidungsstücke also beispielsweise in der folgenden Reihenfolge anziehen:


Dass diese Reihenfolge der Kleidungsstücke gültig ist (also keine Abhängigkeit verletzt) erkennen wir daran,
dass alle Kante von links nach rechts verlaufen:
Wir haben die Knoten so nummeriert, dass jede Kante  erfüllt, dass ihr Startknoten  eine kleinere Nummer besitzt als ihr Endknoten .
Ist also  abhängig von  so wird  auf jeden Fall vor  angezogen.
Wir nennen eine solche Reihenfolge eine topologische Sortierung.

Topologische Sortierungen helfen nicht nur Herrn Trospot beim Anziehen, sie sagen uns auch, in welcher Reihenfolge Abhängigkeiten für Softwarepakete installiert werden sollten oder bestimmen die Abfolge eines
Software-Build-Prozesses.

Wir erkennen an obigen Beispiel, dass es keine topologische Sortierung geben kann, wenn der Abhängigkeitsgraph
einen Kreis enthält, denn wir können nur zum Ausgangspunkt zurückkehren und einen Kreis schließen, indem wir eine Kante von rechts nach links einzeichnen.
Wir weisen im Folgenden nach, dass aber immer eine topologische Sortierung existiert, wenn der Abhängigkeitsgraph frei von Kreisen (wir sagen auch azyklisch) ist.

Wir können eine topologische Sortierung mit Hilfe einer Tiefensuche bestimmen: Die Anschauung ist hier, dass die Tiefensuche immer weiter in den Eingabegraphen hineinläuft, bis sie in einen Knoten findet, der keine ausgehenden Kanten mehr besitzt.
Dieser Knoten ist keine Abhängigkeit eines anderen Knotens und kann daher in der topologischen Sortierung am Ende eingefügt werden.
Allgemein: Sind alle Nachbarn  - also alle Knoten, von denen  eine Vorbedingung ist - bereits in der topologischen Sorierung enthalten, so kann  am Anfang der Sortierung eingefügt werden.
Wir zeigen daher im Folgenden, dass wir eine topologische Sortierung erhalten, wenn wir die Knoten in absteigender Reihenfolge ihrer finish-Zeitpunkte ausgeben.

 Führe eine vollständige Tiefensuche auf  aus.
 Wird dabei ein Knoten schwarz gefärbt, lege ihn auf einen Stack.
 Gib den Stack von oben nach unten aus.
Treffen wir bei der Tiefensuche auf eine Back-Kante, so enthält  gemäß Lemma  einen Kreis und wir geben einen Fehler aus.
Back-Kanten erkennen daran, dass wir einen grauen Nachbarn des gerade untersuchten Knotens finden.
Listing  zeigt den Pseudocode dieses Algorithmus.
[caption=Topologisches Sortieren, float, label=lst:graphalgos:dfs-top-sort]
function top_sort(g):
    d = DFSData(g.n)
    for v=0,...,g.n-1:
        d.color[v] = white
        d.p[v] = null
        d.discovery[v] = -1
        d.finish[v] = -1

    d.time = 0
    S = Stack()

    for v = 0,...,g.n-1:
        if d.color[v] == white:
            top_sort_dfs(g, v, d, S)

    while not S.empty():
        print S.pop()

function top_sort_dfs(g, v, d, S):
    d.discovery[v] = d.time
    d.time++
    d.color[v] = gray
    for e in g.adj[v]:
        if d.color[e.tgt] == gray:
            error "G has no topological ordering"
        if d.color[e.tgt] == white:
            p[e.tgt] = v
            top_sort(g, e.tgt, d, S)

    d.color[v] = black
    S.push(v)
    d.finish = d.time
    d.time++

Wir zeigen jetzt, dass unsere Intution korrekt ist und das Vorgehen tatsächlich eine topologische Sortierung liefert.

 Der Algorithmus topologisches Sortieren mit DFS liefert in Zeit  eine
 topologische Sortierung von  oder entscheidet, dass  einen Kreis enthält und daher keine topologische
 Sortierung besitzt.

 Wir müssen zeigen, dass unser Algorithmus eine topologische Sortierung ausgibt.
 Dazu betrachten wir eine beliebige Kante  in  und müssen zeigen, dass  in der ausgegebenen
 Sortierung vor  steht.
 Das ist genau dann der Fall, wenn  gilt.

 Wenn die Kante  von der Tiefensuche untersucht wird und  ist grau, so ist  eine Back-Kante
 und  enthält nach Lemma  einen Kreis.
 Wir geben also korrekterweise einen Fehler aus.
 Anderenfalls ist  keine Backkante.
 Ist  weiß, machen wir einen rekursiven DFS-Aufruf auf .
 Dann kann  erst schwarz gefärbt werden, nachdem der rekursive Aufruf zurückgekehrt ist und also 
 schwarz gefärbt wurde.
  Also ist .

  Ist  schwarz, ist  bereits abgearbeitet und der finish-Zeitpunkt  ist bereits gesetzt.
  Der finish-Zeitpunkt von  ist aber noch nicht gesetzt, weil  noch nicht abgearbeitet ist.
  Also muss  sein.

  Die Laufzeit des Verfahrens ist identisch zu der einer Tiefensuche, da das verwalten des Stacks sowie die push()- und pop()-Operationen nur konstanten Aufwand erfordern.

Minimale Spannbäume

Graphen modellieren nicht nur abstrakte Beziehungen sondern können z.B. auch konkrete Computernetzwerke darstellen, in denen jeder Knoten ein Computer und jede Kante eine Netzwerkverbindung ist.
Versetzen wir uns kurz in die Situation, dass wir ein solches Netzwerk planen möchten.
Wir haben bereits potentielle Netzwerkverbindungen identifiziert und möchten nun entscheiden, welche der
Verbindungen wir tatsächlich realisieren möchten.


Unser Ziel ist, dass alle Computer über das Netzerk miteinander verbunden sind, so dass wir z.B. das folgende
Netzwerk realisieren könnten.


Unser Netzwerk ist ein Teilgraph unseres ursprünglichen Graphen und bildet in ihm einen Baum
(es enthält keinen Kreis und ist zusammenhängend).
Ein Subgraph, der ein Baum ist und alle Knoten enthält (verbindet) heißt Spannbaum.
Jeder Spannbaum enthält genau  Kanten.
 Sei  ein ungerichteter Graph.
 Sei  ein zusammenhängender, kreisfreier Teilgraph von .
 Dann enthält  genau  Kanten.

 Wir starten mit einem Hilfsgraphen , der zwar alle Knoten von , aber keine Kanten enthält.
 Nun fügen wir die Kanten  von  in  ein - sei  der Graph
  der durch das Einfügen der ersten  Kanten von  in  entsteht.
 Wir zeigen, dass  genau  Zusammenhangskomponenten besitzt.
 Damit ist die Behauptung gezeigt, denn für  (und für kein kleineres ) besitzt
  dann genau eine einzige Zusammenhangskomponente und ist damit zusammenhängend.

 Induktionsanfang. Für  haben wir noch keine Kante in  eingefügt und jeder
 Knoten bildet seine eigene Zusammenhangskomponente.
 Wir haben also  Zusammenhangskomponenten.

 Induktionsschritt. Betrachten wir ein  und den Graphen .
 Nach Induktionsvoraussetzung hat , also  ohne die Kante , genau 
 Zusammenhangskomponenten.
 Nun fügen wir  ein:
 Nehmen wir an, dass  und  in der gleichen Zusammenhangskomponente von  liegen.
 Dann gibt es einen --Pfad  in , der  nicht enthält (denn  wird
 erst in  eingefügt und ist in  noch nicht enthalten.
 Also ist  ein Kreis in  und das ist ein Widerspruch dazu, dass  ein Baum ist.
 Folglich müssen  und  in unterschiedlichen Zusammenhangskomponenten  bzw.  von  liegen.

 Wir behaupten, dass durch das Einfügen von  diese beiden Komponenten  und  zu einer Zusammenhangskomponente vereinigt werden.
 Sei dazu  und .
 Weil  eine Zusammenhangskomponente ist, muss es in  einen --Pfad  geben.
 Analog muss ein --Pfad  in  existieren.
 Also ist  ein --Pfad in  und  ist dort eine Zusammenhangskomponente.
 Es folgt, dass  genau  Zusammenhangskomponenten besitzt.
 
 Wir werden also immer mindestens  Netzwerkverbindungen benötigen, um unser Netzwerk wie gewünscht zu
 realisieren.
 Dennoch gibt es vielleicht einige Netzwerke, die besser geeignet sind als andere:
 Zum Beispiel ist es denkbar, dass unterschiedliche Verbindungen unterschiedlich teuer zu realisieren sind (vielleicht weil sie ein längeres Kabel oder aufwändige Umbauarbeiten benötigen).
 Wir modellieren diesen Umstand, indem wir die Kanten mit Kosten versehen und bezeichnen die Kosten von Kante  mit .
 Unser Ziel ist es, eine Kantenmenge  so zu finden, dass  ein (kosten-)minimaler Spannbaum (MST) ist.
 Das bedeutet, dass jeder andere Spannbaum mindestens die gleichen Gesamtkosten wie  besitzt.
 Dabei definieren wir die Gesamtkosten eines Spannbaums  als die Summe  der Kosten der in ihm enthaltenen Kanten.
 Am Beispiel des obigen Netzwerkes könnten wir etwa folgende Kantenkosten gewählt haben.


Das folgende Bild zeigt einen Spannbaum mit Kosten .



Wir werden im Folgenden zwei Algorithmen kennenlernen, die minimale Spannbäume berechnen.

Ein abstrakter Algorithmus für minimale Spannbäume

Wir betrachten zunächst einen abstrakten Algorithmus zur Berechnung eines minimalen Spannbaumes auf einem Graphen  mit Kantengewichten .
Später untersuchen wir dann zwei konkrete Implementierungen dieser abstrakten Methode.
Unsere Strategie besteht darin, einen MST nach und nach aufzubauen:
Wir starten mit einer leeren Kantenmenge  und fügen solange geeignete Kanten in  ein, bis  ein Spannbaum ist (also alle Knoten verbunden sind).
Dabei möchten wir zwei Eigenschaften von  als Invarianten erhalten:

 ist in jeder Iteration kreisfrei
 ist in jeder Iteration ein Teilgraph eines minimalen Spannbaumes (d.h. es gibt immer einen MST  mit ).
Dabei nennen wir eine Kante, die wir einfügen können ohne Eigenschaft 2 zu verletzen sicher.
Lemma  sagt uns, dass wir wegen Eigenschaft 1 genau  sichere Kanten einfügen müssen, um einen MST zu erhalten.
Damit haben wir folgenden abstrakten Algorithmus.

 Setze 
 Für :
 Sei  eine sichere Kante bezüglich 
 Füge  in  ein, d.h. setze 
 Gib  aus.
Für  gelten die Eigenschaften 1 und 2 trivialerweise, denn die leere Menge ist eine Teilmenge jeder Kantenmenge und kreisfrei.
Die Schleife erhält Eigenschaft 2, da nur sichere Kanten hinzugefügt werden.
Da jeder Spannbaum per Definition kreisfrei ist und  wegen Eigenschaft 2 eine Teilmenge eines Spannbaums ist, muss  auch kreisfrei sein und Eigenschaft 1 bleibt erhalten, wenn wir eine sichere Kante einfügen.
Stoppt der Algorithmus, so haben wir also mit  einen Spannbaum, der ein Teilgraph eines minimalen Spannbaums ist.
Folglich ist  selbst ein minimaler Spannbaum.

Die Schwierigkeit liegt nun darin, sichere Kanten für  zu identifizieren.
Zumindest wissen wir, dass in jeder Iteration eine sichere Kante für  existieren muss, denn schließlich ist
 ein Teilgraph eines MSTs.
Solange also  noch nicht  Kanten enthält, muss es möglich sein,  zu einem MST zu ergänzen.

Sichere Kanten identifizieren
Betrachten wir

eine Situation, die im Laufe unseres abstrakten Algorithmus auftreten könnte: Wir haben begonnen, einen Spannbaum aufzubauen, aber noch sind nicht alle Knoten verbunden (unser Teilgraph  besteht also aus einem oder mehreren Bäumen).
Jeder dieser Bäume teilt die Knoten des Graphen in zwei Partitionen; diejenigen Knoten, die im Baum enthalten sind, und diejenigen Knoten, die nicht im Baum enthalten sind.
Eine solche Aufteilung der Knotenmenge in zwei beliebige disjunkte Partitionen  und  nennen wir einen Schnitt.
Der Einfachheit halber bezeichnen wir oft auch eine beliebige Menge  als Schnitt und meinen damit den Schnitt .
Wir sagen, dass eine Kante  den Schnitt  kreuzt, wenn sie zu genau einem Knoten in  inzident ist, d.h. wenn sie einen Knoten in  mit einem Knoten in   verbindet (formal: wenn  und  gilt, oder umgekehrt).


Wir möchten jetzt den Zusammenhang zwischen Schnitten und Spannbäumen genauer untersuchen und sagen dazu, dass ein Schnitt  eine Kantenmenge  respektiert, wenn keine Kante aus  den Schnitt  kreuzt.
Außerdem nennen wir eine Kante  leicht bezüglich eines Schnittes , wenn keine Kante, die  kreuzt ein geringeres Kantengewicht als  hat.
 Sei  ein ungerichteter Graph mit Kantengewichten  und sei  ein Teilgraph von , der in einem MST von  enthalten ist.
 Sei  ein beliebiger Schnitt in , der  respektiert.
 Jede Kante, die  kreuzt und bzgl.  leicht ist, ist für  sicher.

Wir betrachten eine beliebige leichte Kante , die  kreuzt.
Nach Voraussetzung gibt es einen MST  von , der  enthält.
Enthält  die Kante , dann ist  sicher und wir sind fertig.

Anderenfalls konstruieren wir aus  einen neuen MST , der  enthält.
Sofern uns das gelingt haben wir gezeigt, dass  sicher ist.
Dies erreichen wir, indem wir eine Kante  aus  entfernen und stattdessen 
in  einfügen.



Da  als Spannbaum zusammenhängend ist muss  einen --Weg  enthalten.
Dieser Weg kann die Kante  nicht enthalten, denn  ist nicht in .
Also bildet  mit  einen Kreis in .
Nun kreuzt aber  den Schnitt , so dass  und  auf unterschiedlichen Seiten von 
liegen müssen.
Es gibt daher auf  mindestens eine Kante , die  kreuzt.
Wenn wir  aus  entfernen, zerfällt  in zwei Komponenten  und  die  bzw. 
enthalten.
Indem wir nun  einfügen verbinden wir die beiden Komponenten wieder und haben
einen Spannbaum .

Wir müssen nun noch nachweisen, dass  ein minimaler Spannbaum ist.
Das liegt daran, dass das Gewicht von  gerade  ist.
Nun kreuzen aber  und  beide den Schnitt  und  ist eine leichte Kante, so dass  gelten muss.
Also ist  und  muss ein minimaler Spannbaum sein.

Der Algorithmus von Kruskal

Der Algorithmus von Kruskal ist eine Implementierung unserer abstrakten MST-Methode.
Er sortiert die Kanten aufsteigend nach ihrem Gewicht und versucht sie in dieser Reihenfolge in die anfangs
leere Kantenmenge  einzufügen.
Im Allgemeinen besteht  also aus mehreren Zusammenhangskomponenten, genauer bildet  einen oder mehrere (Teil-)Bäume in  (einen sogenannten Wald).
Betrachtet der Algorithmus nun eine Kante  gibt es zwei Möglichkeiten: Liegen  und  in zwei unterschiedlichen Zusammenhangskomponenten  und  von , so ist  sicher, denn:

Der Schnitt  respektiert 
Die Kante  kreuzt den Schnitt 
Für jede andere Kante , die  kreuzt muss  gelten; ansonsten stünde  vor  in der Sortierung und wäre bereits eingefügt worden.
Anderenfalls schließt  einen Kreis und wird nicht einfügt.
Da der Algorithmus von Kruskal also nur sichere Kanten einfügt implementiert er unsere abstrakte MST-Methode.

Wie entscheiden wir aber, ob  und  in der gleichen Zusammenhangskomponente von  liegen?
Hier kommt uns die Union-Find-Datenstruktur aus dem vorherigen Kapitel zur Hilfe, denn die Knotenmengen der Zusammenhangskomponenten von  bilden eine Partitionierung von .
Initial ist , so dass jeder Knoten von  eine Zusammenhangskomponenten bildet.
Wir erzeugen also zunächst eine Partition für jeden Knoten .
Wird die Kante  eingefügt, verbinden wir die Zusammenhangskomponente von  mit der von ;
wir müssen also von  und  vereinigen, indem wir union(v,w) aufrufen.
Ob  und  in der gleichen Zusammenhangskomponente liegen erkennen wir nun einfach daran, ob find(v) gleich find(w) ist.
Im Detail haben wir folgenden Algorithmus, der als Eingabe einen ungerichteten Graphen  und
Kantengewichte  erhält.

 Sortiere die Kanten nicht-absteigend nach Gewicht, d.h. so, dass
 *
 gilt.
 Setze .
 Initialisiere  Partionen in einer Union-Find-Datenstruktur 
 Für :
 Betrachte 
 Falls U.find(v) !=  U.find(w):
 Setze 
 U.union(v,w)
 Falls : Stopp, gib  aus.
 Fehler,  ist nicht zusammenhängend.

Der Algorithmus benötigt zunächst Zeit  um die Kanten nach Gewicht zu sortieren.
Anschließend wird im schlimmsten Fall jede Kante einmal betrachtet.
Für jede Kante muss zweimal find ausgeführt werden.
Außerdem wird höchstens  mal union durchgeführt.
Unsere baumbasierte Union-Find-Datenstruktur kann  find Operationen in Zeit  und 
union-Operationen in Zeit  ausführen, so dass wir im Worst-Case eine Gesamtlaufzeit von
*
erhalten. Ist der Eingabegraph zusammenhängend enthält er mindestens  Kanten und die Laufzeit vereinfacht sich zu .









fig-mst-kruskal

Beispieldurchlauf des Algorithmus von Kruskal.

Der Algorithmus von Prim

Auch Prims Algorithmus implementiert unsere abstrakte MST-Methode, indem er iterativ sichere Kanten wählt.
Im Unterschied zur Vorgehensweise im Algorithmus von Kruskal bilden die von Prims Algorithmus gewählten Kanten aber stets einen zusammenhängenden Teilgraphen, d.h., einen Baum.
Der Algorithmus startet mit einem Baum , dessen Knoten- und Kantenmenge wir  bzw.  nennen.
Initial besteht der Baum lediglich aus einem beliebigen Startknoten , d.h. es ist  und .
In jedem Schritt wählt er eine gewichtsminimale Kante, die  erweitert (also zu genau einem Knoten aus  inzident ist).
Diese Kante ist immer sicher, denn:

 Die Knotenmenge  bildet einen Schnitt, der  respektiert.
 Jede Kante, die zu genau einem Knoten aus  inzident ist, kreuzt den Schnitt .
 Die gewählte Kante hat unter allen Kanten, die den Schnitt  kreuzen, minimales Gewicht und ist daher leicht.
Der Algorithmus von Prim berechnet also einen minimalen Spannbaum.

Die Schwierigkeit bei der Implementierung dieses Algorithmus besteht darin, dass wir in jedem Schritt eine gewichtsminimale Kante finden müssen,
die  erweitert.
Dazu verwalten wir alle Knoten, die noch an den Baum angeschlossen werden müssen (d.h., nicht in  liegen) in einem Min-Heap.
Als Schlüssel für Knoten  wählen wir die Entfernung  von  zu :
Das ist das geringste Kantengewicht unter allen Kanten, die  mit  verbinden.
Falls es keine Kante gibt, die  mit  verbindet setzen wir .
Zusätzlich verwalten wir in einem Array  Elternzeiger für jeden Knoten; es ist , wenn  über die Kante  an  angeschlossen wird.
Damit haben wir folgenden Algorithmus, der als Eingabe einen ungerichteten Graphen  und eine Gewichtsfunktion  erwartet.

 Für alle Knoten :
 Setze , ; Setze .
 Wähle einen beliebigen Startknoten , setze .
 Erzeuge einen Min-Heap  aus , verwende  als Priorität von .
 Solange  nicht leer ist:
 v = H.extract-min()
 Setze  auf 
 Für alle Kanten :
 Falls connected[w] == false und :
 Setze , 
  
Nach der Initialisierung entfernt der Algorithmus in jeder Iteration einen Knoten  aus dem Min-Heap und erweitert damit implizit den aktuellen Baum um .
Wir können uns hier vorstellen, dass  und  gesetzt wird.(In der ersten Iteration der Schleife ist  und daher ; hier ist dann .)
Die innere Schleife in 8. iteriert nun über alle Nachbarn  von .
Sie testet mit der if-Bedingung in 9., ob durch die Erweiterung von  um  die Distanz von  zu  sinkt, da  nun über die Kante  mit Gewicht  an  angeschlossen werden kann.
Falls das so ist, aktualisieren wir in 10. und 11. die Distanz von .

Der Algorithmus benötigt Zeit  für die Initialisierung inkl. der Erzeugung des Heaps mit makeheap in Zeit .
Anschließend wird höchstens  mal H.extract-min() aufgerufen, denn danach sind alle Knoten an den Baum angeschlossen und  ist leer.
Im schlimmsten Fall wird jede Kante von  einmal betrachtet, so dass die innere Schleife insgesamt höchstens  Iterationen macht und höchstens  mal H.decrease-key() aufgerufen wird.
Verwenden wir eine binären Min-Heap erreichen wir für extract-min() und decrease-key() jeweils eine Worst-Case-Laufzeit von .
Damit besitzt der Algorithmus eine Gesamtlaufzeit von .
Ist  zusammenhängend, so ist  und die Gesamtlaufzeit beträgt .

fig-mst-prim

Beispieldurchlauf des Algorithmus von Prim auf dem Graphen aus Abbildung . Der Algorithmus berechnet den gleichen MST wie der Algorithmus von Kruskal, wählt allerdings die Kanten in einer
anderen Reihenfolge.
Wir sehen, dass im Gegensatz zum Vorgehen beim Algorithmus von Kruskal hier stets ein zusammenhängender Teilgraph erweitert wird.


Kürzeste Wege mit Kantenlängen

Wir haben bereits in einem vorherigen Abschnitt den Algorithmus Breitensuche kennengelernt, mit dem wir in einem gerichteten Graphen einen Weg zwischen zwei Knoten  und  finden können.
Dabei hatten wir auch nachgewiesen, dass die Breitensuche unter allen --Wegen einen kürzesten Weg findet - also einen, der die geringste Anzahl an Kanten besitzt.
In diesem Kapitel möchten wir unser Modell - und den Algorithmus - auf den Fall verallgemeinern, dass die Kanten mit Längen  versehen sind; wir sind also nicht mehr daran interessiert, die Anzahl an Kanten zu minimieren, sondern suchen einen Weg  mit der geringsten Gesamtlänge  (wir schreiben wie zuvor kurz  für ).
Formal ist ein --Weg ein kürzester Weg, wenn es keinen kürzeren --Weg gibt (also für alle anderen --Wege  gilt, dass ).
Wir schreiben  für die Länge eines kürzesten --Weges bezüglich der Kantenlängen  und setzen wieder , wenn kein --Weg existiert.
Wie bei der Breitensuche suchen wir kürzeste Wege von einem Startknoten  zu allen anderen Knoten.

Teilwege von kürzesten Wegen sind kürzeste Wege

Wir halten zunächst fest, dass Teilwege von kürzesten Wegen ihrerseits auch kürzeste Wege sind.
Diese Eigenschaft hatten wir schon in Kapitel  für ungewichtete Wege gesehen.
 Seien  beliebige Kantengewichte.
 Seien außerdem  zwei beliebige Knoten und sei  ein kürzester --Weg bezüglich .
 Dann ist für alle  der Teilweg  von  nach  ein kürzester --Weg bezüglich .

 Wir betrachten einen beliebigen Knoten  auf  und nehmen an, dass  kein kürzester --Weg ist.
 Diese Annahme führen wir zu einem Widerspruch.
 Dazu stellen wir zunächst fest, dass wir die Gesamtlänge von  schreiben können als , wobei
  der Teilweg von  sei, der von  nach  führt.

 Wenn nun also  kein kürzester --Weg ist, dann gibt es einen echt kürzeren --Weg .
 Wir können  zu einem --Weg  erweitern, indem wir  und  aneinanderhängen.
 Die Länge von  beträgt  und daher ist
  echt kürzer als .
 Das ist aber ein Widerspruch zu der Voraussetzung, das  ein kürzester --Weg ist!
 Es folgt, dass  nicht kürzer als  sein kann und daher ist  wie behauptet ein kürzester --Weg.
 
 Mit dem gleiche Argument können wir nachweisen, dass wir die Länge eines kürzesten Weges auf folgende Weise schreiben können:
 
  Sei  ein kürzester --Weg.
  Dann ist .
 Diese Eigenschaft wird von beiden Kürzesten-Wegen-Algorithmen ausgenutzt, die wir sehen werden.

Negative Kantenlängen und negative Kreise

 Wir haben erlaubt, dass die Kantengewichte/-längen auch negativ sein dürfen.
 Nun kann es passieren, dass  einen gerichteten Kreis  enthält, der negative Gesamtlänge  besitzt:
 Läuft nun für einen Knoten  ein --Weg  über einen Knoten auf  (z.B. weil  auf  liegt), dann existiert kein kürzester --Weg, denn die Länge von  reduziert sich jedesmal um , wenn wir  einmal durchlaufen.
 Wir können also  beliebig kurz machen, indem wir  beliebig oft einfügen.
 Enthält  allerdings keinen solchen negativen Kreis, dann gibt es für jeden Knoten  einen kreisfreien kürzesten --Weg:
 Denn enhält ein kürzester Weg einen Kreis mit nicht-negativen Gesamtkosten, dann können wir den Kreis einfach entfernen ohne den Weg zu verlängern.


Der Algorithmus von Dijkstra

Der Algorithmus von Dijkstra funktioniert nur, wenn alle Kantengewichte/-längen nicht-negativ sind.
Wenn wir uns vorsllten, dass die Kantenlängen tatäschlich natürlich Zahlen sind, können wir eine Kante der Länge  dadurch simulieren, dass wir an ihrer Stelle  Kanten ohne Länge einfügen.
Auf dem resultierenden Hilfsgraphen können wir nun eine Breitensuche ausführen, um kürzeste Wege zu berechnen; allerdings besitzt der Graph  viele Kanten, so dass die Breitensuche nun eine Laufzeit von 
besitzt. Die Zahl  kann im Vergleich zur Anzahl der Kanten im Eingabegraphen beliebig groß sein.
Der Algorithmus von Dijkstra simuliert die Breitensuche auf dem Hilfsgraphen, ohne explizite Kantenkopien zu benötigen. Abbildung  zeigt ein Beispiel.
Dazu verwaltet der Algorithmus (implizit) eine Menge  von Knoten.
In dieser initial leeren Menge sind alle Knoten gespeichert, für die bereits ein kürzester Weg berechnet wurde (sie entsprechen den schwarzen Knoten bei der Breitensuche).
In jeder Iteration wächst die Menge  um genau einen Knoten, so dass wir nach  Iterationen stoppen, denn dann ist .
Zusätzlich verwendet der Algorithmus ein Array , in dem er an Position  die Länge eines kürzesten --Weges berechnet.
Wir zeigen später, dass für alle  also  gilt.
Für die Knoten  speichert der Algorithmus in  lediglich die Länge des kürzesten bisher gefundenen
--Weges. Es gilt also  für alle .
Präziser werden wir sehen, dass für  in  die Länge eines kürzesten --Weges gespeichert ist, der nur die Knoten in  als Zwischenknoten verwendet.
Fügen wir also einen neuen Knoten  zu  hinzu, so testen wir, ob wir nun einen kürzeren Weg zu einem Knoten  finden können, indem wir 
verwenden.










Wie die Breitensuche arbeitet der Algorithmus von Dijsktra die Knoten in der Reihenfolge ihres (in  geschätzten) Abstandes zum Startknoten ab.
Er macht daher  Iterationen und wählt in jeder Iteration einen Knoten in  mit dem kleinsten .
Wir verwalten daher die Knoten in  in einem Min-Heap; Knoten  bekommt Priorität .
Der gewählte Knoten wird nun zu  hinzugefügt.
Anschließend aktualisieren wir den -Wert aller Nachbarn von , die nicht in  liegen.
Wie zuvor speichern wir in einem zusätzlichen Array  an Stelle  den Vorgänger von  auf einem kürzesten --Weg.
Der resultierende Algorithmus ist abgesehen von 9. und 10. identisch zum Algorithmus von Prim.
Er erwartet einen gerichteten Graphen , Kantenlängen 

 Für alle Knoten :
 Setze , ; Setze .
 Setze .
 Erzeuge einen Min-Heap  aus , verwende  als Priorität von .
 Solange  nicht leer ist:
 v = H.extract-min()
 Setze  auf 
 Für alle Kanten :
 Falls connected[w] == false und :
 Setze , 
  

Wir zeigen nun, dass der Algorithmus von Dijkstra korrekt arbeitet.
Dazu weisen wir nach, dass der Algorithmus die folgende Invariante erhält.

 Sei  für alle .
 Dann gilt am Ende der Schleife in 5. stets:
 
  Für alle  (also diejenigen, die nicht im Heap  gespeichert sind): 
  Für alle  (also diejenigen, die im Heap  gespeichert sind): 
  wobei wir  setzen, falls die Kante  nicht existiert.

 Wenn das Ende der Schleife zum ersten Mal erreicht wird, gilt die Invariante.
 Dann ist  und für  ist die  korrekt auf  gesetzt und (1) gilt.
 Da nur  in  enthalten ist, vereinfacht sich die Minimierung in (2) zu
 .
 Ist nun  ein Nachbar von , so setzen wir in 10.  auf  und (2) gilt für .
 Anderenfalls bleibt  gesetzt und (2) gilt ebenfalls.

Betrachten wir nun eine beliebige spätere Iteration der Schleife und zeigen, dass diese Iteration
die Invariante erhält.
Sei  der Knoten, den wir in dieser Iteration aus  entfernen und zu  hinzufügen.
Die Schleife ändert  nur für Knoten in  und für , so dass wir (1) nur
für  zeigen müssen.
Wir zeigen zunächst, dass .
Dazu leiten wir aus (2) ab, dass  für einen Knoten  gilt,
und falls  ist, gibt es also einen --Weg über  mit Länge .
Ein kürzester --Weg kann höchstens kürzer sein und es gilt also .

Analog zum Beweis von Lemma  zeigen wir nun, dass auch  gilt.
Dazu betrachten wir einen kürzesten --Weg  und den letzten Knoten  auf , der in 
liegt.
Dieser Knoten  kann nicht  sein, denn  liegt noch nicht in .
Sie  der Nachfolger von  auf ; es kann auch  sein.
Weil  ein kürzester Weg ist muss auch der Teilweg  von  nach 
ein kürzester Weg sein (Lemma ).
Betrachten wir auch noch den Teilweg  von  nach .
Es gilt
*
Damit haben wir gezeigt, dass Invariante (1) erhalten bleibt.

Invariante (2) wird von den Zeilen 9 und 10 wiederhergestellt, nachdem  zu  hinzugefügt wird.


  fig-dijkstra-example
 
Ein Beispieldurchlauf des Algorithmus von Dijkstra mit Startknoten .Neben jedem Knoten  ist der Wert  notiert.

Aus der Invariante folgt die Korrektheit des Algorithmus, denn wenn der Algorithmus stoppt gilt  und damit haben wir
für jeden Knoten , dass .
Die Laufzeit des Algorithmus von Dijsktra ist identisch zu der des Algorithmus von Prim:
Wir müssen  extract-min und  decrease-key()-Operationen durchführen.
Abgesehen davon haben wir für jeden Knoten und jede Kante nur konstanten Aufwand.
Implementieren wir den Algorithmus also auf Basis eines binären Min-Heaps, so erhalten wir eine Gesamtlaufzeit von .

Kürzeste-Wege-Bäume

Der Algorithmus von Dijkstra berechnet neben den Distanzen  auch Vorgängerkanten im Array ; genauer ist  die Kante, die vor 
auf dem berechneten kürzesten --Weg liegt (oder null, wenn  ist).
Die Vorgängerkanten  bilden einen (gerichteten) Baum mit Wurzel .
Das liegt daran, dass unsere kürzesten Wege frei von gerichteten Kreisen sind.
Da jeder Knoten  genau einen Vorgänger auf einem kürzesten Weg besitzt, besitzt jeder Knoten  genau eine eingehende Kante
und wir haben auch keine ungerichteten Kreise.
Wir können aus dem Kürzesten-Wege-Baum für jeden Knoten einen kürzesten Weg ablesen.



Der Kürzeste-Wege-Baum nach einem Durchlauf des Algorithmus von Dijkstra.

Der Algorithmus von Bellman und Ford

Im Gegensatz zum Algorithmus von Dijkstra kann der Algorithmus von Bellman und Ford auch dann kürzeste Wege berechnen, wenn es Kanten mit
negativen Längen gibt.
Er erkennt außerdem wenn der Eingabegraph einen negativen Kreis enthält, der vom Startknoten aus erreichbar ist.

Der Algorithmus berechnet ebenso wie der Algorithmus von Dijsktra in einem Array  an Position  den Wert  für alle .
Er basiert auf einer einfacheren Operation, die wir Relaxierung nennen: Wir testen für eine beliebige Kante , ob .
Ist das der Fall, setzen wir  mit der Intution, dass wir einen --Weg gefunden haben, der zunächst nach  und dann über
die Kante  nach  läuft und daher die Länge  besitzt.
Wir relaxieren solange Kanten, bis wir in  die korrekten Kürzesten-Wege-Distanzen-Distanzen berechnet haben.
Es stellt sich heraus, dass genau dann der Fall ist, wenn keine Relaxierung mehr erfolgreich ist (also  für jede Kante  gilt).

 Sei  ein gerichteter Graph mit Kantenlängen  und sei  ein beliebiger Startknoten.
 Ferner sei kein negativer Kreis von  aus erreichbar.
 Für alle  sei nun  die Länge (bzgl. ) irgendeines, nicht notwendigerweise kürzesten, --Weges, falls ein solcher Weg existiert
 oder , falls  nicht von  erreichbar ist.
 Dann ist  für alle  genau dann, wenn für alle Kanten  gilt, dass .

 Für die Hinrichtung setzen wir voraus, dass  für alle  gilt.
 Nehmen wir an, dass  für eine Kante  gilt und führen diese Annahme zu einem Widerspruch.
 Nach Voraussetzung ist  die Länge eines kürzesten --Weges .
 Verlängern wir  um die Kante , so erhalten wir einen --Weg der Länge .
 Wegen unserer Annahme ist  also kürzer als ein kürzester --Weg, denn dieser hat Länge .
 Das ist ein Widerspruch.

 Für die Rückrichtung betrachten wir einen beliebigen Knoten .
 Falls  ist, ist  nicht von  aus erreichbar und daher ist auch .
 Anderenfalls gibt es einen kürzesten --Weg  mit  und ,
 weil wir davon ausgehen, dass kein negativer Kreis von  aus erreichbar ist.
 Nach Voraussetzung haben wir die folgenden Ungleichungen für die Kanten  auf .
 *
Wenn wir die Ungleichungen ineinander einsetzen, erhalten wir
*
Da  die Länge eines --Weges ist, muss auch  gelten.

Der Algorithmus von Bellman und Ford durchläuft alle Kanten  mal in einer beliebigen Reihenfolge und relaxiert sie.
Analog zur Vorgehensweise beim Algorithmus von Dijkstra berechnen wir im Array  den
Kürzeste-Wege-Baum (s. auch Abbildung ).
[caption=Algorithmus von Bellman und Ford]
function bellman_ford(g, c, s):
    d = Array(g.nNodes)
    p = Array(g.nNodes)
    for v=0,...,g.nNodes:
        d[v] = infinity
        p[v] = null

    d[s] = 0
    for v = 1,...,g.nNodes-1:
        for e in g.edges:
            if d[e.tgt] > d[e.src] + c[e]
                d[e.tgt] = d[e.src] + c[e]
                p[e.tgt] = e

Wie bei der Breitensuche und dem Algorithmus von Dijsktra überlegen wir uns zunächst, dass der Algorithmus von Bellman und Ford die Längen der
kürzesten Wege höchstens überschätzen kann.
Zu jedem Zeitpunkt während der Ausführung des Algorithmus von Bellman und Ford gilt:
*
Insbesondere wird  sich im Laufe des Algorithmus nicht mehr ändern, wenn einmal  gilt.

 Wir beweisen die Behauptung mittels einer Induktion über die Anzahl der Relaxierungsoperationen.
 Induktionsanfang (). Nach 0 Relaxierungsoperationen ist .
 Für alle anderen Knoten  gilt .

 Induktionsschritt .
 Wir setzen voraus, dass nach  Relaxierungsoperationen  für alle Knoten  gilt
 und zeigen, dass die -te Relaxierungsoperationen diese Bedingung erhält.
 Die -te Relaxierungsoperation relaxiere die Kante .
 Sie kann höchstens den Wert  ändern, so dass wir nur sicherstellen müssen, dass weiterhin 
 gilt.
 Ändert sich  nicht, so gilt  weiterhin nach Induktionsvoraussetzung.
 Anderenfalls wird  gesetzt. Es ist daher
 *
 Hierbei gilt die letzte Ungleichung, weil ein kürzester --Weg höchstens kürzer sein kann als ein kürzester --Weg,
 der um die Kante  verlängert wird.
 Damit haben wir den ersten Teil der Behauptung gezeigt.

Wenn einmal  gilt, kann  nicht weiter fallen, denn .
Andererseits kann  aber auch nicht steigen, da eine Relaxierung  nur durchgeführt wird,
wenn  ist.

Damit können wir jetzt die Korrektheit des Algorithmus zeigen.
Die Intuition ist hier, dass der Algorithmus in der -ten Iteration alle kürzesten Wege korrekt
berechnet hat, die  Kanten enthalten.
Sei  ein gerichteter Graph mit Kantenlängen . Ferner sei kein negativer Kreis von  aus erreichbar.
Dann berechnet der Algorithmus von Ford und Bellman für alle  in  die Länge eines kürzeste --Weges bzgl. .

Wir beobachten zunächst (ohne Beweis), dass jeder Knoten  nur dann einen endlichen Wert  haben kann, wenn er von 
aus erreichbar ist.
Wenn  also von  aus nicht erreichbar ist, ist .

Falls  von  aus erreichbar ist, gibt es nach Voraussetzung einen kürzesten --Weg .
Wir zeigen zunächst per Induktion, dass nach der -ten Iteration der äußeren for-Schleife  gilt.

Induktionsanfang (). Nach der -ten Iteration der Schleife gilt .

Induktionsschritt . Wir gehen davon aus, dass vor der -ten Iteration  ist und zeigen, dass nach
der -ten Iteration dann  gilt.
In der -ten Iteration wird inbesondere die Kante  relaxiert, so dass nach der Iteration 
ist.
Außerdem ist , denn  ist als Teilweg eines kürzester Weges ebenfalls ein kürzester Weg (Lemma ).
Wegen Lemma  gilt auch .

Abschließend müssen wir noch beweisen, dass  Iterationen ausreichend sind.
Dazu erinnern wir uns, dass wenn für  ein kürzester --Weg existiert, wir ebenfalls einen kürzesten --Weg finden können, der keinen Kreis enthält.
Dieser kürzeste Weg enthält höchstens  Knoten und daher höchstens  viele Kanten, denn anderenfalls würde er einen Knoten zweimal besuchen und daher einen Kreis enthalten.
Folglich können wir im obigen Beweis  voraussetzen und  Iterationen sind ausreichend.

Der Algorithmus von Bellman und Ford besitzt also eine Laufzeit von , da wir  mal  Kanten relaxieren und eine Relaxierung konstante Zeit benötigt.



Bellman-Ford mit der lexikografischen Kantenreihenfolge
, , , , , , , , .
Nach der dritten Iteration ergibt sich keine Änderung des -Arrays mehr.

Finden von negativen Kreisen

Abschließend halten wir fest, dass der Algorithmus von Bellman und Ford verwendet werden kann, um zu entscheiden, ob der Eingabegraph  einen negativen Kreis bzgl. der Kantenlängen  enthält.
Dazu wählen wir einen beliebigen Startknoten  und stellen zunächst sicher, dass jeder negative Kreis - sofern überhaupt einer existiert - von  aus erreichbar ist.
Das erreichen wir, indem wir für jeden Knoten  und eine große(Wir können z.B.  wählen.) Zahl  eine Hilfskante  mit  einfügen,
sofern keine Kante  existiert.
Dabei wählen wir  so groß, dass die Hilfskante auf keinem kürzesten Weg enthalten sein kann.
Anschließend führen wir den Algorithmus von Bellman und Ford aus.
Nun testen wir erneut für jede Kante , ob  relaxiert werden kann.
Ist das der Fall, behaupten wir, dass ein negativer Kreis in  existiert.

Zum Nachweis dieser Behauptung zeigen wir, dass nach  Iterationen der äußeren for-Schleife
keine Relaxierung mehr möglich ist, sofern der Eingabegraph  keinen negativen Kreis enthält.
Daraus folgt die Behauptung.
Sei also  frei von negativen Kreisen.
Dann ist in unserem Hilfsgraphen ( zusammen mit den Hilfskanten) auch kein negativer Kreis von  aus erreichbar.
Aus Theorem  erhalten wir also, dass nach Abschluss des Algorithmus von Ford und Fulkerson für alle  gilt, dass  ist.
Wäre also nun die Relaxierung einer Kante  erfolgreich, so würde  unter  fallen und das ist ein Widerspruch zu Lemma .
Kann also nach  Iterationen der äußeren for-Schleife noch eine Relaxierung durchgeführt werden, muss ein negativer Kreis existieren.



Kürzeste Wege zwischen allen Knotenpaaren

Wir haben uns bisher mit dem Problem beschäftigt, einen kürzesten Weg von einem Startknoten zu allen anderen Knoten zu finden.
Zum Abschluss des Kapitels möchten wir nun kürzeste Wege zwischen allen Knotenpaaren berechnen (d.h., wir möchten für jedes Paar  von Knoten einen kürzesten --Weg berechnen).
Dieses Problem heißt auch All Pairs Shortest Paths Problem (APSP).
Dazu könnten wir etwa für jeden Knoten  einmal den Algorithmus von Dijsktra mit Startknoten  ausführen.
Dieses Verfahren besitzt eine Laufzeit von .
Wir sehen nun mit dem Algorithmus von Floyd und Warshall ein Verfahren, dass das Problem in Zeit  löst.
Das Verfahren kann mit negativen Kantenlängen umgehen und erkennt negative Kreise.

Dazu betrachten wir den folgenden Beispielgraphen und stellen uns vor, dass wir bereits kürzeste Wege von Knoten 1 zu allen anderen Knoten berechnet haben.
Nun möchten wir kürzeste Wege von Knoten 2 zu allen anderen Knoten bestimmen.


Im Beispielgraphen gibt es drei Möglichkeiten etwa von Knoten  zu Knoten  zu gelangen:

 Wir können den Knoten  direkt über die Kante  erreichen.
 Wir können zunächst zu Knoten  gehen und dann einen kürzesten --Weg verwenden.
 Wir können zunächst zu Knoten  gehen und dann einen kürzesten --Weg verwenden.
Einen kürzesten --Weg kennen wir bereits aus dem vorherigen Aufruf.
Da Teilwege von kürzesten Wege ebenfalls kürzeste Wege sind, kennen wir möglicherweise auch
einen kürzesten --Weg, nämlich dann, wenn der kürzeste --Weg auch den Knoten  besucht
(in diesem Beispiel ist das der Fall).


Wir erkennen also, dass wir bei der Berechnung Information wiederverwenden können - auch wenn im Allgemeinen
unser Eingabegraph natürlich nicht unbedingt so gutmütig aussieht wie im einführenden Beispiel.

Wir gehen im Folgenden davon aus, dass die Knoten von  von  bis  nummeriert sind,
d.h. dass .
Nun berechnen wir zunächst für alle Knotenpaare  die Länge der direkten Verbindung von  nach  (diese Verbindung habe Länge , wenn keine Kante  existiert).
Anschließend erlauben wir, dass Knoten  als Zwischenknoten verwendet wird und testen, ob sich nun
eine günstigere Verbindung ergibt.
Die Länge des kürzesten --Weges über Knoten  erhalten wir dabei aus den Informationen
der ersten Iteration: Wir addieren die Länge des direkten --Weges zu der Länge des direkten --Weges.
Im obigen Beispiel haben wir eine direkte --Kante mit Kosten 5 und einen (kürzeren) --Weg über den Knoten ,
der sich aus einem --Weg () und einem --Weg ()
zusammensetzt.


Wir gehen im folgenden zunächst davon aus, dass der Eingabegraph keine negativen Kreise enthält.
Allgemein nennen wir  die Länge eines einfachen kürzesten --Weges, der
ausschließlich die Knoten  als Zwischenknoten verwenden darf.
Wir haben
*
denn wenn  ist, dürfen wir keine Zwischenknoten (also nur direkte Kanten) verwenden.
Für  können wir  rekursiv aus  berechnen.
Dazu betrachten wir einen kürzesten --Weg , der ausschließlich Zwischenknoten aus
 verwendet und unterscheiden zwei Fälle:

  verwendet nur Zwischenknoten aus , benutzt den neuen
 Zwischenknoten  also nicht. Dann ist .
 Anderenfalls verwendet  den neuen Zwischenknoten .
 Da Teilwege von kürzesten Wegen selbst kürzeste Wege sind, können wir  zerlegen in einen
 kürzesten --Weg  und einen kürzesten --Weg .
 Da  nach Voraussetzung einfach ist, kann der Zwischenknoten  nicht als Zwischenknoten
 in  und auch nicht als Zwischenknoten in  vorkommen, so dass beide Teilwege
 nur Zwischenknoten aus  verwenden können.
 Wir kennen ihre Länge daher als  bzw. .
Damit haben wir
*
Der Algorithmus von Floyd und Warshall besteht nur darin,  für alle  mit
aufsteigendem  zu berechnen.
Der Algorithmus erwartet die gewichtete Adjanzenmatrix  von .

 Für :
 Für :
 Setze 
 Für :
 Für :
 Für :
 
Nach Abschluss des Algorithmus können wir für jede Wahl von  in  die Länge eines kürzesten --Weges ablesen.

In der obigen abstrakten Implementierung des Algorithmus speichern wir für jedes  gerade  viele Einträge in .
Damit benötigt der Algorithmus einen Speicherplatz von .
Bei genauerer Betrachtung stellt sich aber heraus, dass wir mit Speicherplatz  auskommen können:
Bei der Berechnung von  benutzen wir lediglich , so dass wir  verwerfen können, sobald wir 
für alle  berechnet haben.
Mehr noch: Bei der Berechnung von  verwenden wir für alle  und  nur Einträge der -ten Zeile und -ten Spalte von .
Diese entsprechen genau den kürzesten Pfaden, die bei  starten bzw. enden.
Diese Pfade können  aber nicht als Zwischenknoten verwenden, da sie einfach sind - es folgt, dass  und 
für alle  und für alle  gilt.
Beim Übergang von  nach  ändern sich die Spalte und die Zeile von Knoten  also nicht, und da wir  nur in dieser Spalte und in dieser Zeile
lesen, können wir  mit  überschreiben.
Uns genügt daher ein Speicherplatz von .

In der folgenden Implementierung des Algorithmus gehen wir davon aus, dass der Graph in (gewichteter) Adjazenzmatrixdarstellung gegeben ist.
Der Methode floyd-warshall wird also eine -Matrix übergeben.
Diese Matrix enthält für  an der Position  das Gewicht der Kante , sofern diese Kante existiert oder  anderenfalls.
Außerdem ist  für alle .
Wir beschränken uns zunächst darauf, lediglich die Längen der Kürzesten-Wege zu berechnen ohne die kürzesten Wege selbst
zu speichern.
[caption=Algorithmus von Floyd und Warshall]
 function floyd-warshall(A):
    n = A.nColumns  # Anzahl Spalten = Anzahl Zeilen von A
    # 2-dimensionales n*n Array
    d = Array2d(n, n)
    for v=1,...,n:
        for w=1,...,n:
            d[v][w] = A[v][w]

    for k=1,...,n:
        for v=1,...,n:
            for w=1,...,n:
                d[v][w] = min( d[v][w], d[v][k] + d[k][w] )

    return d
Der Algorithmus besitzt eine (Worst-Case-)Laufzeit von , da wir die Initialisierung in Zeit  durchführen können und
der Aufwand pro Iteration der inneren for-Schleife im Hauptteil konstant ist.
Der Speicherbedarf beträgt .
Wir betrachten abschließend ein Beispiel in Abbildung .
fig-floyd-warshall-example

 Beispieldurchlauf des Algorithmus von Floyd und Warshall. In Iteration  ist
die Menge an erlaubten Zwischenknoten . Abbildung  zeigt eine Iteration des Algorithmus im Detail.


 fig-floyd-warshall-detail
 
 Eine Beispieliteration des Algorithmus von Floyd und Warshall im Detail. Wir betrachten hier , so dass Knoten  als neuer erlaubter Zwischenknoten hinzukommt.
 Im Beispiel ist , d.h. wir suchen Wege von Knoten  zu allen Knoten.

Zusätzliche Berechnung der kürzesten Wege

Wir haben bisher nur die Länge der kürzesten Wege berechnet.
Möchten wir zusätzlich auch den Kürzesten-Wege-Baum jedes Knotens  berechnen, so definieren wir  als
den Vorgängerknoten von  auf einem einfachen kürzesten --Weg,  der nur die Knoten  als Zwischenknoten
benutzen darf.
Für  können wir eine ähnliche rekursive Formel aufstellen wie zuvor:
Für  sind keine Zwischenknoten erlaubt, so dass
*
Für  betrachten wir zwei Fälle: Ist  unterscheidet sich der kürzeste --Weg  mit Zwischenknoten aus 
vom kürzesten --Weg  mit Zwischenknoten aus .
Wir haben  aus einem kürzesten --Weg  und einem kürzesten --Weg  zusammengesetzt, so dass der Vorgängerknoten von  auf 
gerade der Vorgängerknoten von  auf  ist.
Wir setzen also .
Anderenfalls ist  und wir können  setzen.
Wie oben können wir argumentieren, dass wir in Iteration  das Array  überschreiben können, so dass es genügt ein 2-dimensionales Array  mitzuführen.
Der Rumpf der innersten Schleife ändert sich zu:

    if d[v][k] + d[k][w] < d[v][w]:
        d[v][w] = d[v][k] + d[k][w]
        p[v][w] = p[k][w]
Diese Änderung hat keinen Einfluss auf die asymptotische Laufzeit.

Negative Kreise

Wir erinnern uns, dass für jeden Knoten  der Eintrag d[v][v] die Länge eiens kürzesten --Weges beschreibt.
Liegt  nicht auf einem negativen Kreis, so ist die Länge dieses Weges gleich 0.
Liegt anderenfalls  auf einem negativen Kreis , so existiert kein kürzester --Weg, da wir jeden --Weg um  verkürzen können, indem wir einmal  durchlaufen.
In diesem Fall ist  und das erkennen wir daran, dass der Algorithmus von Floyd und Warshall in  einen Wert berechnet, der echt kleiner als Null ist.
Um zu erkennen, dass der Eingabegraph einen negativen Kreis enthält genügt es also zu testen, ob einer der Diagonaleinträge von  negativ ist.
In diesem Fall sind die berechneten Kürzesten-Wege-Distanzen nicht korrekt, da wir davon ausgegangen sind, dass alle kürzesten Wege einfach sind und daher keine Kreise enthalten.


Entwurfsmuster

 Wir haben uns bisher hauptsächlich mit den Schritten 1-3 (Modellierung, Lösung mit bekannten Algorithmen und Abwandlung bekannter Algorithmen) unserer Problemlösungsstrategie aus Kapitel  beschäftigt.
 In diesem Kapitel widmen wir uns Schritt 4: Dem Entwurf neuer Algorithmen mit Hilfe von Entwurfsmustern.
 Tatsächlich haben wir bereits Algorithmen kennengelernt, die auf den drei wichtigsten Entwurfsmustern basieren.
 
  Greedy (Gierige) Algorithmen: Die Algorithmen von Prim, Kruskal und Dijkstra
  Algorithmen, die auf dynamischer Programmierung basieren: Die Algorithmen von Prim und Dijkstra, der Algorithmus von Floyd und Warshall.
  Divide-and-Conquer-Algorithmen: MergeSort und QuickSort.
 Das Entwurfsmuster Divide-and-Conquer haben wir bereits in Kapitel  detailliert betrachtet, so dass wir uns hier auf Greedy-Algorithmen und Dynamische Programmierung konzentrieren werden.

Als Beispielproblem für die Entwicklung eines Algorithmus verwenden wir das Rucksackproblem.
Hier betrachten wir  Objekte, die jeweils ein Gewicht und einen Nutzen haben.
Wir möchten diese Objekte in einen Rucksack packen.
Unglücklicherweise darf der gepackte Rucksack aber ein Maximalgewicht nicht überschreiten, so dass wir im
Allgemeinen nicht alle Objekte einpacken können.
Wir suchen deshalb eine erlaubte Auswahl von Objekten mit größtmöglichem Gesamtnutzen.
Rucksackproblem
  Eine Menge von  Objekten , ein Gewicht  und ein Nutzen  für
  jedes Objekt . Eine Gewichtsschranke 
  Wir nennen eine Auswahl  von Objekten, deren Gesamtgewicht  die
  Gewichtsschranke  nicht überschreitet erlaubt.
  Wir suchen unter den erlaubten Auswahlen von Objekten eine Auswahl  mit maximalem
  Gesamtnutzen .
Wir betrachten zunächst ein Beispiel.

fig-knapsack-example


Greedy-Algorithmen

Eine naheliegende Strategie um unseren Rucksack zu füllen besteht darin, zunächst das Objekt mit dem größten Nutzen zu wählen; dann das Objekt mit dem zweit-größten Nutzen usw., solange bis der Rucksack gefüllt ist.
Objekte, die in den Rucksack nicht (mehr) hineinpassen überspringen wir.
Im einführenden Beispiel wählt dieser Algorithmus zunächst das Objekt 3 mit Nutzen 7 und Gewicht 6.
Damit hat der Rucksack eine Restkapazität von 6, so dass wir nun das Objekt 1 mit Nutzen 6 und Gewicht 4
wählen können.
Die lukrativen Objekte 4 und 6 können wir nicht wählen, da der Rucksack jetzt nur noch eine Restkapazität von 2 besitzt.
Wir wählen daher abschließend Objekt 2.

fig-knapsack-example-2

Unsere Auswahl besitzt ein Gewicht von 12 und einen Nutzen von 15; eine bessere Auswahl hatten wir auch oben nicht gefunden.
Unser Algorithmus trifft in jeder Iteration die Wahl, die lokal - für diese Iteration - am besten ist
heißt gierig oder (auf Englisch) greedy.
Leider lässt sich dieser einfache Algorithmus sehr leicht austricksen.
Betrachten wir die folgende Eingabe mit Gewichtsschranke 10.
Sie enthält ein Objekt mit Gewicht 10 und Nutzen 2, sowie 10 Objekte mit Gewicht 1 und Nutzen 1.

fig-knapsack-example-3


Unser Algorithmus wählt Objekt 11 und danach ist der Rucksack voll.
Diese Auswahl besitzt einen Nutzen von 2.
Eine bessere Auswahl wählt die Objekte , die insgesamt
ein Gewicht von 10 und einen Nutzen von 10 aufweisen.

fig-knapsack-example-4

Hier war unser Algorithmus zu gierig!

Eine bessere Strategie besteht darin, zunächst die Effizienz , d.h. den Nutzen pro Gewicht jedes Objektes zu berechnen.
Anschließend wählen wir greedy Objekte nach größter Effizienz.
Nun kann unser Beispiel den Algorithmus nicht mehr austricksen: Die Objekte 1,,10 besitzen jeweils eine Effizienz von 1, während Objekt 11 nur eine Effizienz von 2/10 besitzt.
Der Algorithmus Greedy nach Effizienz wählt also im obigen Beispiel die Objekte .

Dennoch können wir auch den verbesserten Algorithmus in die Irre führen.
Die folgende Eingabe besitzt ein Objekt mit Nutzen 7 und Gewicht 6, sowie zwei Objekte mit Nutzen 5 und Gewicht 5.
Wir wählen erneut .

fig-knapsack-example-5


Man kann allerdings zeigen, dass Greedy nach Effizienz immer eine Lösung mit Nutzen mindestens   berechnet, wenn es eine Lösung mit Nutzen  gibt. Der Algorithmus lässt sich also nicht beliebig täuschen.

Wir weisen nun nach, dass Greedy nach Effizienz korrekt funktioniert (also eine erlaubte Auswahl mit maximalem Nutzen findet), wenn wir es erlauben, Objekte nur teilweise einpacken.
Wir definieren daher eine Objektauswahl, indem wir für jedes Objekt  eine Zahl  angeben, die uns sagt, welchen Anteil von Objekt  wir in den Rucksack packen (ist  wird  überhaupt nicht eingepackt, ist  wird  vollständig eingepackt).
Wird nun etwa Objekt  zur Hälfte eingepackt (ist also ), so müssen wir auch nur die Hälfte 
des Gewichtes von  tragen, erhalten aber auch nur den halben Nutzen .
Eine Auswahl ist also nun erlaubt, wenn  gilt.
Sie generiert einen Nutzen von .

Rucksackproblem mit teilbaren Objekten
  Eine Menge von  Objekten , ein Gewicht  und ein Nutzen  für
  jedes Objekt . Eine Gewichtsschranke .
  Eine Zahl  für jedes Objekt  so, dass 
  gilt und  maximal ist.
Wir betrachten den Algorithmus Greedy-nach-Effizienz.

 Sortiere die Objekte so, dass
 *
 gilt.
 Setze ,  für alle .
 
 Solange  und :
 Falls :
 Setze  und 
 Sonst:
 Setze  und .
 
 Gib  zurück.
Der Algorithmus Greedy-nach-Effizienz  trifft niemals eine unerlaubte Auswahl: Er stellt sicher, dass in  die Restkapazität des Rucksacks gespeichert ist.
Ist also , so kann Objekt  vollständig eingepackt werden.
Anderenfalls wählt der Algorithmus einen Anteil von  von Objekt ; dieser Anteil
besitzt genau Gewicht  füllt daher den Rucksack.
Der Algorithmus stoppt, wenn die Restkapazität des Rucksacks erschöpft ist.
Wir zeigen nun, dass Greedy-nach-Effizienz auch eine Lösung mit maximalem Nutzen berechnet.
Eine solche Lösung nennen wir optimal.

 Greedy-nach-Effizienz berechnet eine optimale Lösung für das Rucksackproblem mit teilbaren
 Objekten.

Für den Beweis gehen wir davon aus, dass die Objekte bereits nach ihrer Effizienz sortiert sind,
d.h. das  gilt.

Wählt Greedy-nach-Effizienz alle Objekte vollständig (d.h., ist  für alle ),
so ist das optimal, denn eine optimale Lösung kann höchstens eine Teilmenge der gewählten Objekte
einpacken und daher keinen höheren Nutzen als Greedy-nach-Effizienz erzielen.

Anderenfalls ist das Gesamtgewicht  aller Objekte größer als die Gewichtsschranke .
Nach Konstruktion hat unsere Lösung  folgende Struktur:
Ist  das erste Objekt, dass der Algorithmus nicht vollständig gewählt hat, so ist ,
 und  - schließlich wählt der Algorithmus jedes Objekt vollständig,
dass er vollständig wählen kann; hat er ein Objekt nur teilweise gewählt, so stoppt er.
Außerdem ist in diesem Fall der Rucksack vollständig gefüllt, d.h.  (denn der Algorithmus
wählt so viel von  wie möglich).

Betrachten wir jetzt eine beliebige optimale Lösung .
Auch  muss den Rucksack vollständig füllen, denn ist , so können wir ein  erhöhen ohne den Gesamtnutzen von  zu verringern.

Um einzusehen, dass  und  den gleichen Nutzen besitzen, zeigen wir, dass wir  in  umwandeln
können, ohne den Nutzen von  zu verringern.
Sei  das erste Objekt, das  nicht vollständig wählt, d.h. der kleinste Index mit .
Ist nun , so ist  (eine Lösung dieser Form kann von Greedy-nach-Effizienz berechnet werden, und  ist eine erlaubte Auswahl).
Ansonsten gibt es einen Index  mit ; wir wählen das größte solche .
Wir ändern nun , indem wir etwas weniger von Objekt  wählen und dafür etwas mehr von Objekt .
Da die Objekte nach Effizienz sortiert sind und  ist, besitzt  eine größere Effizienz als  und
durch unsere Änderung kann der Nutzen von  nicht sinken.
Diese Konstruktion wiederholen wir solange, bis  gilt.

Der Beweis folgt einer Struktur, die typisch für Korrektheitsbeweise von Greedy-Algorithmen ist: Wir vergleichen die Greedy-Lösung mit einer optimalen Lösung. Sind beide Lösungen gleich, sind wir fertig. Anderenfalls betrachten den ersten Punkt, an dem sich beide Lösungen unterscheiden. Nun zeigen wir, dass wir einen Austausch durchführen können, der die optimale Lösung in die Greedy-Lösung überführt (oder umgekehrt) ohne die Kosten der Lösungen zu ändern. Dabei benutzen wir, dass der Greedy-Algorithmus eine lokal beste Entscheidung getroffen hat.
Dieses Vorgehen nennen wir einen Beweis mit Austauschargument.
Ein solches Austauschargument hatten wir bereits in Theorem  gemacht, das die Grundlage für die Korrektheitsbeweise der Greedy-Algorithmen von Kruskal und von Prim bildet: Hier vergleichen wir einen Spannbaum aus leichten Kanten mit einem minimalen Spannbaum und zeigen, dass wir die Bäume durch das Austauschen von Kanten ineinander überführen können.

Dynamische Programmierung

In diesem Abschnitt werden wir mit dem Entwurfsmuster Dynamische Programmierung einen Algorithmus für das Rucksackproblem entwerfen.










Stellen wir uns kurz vor, dass wir für eine gegebene Eingabe  mit Gewichten , Nutzen  für  und einer Gewichtsschranke  (wir schreiben kurz  für diese Eingabe) schon eine optimale Auswahl  von Objekten kennen.
Enthält  das Objekt , so muss der Teilrucksack mit Gewichtsschranke  von  optimal bepackt werden:

fig-knapsack-example-6

Wir beobachten also, dass in diesem Sinne Teilauswahlen von optimalen Auswahlen ebenfalls optimal sein müssen.
Eine ähnliche Eigenschaft hatten wir bereits bei den kürzesten Wegen gesehen.

Wir unterscheiden nun zwei Möglichkeiten für die optimale Auswahl .

Enthält die optimale Auswahl  das Objekt , so erhalten wir eine optimale Auswahl für die Gewichtsschranke  als .
Umgekehrt liefert jede optimale Auswahl  aus den Objekten  und die reduzierte Gewichtsschranke  eine erlaubte Auswahl  für
unsere ursprüngliche Eingabe.
Enthält  das Objekt  nicht, so bleibt  eine optimale Auswahl, wenn wir Objekt  aus der Eingabe entfernen - also  statt  betrachten.
Umgekehrt ist jede optimale Auswahl von  auch eine erlaubte Auswahl für .
Wir können also eine optimale Auswahl für  konstruieren, indem wir zwei Teillösungen berechnen:

eine optimale Auswahl für .
eine optimale Auswahl für ; diese Auswahl ergänzen wir um Objekt .
Eine von diesen beiden Auswahlen muss optimal für  sein; wir wählen also diejenige mit größerem Nutzen.
Wenn wir also mit  den maximalen erreichbaren Nutzen mit den Objekten  und Gewichtsschranke  bezeichnen erhalten wir in Formeln:
*
Im Allgemeinen kann es passieren, dass  ist; in diesem Fall haben wir die Gewichtsschranke überschritten und legen fest, dass
unsere Auswahl in diesem Fall einen Nutzen von  besitzt (daher setzen wir  falls  ist).
Nun genügt es,  für alle  und  zu berechnen. Wir erhalten den Nutzen der optimalen Auswahl in .

 Mit der Konvention, dass  für :
 Setze  für alle .
 Setze  für alle .
 Für :
 Für :
 Setze 
 Gib  aus.
Möchten wir nicht nur den maximalen Nutzen sondern auch die zugehörige Auswahl von Objekten berechnen, halten wir in einer zusätzlichen Variablen  nach, ob die optimale Auswahl aus  und Gewichtsschranke  das Objekt  enthält (d.h. wir setzen  falls  und  sonst).

Abschließend halten wir fest, dass unser Algorithmus eine Laufzeit von  besitzt.
Er ist also schnell, wenn  klein ist.

Das Vorgehen in unserem Entwurf ist typisch für das Entwurfsmuster dynamische Programmierung:

 Wir untersuchen den Aufbau einer optimalen Lösung; z.B. zeigen wir, dass eine optimale Lösung aus optimalen Teillösungen besteht.
 Wir geben eine Rekursionsgleichung (Bellmansche Optimalitätsgleichung) für den Wert einer optimalen Lösung an.
 Wir berechnen systematisch eine Lösung für die Rekursionsgleichung.
Wir haben dieses Vorgehen bereits beim Algorithmus von Floyd und Warshall gesehen.


Ausblick: Komplexitätstheorie


In diesem Kapitel besprechen wir ein Thema, das eigentlich in der Vorlesung Theoretische Informatik verankert ist und dort ausführlich besprochen werden wird. Wir geben hier den folgenden Ausblick aus folgenden Gründen: a) Das Thema is so wichtig, dass eine zweifache Erwähnung innerhalb der Pflichtvorlesungen gerechtfertigt ist und b) Wir möchten verdeutlichen, dass Algorithmik und Komplexitätstheorie (das ist das Thema dieses Kapitels) untrennbar miteinander verwobene Themengebiete sind. Während die Algorithmik konstruktive Ergebnisse liefert (es gibt einen Algorithmus für das Sortierproblem, welcher korrekt ist und Laufzeit  hat), untersucht die Komplexitätstheorie, was die Grenzen der effizienten Lösbarkeit von algorithmischen Problemen sind. Für erfolgreiche algorithmische Arbeit braucht man immer beides: Kenntnis von Algorithmen und Geschick im Entwurf von Algorithmen einerseits, aber andererseits eben auch Wissen darüber, wann es eher aussichtslos ist, nach einem schnelleren Algorithmus zu suchen.

Am liebsten würden wir für jedes Problem genau wissen, welche asymptotische Laufzeit im Worst-Case wir erreichen können. Für vergleichsbasierte Sortieralgorithmen wissen wir zum Beispiel: Es gibt Algorithmen, die Laufzeit  erreichen (zum Beispiel das in Kapitel  beschriebene MergeSort), und zumindest vergleichsbasierte Algorithmen, die das Sortierproblem lösen, brauchen auch mindestens Laufzeit  (das steht in Theorem  in Kapitel ).

Es sei direkt bemerkt, dass wir eine solch glückliche Situation in diesem Kapitel nicht vorfinden werden. Dieses Kapitel beschäftigt sich mit einer Menge von vermutlich sehr schweren algorithmischen Problemen. Viele Informatiker:innen vermuten, dass es für diese Probleme keine schnellen Algorithmen gibt. Warum das so ist, werden wir in diesem Kapitel besprechen.

Polynomielle Laufzeiten
In Abschnitt  lernen wir Probleme kennen, die vermutlich schwer zu lösen sind. Was aber soll das eigentlich bedeuten, schwer zu lösen? Das besprechen wir hier.

Ein vermutlich schweres Problem kennen wir tatsächlich bereits aus dem letzten Kapitel: Das Rucksackproblem. Wir wissen, dass wir für eine Eingabe  mit , Gewichten , Nutzen  für  und einer Gewichtsschranke  das Rucksackproblem in Zeit  lösen können. Der Algorithmus ist sogar recht schlank und schnell implementierbar. Wo ist also das Problem?

Das Problem liegt darin, dass die Laufzeit den Faktor  enthält. Um zu verstehen, warum dies problematisch ist, schauen wir uns die Eingabe des Problems an und überlegen uns, wie lang diese eigentlich ist. Wir überlegen uns, dass eine Zahl   durch  Bits dargestellt werden kann und rechnen aus, wie lang eine Eingabe für das Rucksackproblem ist. Dabei abstrahieren wir von konstanten Faktoren und veranschlagen für eine Zahl  als Länge von   (dass  nicht notwendigerweise eine ganze Zahl ist, ignorieren wir).







Wie lang ist also die Eingabe ? Die Menge  können wir durch Angabe von  darstellen. Danach müssen wir die  Gewichte und die  Nutzenwerte in die Eingabe einfügen, und das Gesamtgewicht , welches eine einzelne Zahl ist. Die Länge dieser Eingabe ist im logarithmischen Kostenmaß also gegeben durch
*
wobei  und  die jeweils größten vorkommenden Gewichts- und Nutzenwerte seien.
Das sieht kompliziert aus, aber uns interessiert vor allem eines: Wenn wir die Gewichtsschranke  erhöhen, dann steigt die Laufzeit(Es sei angemerkt, dass wir bei der Analyse der Laufzeit in dieser Vorlesung generell das uniforme Kostenmaß verwenden. Das bedeutet, dass wir ignorieren, dass Operationen länger dauern, wenn die involvierten Zahlen größer sind. Eigentlich sollten wir die Laufzeit hier konsequenterweise im logarithmischen Kostenmaß angeben. Dadurch wird sie aber nur größer, so dass wir das an dieser Stelle ignorieren werden und nicht weiter auf die Unterscheidung eingehen.) unseres Algorithmus linear an, während die Eingabelänge nur logarithmisch wächst. Anders ausgedrückt: Die Laufzeit unseres Algorithmus ist exponentiell in der Eingabelänge, weil nämlich  ist und unsere Laufzeit den Term  enthält. Wenn man die Länge der Eingabe mit  bezeichnet, ist die Laufzeit also .

Man kann auch ganz praktisch beobachten, welchen Effekt dies hat, wenn man eine Implementierung des Algorithmus testet. Es ist sehr einfach, die Laufzeit erheblich zu steigern, indem man  erhöht. Wir können problemlos  auf  setzen (und vielleicht auch einige Gewichtswerte entsprechend erhöhen) und dann unser Programm starten, um unseren Rechner für einige Zeit außer Gefecht zu setzen. Nun stelle man sich zum Vergleich vor, man wolle denselben Effekt auf die Laufzeit erreichen, indem man die Anzahl der Objekte auf  erhöht. Dafür muss man eine Menge Zahlen in die Eingabedatei schreiben! Bezüglich der Anzahl  der Objekte (also wenn man den Faktor  einfach ignoriert und nur  anschaut) ist unser Algorithmus nämlich polynomiell:  kommt zwar in der Laufzeit vor, ist aber auch als Faktor in der Eingabelänge enthalten.


Um noch eine weitere Anschauung zu liefern, warum wir polynomielle Laufzeiten anstreben, schauen wir uns die Entwicklung der Funktionen  und  an. Beide Funktionen hätte man lieber nicht als Laufzeit eines Algorithmus:  ist für praktische Anwendungen ganz schön langsam, und  ist exponentiell. Intuitiv ist es aber verlockend, zu glauben,  sei nicht so schlimm, weil  so klein ist. Die folgende Tabelle zeigt, dass das nicht so ist, und veranschaulicht, warum wir  bevorzugen.




Man kann sich überlegen, dass für jede Wahl zweier Konstanten  und  die Funktion  asymptotisch schneller wächst als die Funktion . Dies motiviert die Unterscheidung zwischen polynomiellen Laufzeiten und nicht-polynomiellen Laufzeiten.


 Wir sagen, dass die Laufzeit  eines Algorithmus polynomiell beschränkt ist, wenn es eine Konstante  gibt, so dass  gilt. Hierbei bezeichnet  die Eingabelänge. Mit einem polynomiellen Algorithmus meinen wir im Folgenden einen Algorithmus mit polynomiell beschränkter Laufzeit.

 Als exponentiell beschränkt bezeichnen wir die Laufzeit, wenn es eine Konstante  gibt, so dass  gilt.

Mit Ausnahme des Algorithmus für das Rucksackproblem haben alle in dieser Vorlesung besprochenen Algorithmen eine polynomiell beschränkte Laufzeit. Wenn man das Rucksackproblem einschränkt, indem man fordert, dass  klein sein muss - zum Beispiel maximal  - dann wird auch der Algorithmus für das Rucksackproblem polynomiell. Tatsächlich ist das Rucksackproblem nur (vermutlich) schwierig, wenn  beliebig groß sein darf.

Die Komplexitätsklassen P und EXP

Die wichtigsten Klassen aus der Komplexitätstheorie sind für Entscheidungsprobleme definiert. Dies sind Probleme, bei denen die Antwort nur ja oder nein lauten kann. Wir können zum Beispiel folgende Entscheidungsvariante des Rucksackproblems definieren.

Entscheidungsvariante des ganzzahligen Rucksackproblems (KP)
  Eine Menge von  Objekten , ein Gewicht  und ein Nutzen  für  jedes Objekt , eine Gewichtsschranke  und ein Mindestnutzenwert .
  Eine Auswahl  so, dass 
  gilt und .

Dieses Problem ist sicherlich nicht schwerer als das normale ganzzahlige Rucksackproblem. Wir nennen das Problem KP (für knapsack) und meinen damit immer die Entscheidungsvariante.
Auf ähnliche Weise können wir Entscheidungsvarianten für andere in der Vorlesung besprochene Probleme definieren, zum Beispiel:

 Gegeben einen ungerichteten ungewichteten Graphen , ist  zusammenhängend?
 Gegeben einen ungerichteten gewichteten Graphen  mit Gewichten  und eine Zahl , gibt es einen Spannbaum mit Kosten ?
Wir können Breitensuche oder Tiefensuche verwenden, um die erste Frage zu beantworten, und den Algorithmus von Kruskal oder Prim, um die zweite Frage zu beantworten. Wir wissen also, dass diese Entscheidungsprobleme in polynomieller Zeit lösbar sind. Damit sind diese beiden Probleme in der Komplexitätsklasse .


 Ein Entscheidungsproblem ist in der Komplexitätsklasse P, wenn es einen Algorithmus gibt, der das Problem korrekt löst und dessen Laufzeit polynomiell beschränkt ist.

Für die Entscheidungsvariante des Rucksackproblems wissen wir nicht, ob es in P liegt (aber wir vermuten, dass es nicht in P liegt). Klar ist, dass es in der folgenden Klasse EXP liegt.


 Ein Entscheidungsproblem ist in der Komplexitätsklasse EXP, wenn es einen Algorithmus gibt, der das Problem korrekt löst und dessen Laufzeit exponentiell beschränkt ist.

Um zu argumentieren, dass die Entscheidungsvariante des Rucksackproblems in EXP liegt, reicht es, sich zu überlegen, das man alle möglichen Teilmengen der Objektmenge nacheinander aufzählen kann, um herauszufinden, ob eine davon beide Schranken erfüllt. Es sind  verschiedene Teilmengen (wenn man die leere Menge mitzählt), und die Überprüfung, ob eine Teilmenge die Schranken erfüllt, benötigt  Zeit. Der Algorithmus hat also Laufzeit , was exponentiell beschränkt ist.
Alternativ kann man auch mit der Laufzeit  des uns bereits bekannten Algorithmus argumentieren, dann muss man ähnlich zu unseren obigen Überlegungen darauf eingehen, wie sich  zur Eingabelänge verhält.

Ein Beispiel für ein Problem, von dem wir wissen, dass es in EXP und nicht in P liegt, ist eine  verallgemeinerte Version von Schach.
Das Brettspiel Schach ist seit vielen Jahrhunderten bekannt und wird landläufig als schwierig empfunden. Es eignet sich gut als illustratives Beispiel. Um formal Aussagen über die asymptotische Laufzeit von Algorithmen treffen zu können, muss man Schach zunächst für variable Spielbrettgrößen definieren. Wir verweisen dazu auf die vollständige Definition in  und stellen uns vor, dass das Problem sinnvoll verallgemeinert ist und das uns bekannte Spiel auf einem -Brett als Spezialfall enthält.

-Schach (informelle Definition)
 Eine Zahl , eine Spielsituation auf einem -Schachfeld.
 Ja, wenn es eine Gewinnstrategie für Weiß gibt, d.h. dass unabhängig von den Zügen des anderen Spielers Weiß einen Sieg erzwingen kann.

 Nein sonst.

Für die in  definierte verallgemeinerte Version von Schach wird in  gezeigt, dass das Problem in EXP ist und nicht in P liegt. Abbildung  veranschaulicht die Komplexitätsklassen P und EXP. P ist eine Teilmenge von EXP, da jede polynomiell beschränkte Laufzeit auch exponentiell beschränkt ist. Innerhalb von P liegt zum Beispiel das Problem, zu entscheiden, ob ein Graph zusammenhängend ist. In EXP, aber außerhalb von P, liegt das verallgemeinerte Schachproblem. Das Rucksackproblem können wir noch nicht einzeichnen - wir wissen nicht, ob es innerhalb oder außerhalb von P liegt.



Die Klassen P und EXP.

Die Klasse NP

Das Rucksackproblem gehört zu einer Klasse von Problemen, von der wir glauben, dass sie mehr Probleme als P und weniger Probleme als EXP enthält. Dies ist aber nicht bewiesen.
Diese Klasse heißt NP. Die Abkürzung steht für nichtdeterministisch polynomiell, denn in NP sind alle Probleme, für die es nichtdeterministische Algorithmen gibt, deren Laufzeit polynomiell beschränkt ist. Wir verweisen für die Definition und Erklärung von Nichtdeterminismus auf die Vorlesung
Theoretische Informatik bzw. entsprechende Fachliteratur und verwenden eine alternative und etwas vereinfachte Definition von NP.


 Ein Problem  ist in der Komplexitätsklasse NP, wenn es einen Algorithmus  mit polynomiell beschränkter Laufzeit gibt, welcher als Eingabe Tupel  erhält, so dass
 
  für alle Eingaben  des Problems , für die die richtige Antwort Nein lautet,  jedes Tupel  ablehnt (egal, was für ein  angehängt wird), und
  für alle Eingaben  des Problems , für die die richtige Antwort Ja lautet, ein  existiert, dessen Länge polynomiell beschränkt ist in der Länge von , und für das  von  akzeptiert wird.
 
Den Algorithmus nennen wir auch Verifizierer, denn wir stellen uns vor, dass in einem Tupel  der hintere Teil, also , ein Beweis dafür sein soll, dass die richtige Antwort für Eingabe  Ja lautet. Wenn die richtige Antwort Nein ist, kann es einen solchen Beweis nicht geben. Wenn die richtige Antwort aber Ja lautet, so verlangen wir, dass es mindestens einen gültigen Beweis gibt, für den  das Tupel  tatsächlich akzeptiert.

Wir bemerken, dass in der Definition zwar ein polynomieller Algorithmus verlangt wird - dieser soll aber nicht das eigentliche Problem  lösen. Hilft uns dies überhaupt weiter, bzw. wenn ein Problem diese Definition erfüllt und somit in NP ist, liegt das Problem dann auch automatisch in EXP? Dies ist der Fall, denn wir können, grob gesprochen, alle Beweise  ausprobieren, um herauszufinden, ob es einen gültigen Beweis gibt. Da die Länge von  polynomiell beschränkt sein muss, gibt es nur exponentiell viele verschiedene Beweise, die wir betrachten müssen. Diese Argumentation ist leider etwas vage, was daran liegt, dass bereits unsere Definition von NP nicht formal vollständig ist. Ein formaler Beweis verfolgt aber im Kern diese Argumentation.

Die Definition mag kompliziert erscheinen, doch häufig sind die verlangten Beweise einfach zu erklären. Schauen wir uns zum Beispiel das Rucksackproblem an. Wenn wir beweisen wollen, das es eine Auswahl von Objekten gibt, deren Gewicht höchstens  und deren Nutzen mindestens  ist, wie könnte ein geeigneter Beweis aussehen? Ein geeigneter Beweis ist einfach eine geeignete Auswahl von Objekten. Eine solche Auswahl können wir sicherlich mit polynomieller Größe beschreiben, und es kann auch leicht verifiziert werden, ob die Schranken eingehalten werden. Für viele Probleme können wir einfach die Lösung als Beweis verwenden. Die obige Definition verlangt ja nur, dass es einen solchen Beweis im Ja-Fall geben muss - nicht, dass man ihn auch schnell berechnen kann. Es ist auch klar, dass es im Nein-Fall keinen Beweis gibt - denn dann gibt es ja eben keine Lösung, die die Schranken einhält.

Wir kennen nun also eine Klasse von Problemen, von der wir wissen, dass sie P enthält, und dass sie in NP enthalten ist. Was wir nicht wissen, ist, ob P und NP wirklich verschieden sind. Da Schach in EXP, aber nicht in P liegt, wissen wir, dass zumindest nicht alle drei Klassen identisch sein können. Sehr viele Informatiker:innen glauben, dass alle drei Inklusionen echt sind, dass also in NP mehr Probleme liegen als in P, und in EXP mehr Probleme als in P.



Die Klassen P, NP und EXP.

Weitere Probleme in NP
Abbildung  zeigt eine Veranschaulichung der Klassen P, NP und EXP. Dabei sind die Kreise um P und NP gestrichelt gezeichnet, da nicht bewiesen ist, dass P und NP nicht diesselbe Klasse sind. Die Frage, ob P und NP verschieden sind, ist sehr berühmt und z.B. eins der zehn Millenium-Probleme, für deren Lösung das Clay-Institut in New Hampshire (USA) einen Preis von einer Million Dollar ausgeschrieben hat. In der Zeichnung sind noch zwei weitere Probleme eingezeichnet, die in der Algorithmik un der Komplexitätstheorie wichtig sind und die wir im Folgenden zusammen mit Varianten kennenlernen werden.


Erfüllbarkeitsprobleme
Wir erinnern uns an boolesche Funktionen. Eine boolesche Funktion bildet  Bits auf ein einzelnes Bit ab. Man kann jede boolsche Funktion durch einen booleschen Ausdruck darstellen, der aus den Eingabevariablen, Negationen, UND-Verknüpfungen und ODER-Verknüpfungen besteht. An eine solche Darstellung denken wir, wenn es darum geht, eine boolesche Funktion als Eingabe eines Problems zu kodieren.
Das Urgestein der schwierigen Probleme im Sinne dieses Kapitels ist das sogenannte Erfüllbarkeitsproblem.

Erfüllbarkeitsproblem (SAT)
  Eine boolesche Funktion .
  Die Ausgabe ist
    
     ja, wenn es eine Belegung  gibt, so dass  ist, und
     nein sonst, also wenn für alle Belegungen  immer  gilt.
    
Es gibt viele Algorithmen für das Erfüllbarkeitsproblem und Varianten des Erfüllbarkeitsproblems. Einen polynomiellen Algorithmus kennen wir allerdings nicht. Wir können uns leicht einen Algorithmus mit exponentieller Laufzeit überlegen: Wir testen alle  Möglichkeiten, wie die  Variablen gesetzt werden können, und überprüfen für jede dieser Möglichkeiten, ob  erfüllt ist. Wenn die Überprüfung für eine Belegung Zeit  benötigt, dann hat dieser Algorithmus die Laufzeit . Dabei ist  linear in der Eingabelänge. Wir überlegen uns dies für das folgende eingeschränkte Erfüllbarkeitsproblem, das uns mehr beschäftigen wird.




-SAT
  Eine boolesche Funktionen  in konjunktiver Normalform, wobei jede Klausel aus maximal drei Literalen besteht.
  Die Ausgabe ist
    
     ja, wenn es eine Belegung  gibt, so dass  ist, und
     nein sonst, also wenn für alle Belegungen  immer  gilt.
    
Eingaben für -SAT sind also ganz spezielle boolesche Funktionen, nämlich solche, die sich als Konjunktion von Diskunktionen darstellen lassen (das geht für alle Funktionen), wobei die Disjunktionen nur aus einem, zwei oder drei Literalen bestehen dürfen. Ein Beispiel für eine solche Funktion ist:



Die Disjunktionen werden auch Klauseln genannt, und häufig wird die Anzahl der Klauseln in der Eingabe mit  bezeichnet. Eine Eingabe für -SAT besteht also aus zwei Zahlen  und  Klauseln. Eine Klausel kann man durch Angabe der bis zu drei beteiligten Variablen und der Information, welche davon negiert sind, darstellen. Die Eingabelänge ist ist dann . Zur Überprüfung, ob eine Belegung die Formel erfüllt, geht man die Klauseln nacheinander durch und überprüft, ob eines der bis zu drei angegebenen Literale der Klausel von der Belegung erfüllt wird. Ist dies nicht so, kann abgebrochen werden; die Belegung erfüllt die Formel nicht. Wurden alle  Klauseln überprüft und sind erfüllt, so ist die Belegung erfüllend. Die Laufzeit dieses Algorithmus ist  und damit erhalten wir für unseren obigen Algorithmus eine Laufzeit von .

Tourprobleme

Das Hamiltonkreisproblem besteht darin, zu entscheiden, ob in einem Graphen eine Tour enthalten ist, d.h. ein einfacher Kreis, der jeden Knoten genau einmal enthält. Figure  zeigt einen Graphen (links) sowie zwei in ihm enthaltene Hamiltonkreise. Figure  zeigt einen Graphen, der keinen Hamiltonkreis enthält. Dies kann man durch geschicktes Ausprobieren verschiedener Möglichkeiten, einen Kreis zu bilden, nachweisen. Der Graph in Figure  ist auch als Petersen-Graph bekannt.



Ein Graph und zwei in ihm enthaltene Hamiltonkreise.


Hamiltonkreisproblem
  Ein ungerichteter, ungewichteter Graph .
  Die Ausgabe ist
    
     ja, wenn  eine Tour enthält,
     nein sonst.
    
Das Hamiltonkreisproblem ist in NP: Genau für die Graphen, die einen Hamiltonkreis besitzen, kann dies durch Angabe eines ebensolchen Hamiltonkreises bewiesen werden.



Der Petersen-Graph. Dieser enthält keinen Hamiltonkreis.

Bekannter als das Hamiltonkreisproblem ist das Problem des/der Handlungsreisenden (Travelling Salesperson Problem). Gegeben ist hier ein gewichteter Graph. Jede Tour im Graphen hat jetzt Kosten, nämlich die Summe der Gewichte der enthaltenen Kanten.

Problem des/der Handlungsreisenden (TSP)
  Ein ungerichteter Graph , Gewichte , und .
  Die Ausgabe ist
    
     ja, wenn  eine Tour enthält, deren Kosten maximal  sind, und
     nein sonst.
    
Auch für TSP können wir als Beweis eine Tour angeben, nur dass diese nun die entsprechenden Maximalkosten einhalten muss. Auch TSP ist also in NP.

Polynomielle Reduktionen

 Reduktionen bieten eine Möglichkeit, Ergebnisse über die Komplexität eines Problems auf ein anderes zu übertragen. Polynomielle Reduktionen sind in polynomieller Zeit berechenbar und erfüllen die folgende recht strikte Eigenschaft.


Es seien  und  zwei algorithmische Probleme und  und  die Mengen aller möglichen Eingaben für  und .
Eine polynomielle Reduktion von  auf  ist eine Abbildung , die in polynomieller Zeit berechnet werden kann und die für alle   erfüllt, dass  genau dann eine Ja-Eingabe für  ist, wenn  eine Ja-Eingabe für  ist.

Existiert eine solche Reduktion, so heißt  auf  polynomiell reduzierbar und wir schreiben .

Wir überlegen uns zuerst ein ganz einfaches Beispiel. Sei Problem  das Entscheidungsproblem, welches für ein gegebenes Array  daraus besteht, zu entscheiden, ob  aufsteigend sortiert ist. Sei Problem  das Entscheidungsproblem, welches für ein gegebenes Array  daraus besteht, zu entscheiden, ob  absteigend sortiert ist. Wir stellen uns jetzt vor, dass wir einen Algorithmus  besitzen, der Problem  löst - zum Beispiel eine Software-Bibliothek, die wir in unserem Code einbinden können. Wir wollen aber Problem  lösen. Wir multiplizieren dazu alle Zahlen in  mit  und nennen das entstehende Array . War  zuvor aufsteigend sortiert, so ist  absteigend sortiert. War  nicht aufsteigend sortiert, so ist  auch nicht absteigend sortiert. Wir können also  mit Eingabe  starten und das Ergebnis übernehmen. Außerdem ist  in polynomieller Zeit aus  berechenbar. Wir habne also eine polynomielle Reduktion beschrieben.

Allerdings bringt uns diese Reduktion keinen großen Erkenntnisgewinn bezüglich der Komplexität der beteiligten Probleme - die Frage, ob ein Array aufsteigend sortiert ist, können wir sowieso in polynomieller Zeit beantworten. Wir benötigen dafür eigentlich keine Software-Bibliothek und keinen Algorithmus .

Polynomielle Reduktionen sind besonders spannend, wenn wir die Komplexität eines Problems noch nicht kennen. Könnten wir zum Beispiel zeigen, dass die Entscheidungsvariante des Rucksackproblems (KP) polynomiell reduzierbar ist auf das Zusammenhangsproblem, so hätten wir direkt einen polynomiellen Algorithmus für KP gefunden: Wir könnten dann jede Eingabe umwandeln, mit Breitensuche lösen und das Ergebnis zurückmelden. Eine solche polynomielle Reduktion werden wir jedoch wahrscheinlich nicht finden, da viele Wissenschaftler:innen davon ausgehen, dass KP nicht in polynomieller Zeit lösbar ist.

Finden wir hingegen eine Reduktion von einem Problem, das uns schwierig erscheint, auf KP, dann ist dies ein Indiz dafür, dass das Rucksackproblem schwierig ist. Wir betrachten die folgende polynomielle Reduktion (aus , Satz 3.4.3).

 



 Wir müssen eine Abbildung aller gültigen -SAT-Eingaben auf Eingaben für das Rucksackproblem finden, wobei die Abbildung polynomiell sein muss und wir erreichen müssen, dass Ja-Eingaben auf Ja-Eingaben abgebildet werden und Nein-Eingaben auf Nein-Eingaben.

 Sei also eine Eingabe für -SAT gegeben. Diese besteht aus  Klauseln  und die Klauseln haben die Form  für . Als Beispiel können wir uns an die oben verwendete Formel  erinnern.









 Wir wandeln die -SAT Eingabe jetzt in eine Eingabe für KP mit sehr großen Zahlen um.
 Die Eingabe für KP verwendet  und unterscheidet auch bei den Objekten nicht zwischen Nutzen und Gewicht. Die Zahl  hat  Stellen. Die ersten  Stellen sind alle , die  hinteren Stellen sind alle . In unserem Beispiel haben wir drei Klauseln und drei Variablen, so dass wir  erhalten. Wir bemerken, dass  schnell groß wird.

Wir definieren vier verschiedene Arten von Zahlen,  Zahlen ,  Zahlen ,  Zahlen  und  Zahlen . Es gibt dann in der KP-Instanz  Objekte. Die ersten  Objekte haben Gewichts- und Nutzenwerte , und so weiter.

 Die erste Art von Zahlen beziehen sich auf positive Literale in der -SAT-Instanz. Der Wert von  hängt davon ab, in welchen Klauseln  vorkommt. Auch die Zahlen  hat Länge . In den ersten  Stellen ist Stelle  genau dann , wenn  in Klausel  vorkommt. Die hinteren  Stellen von  sind fast alle . Nur die . Stelle ist  (dies ist die . Stelle, wenn man die vorderen  Stellen mitzählt). Die Zahl  ist ganz ähnlich aufgebaut, nur dass sie von negativen Literalen abhängt. Der Wert von  hängt davon ab, in welchen Klauseln  vorkommt. In den ersten  Stellen ist Stelle  genau dann , wenn  in Klausel  vorkommt. Im hinteren Teil ist wieder genau die . Stelle .

Der Sinn der Zahlen von Typ  und  wird gleich klar werden. Die Zahl  besteht nur aus Nullen bis auf Stelle  im vorderen Block, diese Stelle ist . Die Zahl  besteht ebenfalls nur aus Nullen bis auf Stelle  im vorderen Block, diese Stelle ist . In unserem Beispiel erhalten wir also die folgenden Zahlen, wobei in der Mitte zur besseren Lesbarkeit ein Abstand eingefügt ist, den die Zahl natürlich eigentlich nicht enthält.

*

Damit haben wir die KP-Instanz vollständig beschrieben. Sie kann in polynomieller Zeit berechnet werden: Für das Aufstellen von  müssen wir nur  und  kennen, und für das Aufstellen der  Zahlen müssen wir nur prüfen, welche Literale in welchen Klauseln vorkommen.

Wir nehmen nun an, dass eine Ja-Instanz für -SAT vorliegt. Dann müssen wir zeigen, dass die von uns definierte Instanz eine Ja-Instanz für KP ist. Wir bemerken zunächst, dass dies bedeutet, dass wir nachweisen müssen, dass sich  aus den  Zahlen bilden lässt. Genau dann ist die KP-Instanz lösbar, da wir ja  definiert haben. Da die -SAT-Instanz eine Ja-Instanz ist, gibt es eine erfüllende Belegung der Variablen. Wir wählen aus  genau dann  aus, wenn  in der erfüllenden Belegung auf  gesetzt ist, und sonst wählen wir  aus. Auf diese Weise wählen wir  Zahlen (bzw. Objekte) aus. Wir stellen fest, dass die Summe der von uns ausgewählten Zahlen im hinteren Bereich exakt  ergibt, weil wir für jede Stelle genau eine Zahl ausgewählt haben, die an dieser Stelle  ist. Im vorderen Teil können die Ziffern ,  oder  auftreten. Es kann nicht sein, dass in der Summe an einer Stelle mehr als  auftritt, weil das bedeuten würde, dass mehr als  der Zahlen  an Position  im vorderen Teil eine  haben. Das kann nicht sein, weil Klausel  maximal drei Literale enthält. Es kann auch nicht sein, dass eine Stelle  ist, weil das bedeuten würde, dass wir diese Klausel nicht erfüllen.
Da alle Stellen im vorderen Bereich mindestens  und maximal  sind, können wir durch Auswahl passender  und  die Summe auf  erhöhen. Es gibt also eine Auswahl an Zahlen, deren Summe gerade  ergibt.
In unserem Beispiel ist  eine erfüllende Belegung. Wir wählen also die Zahlen  aus und erhalten  als Zwischensumme
*
Durch Auswahl aller  und  können wir diese Summe auf  erhöhen.

Sei nun die KP-Instanz eine Ja-Instanz. Uns steht also eine Auswahl von Objekten zur Verfügung, deren Summe  ergibt. Wir stellen fest, dass diese für jedes  immer entweder  oder  enthalten muss, denn nur dann ist die entsprechende Stelle im hinteren Block genau . Dies definiert eine Belegung aller Variablen. Weiterhin kann die Instanz zwar Zahlen der Art  und  enthalten, diese summieren sich aber an jeder Stelle immer maximal zu  auf. Damit die Summe im vorderen Block immer  ist, muss für jede Stelle  im vorderen Block eine Zahl aus  gewählt worden sein, die hier eine  enthält. Dies bedeutet aber, dass die Belegung der entsprechenden Variable Klausel  erfüllt. Damit erfüllt die Variablenbelegung alle Klauseln und die -SAT-Instanz ist eine Ja-Instanz.

Wir wissen jetzt, dass 3 ist. -SAT ist also nicht schwieriger als KP (zumindest, wenn wir sehr große Zahlen als Eingabe für KP erlauben). Aber wie schwierig ist -SAT?
Wir halten zunächst den folgenden Fakt fest.

 

Ein Beweis dieser Aussage findet sich zum Beispiel in , Satz 3.4.1. Dieser beruht darauf, dass jede boolesche Formel zunächst in KNF dargestellt werden und dann lange Klauseln durch polynomiell viele Klauseln der Länge  ersetzt werden können.

Als nächstes stellen wir fest, dass polynomielle Reduktionen transitiv sind: Wir können, wenn wir  zeigen wollen, die polynomiellen Reduktionen aus Theorem  und Theorem  hintereinander verwenden. Damit wissen wir, dass KP nicht leichter ist als SAT. Die Bedeutung dieser Aussage lernen wir im nächsten Abschnitt kennen.

NP-Vollständigkeit

NP-vollständige Probleme sind die schwierigsten Probleme in NP.


 Ein Problem   heißt NP-schwierig, wenn für jedes Problem  in NP gilt:
 


 Ein Problem , das selbst in NP enthalten ist und NP-schwierig ist, heißt NP-vollständig.

Ein polynomieller Algorithmus für ein NP-vollständiges Problem könnte also verwendet werden, um alle Probleme in NP in polynomieller Zeit zu lösen! Es lässt sich tatsächlich beweisen, dass diese starke Eigenschaft für das Problem SAT erfüllt ist. Der entsprechende Satz ist sehr berühmt und hat die NP-Vollständigkeitstheorie begründet.

[Satz von Cook / Levin] SAT ist NP-vollständig.

Ein Beweis dieser Aussage findet sich in , Satz 3.3.7. Er erfordert als Grundlage einen formaleren Umgang mit NP als wir ihn in diesem Ausblick hier verwenden, und insbesondere die formal präzise Definition eines zugrundeliegenden Rechenmodells (verwendet wird die sogenannte Turingmaschine). Der eigentliche Beweis ist dann zwar etwas technisch, aber aus elementaren Schritten zusammengesetzt.

Da polynomielle Reduktionen transitiv sind, folgt aus Theorem  und Theorem  und Theorem , dass auch KP NP-vollständig ist. NP-Vollständigkeit wurde inzwischen für Hunderte von Problemen nachgewiesen. Insbesondere sind auch das Hamiltonkreisproblem und TSP NP-vollständig (siehe z.B. Satz 3.4.8 und 3.4.9 in  ).

